Given these constraints, who can build new digital infrastructure? Constraints, motivations, and strategies all depend on the circumstance of those doing the development. The undone work of infrastructure is being nibbled at around the edges[^actuallywork] by several different kinds of organization already ranging in scale and structure. A short survey to give us some notion of how we should seek to organize infrastructure building:

[^actuallywork]: aka doing hard development work in sometimes adverse conditions.

### Institutional Core Facilities

Centralized "core" facilities are maybe the most typical form of infrastructure development and resource sharing at the level of departments and institutions. These facilities can range from minimal to baroque extravagance depending on institutional resources and whatever complex web of local history brought them about. 

A [subproject](https://reporter.nih.gov/project-details/9444124#sub-Projects) within a [PNI Systems Core](https://projectreporter.nih.gov/project_info_details.cfm?aid=9444124) grant echoes a lot of the thoughts here, particularly regarding effort duplication[^tymae]: 

> Creating an Optical Instrumentation Core will  address the problem that much of the technical work required to innovate and maintain these instruments has  shifted to students and postdocs, because it has exceeded the capacity of existing staff. This division of  labor is a problem for four reasons: (1) lab personnel often do not have sufficient time or expertise to produce  the best possible results, (2) the diffusion of responsibility leads people to duplicate one another’s efforts, (3)  researchers spend their time on technical work at the expense of doing science, and (4) expertise can be lost  as students and postdocs move on. For all these reasons, we propose to standardize this function across  projects to improve quality control and efficiency. Centralizing the design, construction, maintenance, and  support of these instruments will increase the efficiency and rigor of our microscopy experiments, while  freeing lab personnel to focus on designing experiments and collecting data.   

[^tymae]: Thanks a lot to the one-and-only brilliant Dr. Eartha Mae Guthman for suggesting looking at the BRAIN initiative grants as a way of getting insight on core facilities.

While core facilities are an excellent way of expanding access, reducing redundancy, and standardizing tools *within* an institution, as commonly structured they can displace work spent on efforts that would be portable *outside* of the institution. Elite institutions can attract the researchers with the technical knowledge to develop the instrumentation of the core and infrastructure for maintaining it, but this development is only occasionally made usable by the broader public. The Princeton data science core is an excellent example of a core facility that does makes its software infrastructure development [public](https://github.com/BrainCOGS)[^pnidatascience], which they should be applauded for, but also illustrative of the problems with a core-focused infrastructure project. For an external user, the documentation and tutorials are incomplete -- it's not clear to me how one would set this up for my institute, lab, or data, and there are several places of hard-coded Princeton-specific values that I am unsure how exactly to adapt[^pnicaveat]. I would consider this example a high-water mark, and the median openness of core infrastructure falls far below it. I was unable to find an example of a core facility that maintained publicly-accessible documentation on the construction and operation of its experimental infrastructure or the management of its facility. 

This might be unsurprising given the economic structure of most core facilities: an institution pays for a core to benefit the institution, and downstream public benefits are a nice plus but not high up in the list of concerns (if present at all). Core facilities are thus unlikely to serve as the source of mass infrastructure, but they do serve as a point of local coordination within institutions, and so given some larger means of coordination may still be useful.

[^pnidatascience]: > Project Summary: Core 2, Data Science [...] In addition, the Core will build a data science platform that stores behavior, neural activity, and neural connectivity in a relational database that is queried by the DataJoint language. [...] This data-science platform will facilitate collaborative analysis of datasets by multiple researchers within the project, and make the analyses reproducible and extensible by other researchers. [...] https://projectreporter.nih.gov/project_info_description.cfm?aid=9444126&icde=0 

[^pnicaveat]: Though again, this project is exemplary, built by friends, and would be an excellent place to start extending towards global infrastructure. 

### Centralized Institutes

Outside of universities, the Allen Brain Institute is perhaps the most impactful reflection of centralization in neuroscience. The Allen Institute has, in an impressively short period of time, created several transformative tools and datasets, including its well-known atlases {% cite leinGenomewideAtlasGene2007 %} and the first iteration of its [Observatory](http://observatory.brain-map.org/) project which makes a massive, high-quality calcium imaging dataset of visual cortical activity available for public use. They also develop and maintain software tools like their [SDK](https://allensdk.readthedocs.io/en/latest/) and Brain Modeling Toolkit [(BMTK)](https://alleninstitute.github.io/bmtk/), as well as a collection of [hardware schematics](https://portal.brain-map.org/explore/toolkit/hardware) used in their experiments. The contribution of the Allen Institute to basic neuroscientific infrastructure is so great that, anecdotally, when talking about scientific infrastructure it's not uncommon for me to hear something along the lines of "I thought the Allen was doing that."

Though the Allen Institute is an excellent model for scale at the level of a single organization, its centralized, hierarchical structure cannot (and does not attempt to) serve as the backbone for all neuroscientific infrastructure. Performing single (or a small number of, as in its also-admirable [OpenScope Project](https://alleninstitute.org/what-we-do/brain-science/news-press/articles/three-collaborative-studies-launch-openscope-shared-observatory-neuroscience)) carefully controlled experiments a huge number of times is an important means of studying constrained problems, but is complementary with the diversity of research questions, model organisms, and methods present in the broader neuroscientific community. 

Christof Koch, its director, describes the challenge of centrally organizing a large number of researchers:

> Our biggest institutional challenge is organizational: assembling, managing, enabling and motivating large teams of diverse scientists, engineers and technicians to operate in a highly synergistic manner in pursuit of a few basic science goals {% cite grillnerWorldwideInitiativesAdvance2016 %}
> 
> These challenges grow as the size of the team grows. Our anecdotal evidence suggests that above a hundred members, group cohesion appears to become weaker with the appearance of semi-autonomous cliques and sub-groups. This may relate to the postulated limit on the number of meaningful social interactions humans can sustain given the size of their brain {% cite kochBigScienceTeam2016 %}

These institutes too are certainly helpful in building core technologies for the field, but they aren't necessarily organized for developing mass-scale infrastructure. They reflect the capabilities and needs of the institute itself, which are likely to be radically different than a small lab. They can build technologies on a background of expensive cloud storage and computation and rely on a team of engineers to implement and maintain them. So while the tools they make are certainly *useful* we shouldn't count on them to build the systems we need for scientists at large.

### Meso-scale collaborations

Given the diminishing returns to scale for centralized organizations, many have called for smaller, "meso-scale" collaborations and consortia that combine the efforts of multiple labs {% cite mainenBetterWayCrack2016 %}. The most successful consortium of this kind has been the International Brain Laboratory {% cite abbottInternationalLaboratorySystems2017 woolKnowledgeNetworksHow2020 %}, a group of 22 labs spread across six countries. They have been able to realize the promise of big team neuroscience, setting a new standard for performing reproducible experiments across many labs {% cite laboratoryStandardizedReproducibleMeasurement2020 %} and developing data management infrastructure to match {% cite laboratoryDataArchitectureLargescale2020 %}[^dataportal]. Their project thus serves as the benchmark for large-scale collaboration and a model from which all similar efforts should learn from.

[^dataportal]: Seriously, don't miss their extremely impressive [data portal](https://data.internationalbrainlab.org/).

Critical to the IBL's success was its adoption of a flat, non-hierarchical organizational structure, as described by Lauren E. Wool:

> IBL’s virtual environment has grown to accommodate a diversity of scientific activity, and is supported by a flexible, ‘flattened’ hierarchy that emphasizes horizontal relationships over vertical management. [...] Small teams of IBL members collaborate on projects in Working Groups (WGs), which are defined around particular specializations and milestones and coordinated jointly by a chair and associate chair (typically a PI and researcher, respectively). All WG chairs sit on the Executive Board to propagate decisions across WGs, facilitate operational and financial support, and prepare proposals for voting by the General Assembly, which represents all PIs. {% cite woolKnowledgeNetworksHow2020 %}

They should also be credited with their adoption of a form of consensus decision-making, [sociocracy](https://sociocracy.info), rather than a majority-vote or top-down decisionmaking structure. Consensus decision-making systems are derived from those developed by [Quakers and some Native American nations](https://rhizomenetwork.wordpress.com/2011/06/18/a-brief-history-of-consenus-decision-making/), and emphasize collective consent rather than the will of the majority.

The infrastructure developed by the IBL is impressive, but its focus on a single experiment makes it difficult to expand and translate to widescale use. The hardware for the IBL experimental apparatus is exceptionally well-documented, with a [complete and detailed build guide](https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732) and [library of CAD parts](https://figshare.com/articles/online_resource/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_CAD_files_for_behavior_rig/11639973), but the documentation is not modularized such that it might facilitate use in other projects, remixed, or repurposed. The [experimental software](https://github.com/int-brain-lab/iblrig) is similarly single-purpose, a chimeric combination of Bonsai {% cite lopesBonsaiEventbasedFramework2015 %} and [PyBpod](https://github.com/pybpod/pybpod) [scripts](https://github.com/int-brain-lab/iblrig/tree/master/tasks/_iblrig_tasks_ephysChoiceWorld). It unfortunately [lacks](https://iblrig.readthedocs.io/en/latest/index.html) the API-level documentation that would facilitate use and modification by other developers, so it is unclear to me, for example, how I would use the experimental apparatus in a different task with perhaps slightly different hardware, or how I would then contribute that back to the library. The experimental software, according to the [PDF documentation](https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732), will also not work without a connection to an [alyx](https://github.com/cortex-lab/alyx) database. While alyx was intended for use outside the IBL, it still has [IBL-specific](https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/management/commands/files.py#L188) and [task-specific](https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/fixtures/data.datasettype.json#L29) values in its source-code, and makes community development difficult with a similar [lack](https://alyx.readthedocs.io/en/latest/) of API-level documentation and requirement that users edit the library itself, rather than temporary user files, in order to use it outside the IBL.

My intention is not to denigrate the excellent tools built by the IBL, nor their inspiring realization of meso-scale collaboration, but to illustrate a problem that I see as an extension of that discussed in the context of core facilities --- designing infrastructure for one task, or one group in particular makes it much less likely to be portable to other tasks and groups. This argument is much more contingent on the specific circumstances of the consortium than the prior arguments about core facilities and institutes: when organized with mass-infrastructure in mind, collaborations between semi-autonomous groups across institutions could be a powerful mode of tool development.

It is also unclear how replicable these consortia are, and whether they challenge, rather than reinforce technical inequity in science. Participating in consortia systems like the IBL requires that labs have additional funding for labor hours spent on work for the consortium, and in the case of graduate students and postdocs, that time can conflict with work on their degrees or personal research which are still far more potent instruments of "remaining employed in science" than collaboration. In the case that only the most well-funded labs and institutions realize the benefits of big team science without explicit consideration given to scientific equity, mesoscale collaborations could have the unintended consequence of magnifying the skewed distribution of access to technical expertise and instrumentation.

The central lesson of the IBL, in my opinion, is that governance matters. Even if a consortium of labs were to form explicitly to build mass-scale digital infrastructure, without a formal system to ensure contributors felt heard and empowered to shape the project it would soon become unfocused or unsustainable. Even if this system is not perfect, with some labor still falling unequally on some researchers, it is a promising model for future collaborative consortia.

### The rest of us...

Outside of ivies with rich core facilities, institutes like the Allen, or nascent multi-lab consortia, the rest of us are largely on our own, piecing together what we can from proprietary and open source technology. The world of open source scientific software has plenty of energy and lots of excellent work is always being done, though constrained by the circumstances of its development described briefly above. Anything else comes down to whatever we can afford with remaining grant money, scrape together from local knowledge, methods sections, begging, borrowing, and (hopefully not too much) stealing from neighboring labs.  

The state of broader scientific deinfrastructuring is perhaps to be expected given our relationship to informational monopolies that in some part depend on it, but unlike many other industries or professions there is reason for hope in science. Science is packed with people with an enormous diversity of skills, resources, and perspectives. Publicly funded science is relatively unique as a labor system that does not strictly depend on profit. There is widespread discontent with the systems of scientific practice, and so the question becomes how we can organize our skill, labor, and energy to rebuild the systems that constrain us.

A third option from the standardization offered by centralization and the blooming, buzzing, beautiful chaos of disconnected open-source development is that of decentralized systems, and with them we might build the means by which the "rest of us" can mutually benefit by organizing our knowledge and labor.

We don't need to wait for permission from a memo from a funding body or the founding of some new organization. We do have to recognize that while we might have very different roles to play, we are all responsible for the state of digital scientific infrastructure. We should take courage and purpose in knowing that we are not alone, and that our problems are just one reflection of the model of digital enclosure and surveillance that defines the information economy. There is no need for distance or animosity with the other modes of organization described above, as if what we intend to build is truly useful to *everyone* except those that profit from its absence, then that certainly includes them.  Shunting the vision of a better future onto some as-yet formed effort is precisely the trap we should avoid: our existing organizations *should* be a part of the work of rebuilding our infrastructure precisely because we should be reconsidering the ways that *we, ourselves* work. Seeing a subscription to this platform monopolist's cloud, or that knowledge baron's prestige hierarchy as not being a value-neutral decision begs an alternative from people, labs, and institutions alike. The diversity in what that means for different groups is a *strength,* not a weakness, but it does require some shared vision and notion of how to get there. The rest of the paper is an attempt to draft one.
