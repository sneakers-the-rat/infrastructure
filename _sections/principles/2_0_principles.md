What should we build?

The infrastructural systems I will describe here are similar to previous notions of "grass-roots" science articulated within systems neuroscience {% cite mainenBetterWayCrack2016 %}, "small tech" {% cite balkanSmallTechnologyFoundation %} or the anti software software club's manifesto {% cite kaplanPartAntisoftwareAction2020 %} in the web development world , and shares some of the motivations of the [Solid project](https://solidproject.org/) {% cite sambraSolidPlatformDecentralized2016 %}, but ultimately draws from a set of ideas with broad and deep history in many domains of computing. My intention is to provide a more prescriptive scaffolding for their design and implementation as a way of painting a picture of what science could be like. This sketch is not intended to be final, but a starting point for further negotiation and refinement.

Throughout this section, when I am referring to any particular piece of software I want to be clear that I don't intend to be dogmatically advocating that software *in particular*, but software *like it* that *shares its qualities* --- no snake oil is sold in this document. Similarly, when I describe limitations of existing tools, without exception I am describing a tool or platform I love, have learned from, and think is valuable --- learning from something can mean drawing respectful contrast! Many of these technologies have long and torrid social histories, and so when invoked as examples I don't necessarily mean to import along with them all the unmentioned baggage that might accompany them[^permauri].

[^permauri]: As one example, while I will write about [linked data](https://www.w3.org/DesignIssues/LinkedData.html), I don't necessarily mean it in precisely the original instantiation as an irrevocable URI/RDF/SPARQL-only web, but do draw on its triplet link structure.

## Design Principles

I won't attempt to derive a definition of decentralized systems from first principles here, but from the constraints described above, some design principles that illustrate the idea emerge naturally. For the sake of concreteness, in some of these I will draw from the architectural principles of the internet protocols (specifically TCP/IP): the most successful decentralized digital technology project to date.


###  Protocols, not Platforms

Much of the basic technology of the internet was developed as protocols that describe the basic attributes and operations of a process. A simple and common example is email over SMTP (Simple Mail Transfer Protocol) {% cite Rfc5321SimpleMail %}. SMTP describes a series of steps that email servers must follow to send a message: the sender initiates a connection to the recipient server, the recipient server acknowledges the connection, a few more handshake steps ensue to describe the senders and receivers of the message, and then the data of the message is transferred. Any software that implements the protocol can send emails to and from any other. The protocol basis of email is the reason why it is possible to send an email from a gmail account to a hotmail account (or any other hacky homebrew SMTP client) despite being wholly different pieces of software. 

In contrast, *platforms* provide some service with a specific body of code usually without any pretense of generality. In contrast to email over SMTP, we have grown accustomed to not being able to send a message to someone using Telegram from WhatsApp, switching between multiple mutually incompatible apps that serve nearly identical purposes. Platforms, despite being *theoretically* more limited than associated protocols, are attractive for many reasons: they provide funding and administrative agencies a single point of contracting and liability, they typically provide a much more polished user interface, and so on. These benefits are short-lived, however, as the inevitable toll of lock-in and shadowy business models is realized.

By virtue of being intended for use by many independent organizations rather than under the sole control of a platform-holder, protocols are a complicated political effort that embed and facilitate systems of belief and power (see re: TCP/IP {% cite larsenPoliticalNatureTCP2012 %}, ActivityPub {% cite lemmer-webberStandardsDivisionsCollaboration2018 %}). For example, in order to arrive at a version of TCP/IP that kept the intermediate relays relatively simple at the expense of reliability, the manufacturer of the "smart" relays had to be excluded from the group. TCP/IP's success was not inevitable: it was one of several protocols, becoming the default over proprietary competitors from telecommunication and network hardware companies because of some combination of timing, its relative absence of bureaucracy, and institutional adoption (depending on who does the accounting){% cite larsenPoliticalNatureTCP2012 %}. 

Seemingly prosocial protocols can be used by industries to pre-empt an alternative that would undermine their profit model --- a notable example for academics being the DOI system, created in order for publishers to preserve control over their intellectual property {% cite rosenblattDigitalObjectIdentifier1997 %}. The STM association[^stm] hastily[^hastydoi] threw its weight behind the DOI-X initiative at its 1999 meeting. The impending creation of PubMed Central by the National Library of Medicine (and see then-NIH Director Harold Varmus' and others self-described "radical" departure from publishers with what became PLoS {% cite varmusArtPoliticsScience2009 robertsBuildingGenBankPublished2001  %}) posed an existential threat to for-profit publishing. At the time there was no unified means of linking to scholarly work[^urisnew], and bilateral publisher-publisher linking deals threatened the smooth operation of business, so an NIH-owned platform might have made journals might lose their status as the obligate dissemination platform. According to Bob Campbell, STM chair at the time: "our consensus was that publishers should be the ones doing the linking." Unlike the anarchic URI/URL, The DOI system requires a registrar (denoted by the prefix before the slash, `doi:10.xxxx/yyyyy`) to create DOI names {% cite isoISO2632420122012 %}. In the US, that means being an institution with an approved [CrossRef membership](https://www.crossref.org/services/content-registration/), which [requires](https://www.crossref.org/membership/terms/) members not to link to intellectual property infringing content, and to use DOIs as their default reference links to other works. Effectively, though it is an "open[^isocostsmoney]" standard, the DOI system ensures that publishers remain in control of what counts as scholarly work {% cite crossrefFormationCrossRefShort2009 %}.

[^stm]: The global trade association of publishers that serves as its lobbying and propaganda arm.

[^hastydoi]: The description provided by the "official" CrossRef 10 year retrospective paints a picture of panicked executives making an announcement for something they weren't quite sure what it would be, but it would be *something* to compete with pubmed:

	> We decided to issue an announcement of a broad STM reference linking initiative. It was, of course, a strategic move only, since we had neither plan nor prototype.”
	>
	> A small group led by Arnoud de Kemp of Springer-Verlag met in an adjacent room immediately following the Board meeting to draft the announcement, which was distributed to all attendees of the STM annual meeting the following day and published in an STM membership publication.
	>
	> Campbell recalled running into Bolman and Swanson (neither of whom was then on the STM Board) in the hotel lobby immediately after the drafting of the announcement. Their astonishment at hearing what had just transpired was matched by Campbell’s own on learning what they had been working on. [...]
	>
	> Bolman and Swanson chose to seize the moment, and called an ad hoc meeting the following evening, Tuesday, October 12, to announce their venture and assemble a coalition of publishers to launch it. [...]
	>
	> The potential benefit of the service that would become CrossRef was immediately apparent. Organizations such as AIP and IOP (Institute of Physics) had begun to link to each other’s publications, and the impossibility of replicating such one-off arrangements across the industry was obvious. As Tim Ingoldsby later put it, “All those linking agreements were going to kill us.” {% cite crossrefFormationCrossRefShort2009 %}

[^urisnew]: It is hard to appreciate in retrospect how radical URLs/URIs were at the time --- it might seem trivial to us now to be able to arbitrarily link to different locations on the internet, but before the internet linking was a carefully controlled process within publishing, looking more like ISBN and ISSNs than hyperlinks. 

[^isocostsmoney]: Reading the standard costs 88 Swiss Francs.

When approaching protocols, we should do so with humility and caution: work in smaller teams with shared visions with the intention of rough consensus around multiple instances of working code. We should refuse participation by the wide range of industries and interest groups circling each domain of infrastructure, their protocols and standards are siren songs.



### Integration, not Invention

At the advent of the internet protocols, several different institutions and universities had already developed existing network infrastructures, and so the "top level goal" of IP was to "develop an effective technique for multiplex utilization of existing interconnected networks," and "come to grips with the problem of integrating a number of separately administered entities into a common utility" {% cite clarkDesignPhilosophyDARPA1988 %}. As a result, IP was developed as a 'common language' that could be implemented on any hardware, and upon which other, more complex tools could be built. This is also a cultural practice: when the system doesn't meet some need, one should try to extend it rather than building a new, separate system --- and if a new system is needed, it should be interoperable with those that exist.

This point is practical as well as tactical: to compete, an emerging protocol should integrate or be capable of bridging with the technologies that currently fill its role. A new database protocol should be capable of reading and writing existing databases, a new format should be able to ingest and export to existing formats, and so on. The degree to which switching is seamless is the degree to which people will be willing to switch. 

This principle runs directly contrary to the current incentives for novelty and fragmentation and the dominant economic model of software platforms, which must be counterbalanced by design choices elsewhere.  

### Embrace Heterogeneity, Be Uncoercive

In addition to integrating with existing systems, it must be straightforward for unanticipated future development to be integrated to accomodate unanticipated needs and practices. This idea is related to "the test of independent invention", summarized with the question "if someone else had already invented your system, would theirs work with yours?" {% cite berners-leePrinciplesDesign1998 %}. Rather than attempting to *a priori* divine a single perfect universal protocol, we should design multiple with extensibility in mind (see this discussion of the extensibility models of ActivityPub to XMPP {% cite schubertActivityPubFinalThoughts2019 %} and Christopher Yoo's description of the tradeoffs of the internet's layered protocols {% cite yooProtocolLayeringInternet2013 %}) to leave open the opportunity for porting functionality between them. 

This principle also has tactical elements. An uncoercive system allows users to gradually adopt it rather than needing to adopt all of its components in order for any one of them to be useful. We shouldn't rely on potential users making dramatic changes to their existing practices. For example, an experimental framework should not insist on a prescribed set of supported hardware and rigid formulation for describing experiments. Instead it should provide affordances that give a clear way for users to extend the system to fit their needs {% cite carpenterRFC1958Architectural1996 %}.There always needs to be a *benefit* to adopting further components of the system to encourage *voluntary* adoption, but it should never be *compulsory.* For example, again from experimental frameworks, it should be possible to use it to control experimental hardware without needing to use the rest of the experimental design, data storage, and interface system. To some degree this is accomplished with a modular system design where designers are mindful of keeping the individual modules independently useful. 

A noncoercive architecture also prioritizes the ease of leaving. Though this is somewhat tautological to protocol-driven design, specific care must be taken to enable export and migration to new systems. Multiplicity of design and making leaving easy help ensure that early missteps in development of the system are not fatal, preventing lock-in to a component that becomes fixed and stagnant. 

### Empower People, not Systems

Because IP was initially developed as a military technology by DARPA, a primary design constraint was survivability in the face of failure. The model adopted by internet architects was to move as much functionality as possible from the network itself to the end-users of the network --- rather than the network itself guaranteeing a packet is transmitted, the sending computer will do so by requiring a response from the recipient {% cite clarkDesignPhilosophyDARPA1988 %}.

For infrastructure, we should make tools that don't require a central team of developers to maintain, a central server-farm to host data, or a small group of people to govern. Whenever possible, data, software, and hardware should be self-describing[^selfdescribing], so one needs minimal additional tools or resources to understand and use it. It should never be the case that funding drying up for one node in the system causes the entire system to fail. 

Practically, this means that the tools of digital infrastructure should be deployable by individual people and be capable of recapitulating the function of the system without reference to any central authority. Researchers need to be given control over the function of infrastructure: from controlling sharing permissions for eg. clinically sensitive data to assurance that their tools aren't spying on them. Formats and standards must be negotiable by the users of a system rather than regulated by a central governance body. 

### Infrastructure is Social

The alternative to centralized governing and development bodies is to build the tools for community control over infrastructural components. This is perhaps the largest missing piece in current scientific tooling. On one side, decentralized governance is the means by which an infrastructure can be maintained to serve the ever-evolving needs of its users. On the other, a sense of community ownership is what drives people to not only adopt but contribute to the development of an infrastructure. In addition to being a source of all the warm fuzzies of socially affiliative "community-ness," any collaborative system needs a way of ensuring that the practice of maintaining, building, and using it is designed to *visibly and tangibly benefit* those that do, rather than be relegated to a cabal of invisible developers and maintainers {% cite grudinGroupwareSocialDynamics1994 randallDistributedOntologyBuilding2011 %}.


Governance and communication tools also make it possible to realize the infinite variation in application that infrastructures need while keeping them coherent: tools must be built with means of bringing the endless local conversations and modifications of use into a common space where they can become a cumulative sense of shared memory.


I will return to this idea in [Archives Need Communities](#archives-need-communities) in the context of social dynamics of private bittorrent trackers, as well as propose a set of basic communication and governance tools in [Rebuilding Scientific Communication](#rebuilding-scientific-communication).

### Usability Matters

It is not enough to build a technically correct technology and assume it will be adopted or even useful, it must be developed embedded within communities of practice and *be useful for solving problems that people actually have.* We should learn from the struggles of the semantic web project. Rather than building a fully prescriptive and complete system first and deploying it later, we should develop tools whose usability is continuously improved *en route* to a (flexible) completed vision. 

The adage from RFC 1958[^rfc1958] "nothing gets standardized until there are multiple instances of running code" {% cite carpenterRFC1958Architectural1996 %} captures the dual nature of the constraint well. Workable standards don't emerge until they have been extensively tested in the field, but development without an eye to an eventual protocol won't make one. 

We should read the [gobbling up](https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish) of open protocols into proprietary platforms that defined "Web 2.0" as instructive[^moneyofc]{% cite markoffTomorrowWorldWide1996 %}. *Why* did Slack outcompete IRC?[^slackirc] The answer is relatively simple: it was relatively simple to use. Using a contemporary example, to [set up a Synapse server](https://matrix-org.github.io/synapse/latest/setup/installation.html) to communicate over [Matrix](https://matrix.org/docs/spec/) one has to wade through dozens of shell commands, system-specific instructions, potential conflicts between dependent packages, set up an SQL server... and that's just the backend, we don't even have a frontend client yet! In contrast, to use Slack you download the app, give it your email, and you're off and running.

[^moneyofc]: (in addition to a demonstration of the raw power of concentrated capital, of course)

The control exerted by centralized systems over their system design does give certain structural advantages to their usability, and their for-profit model gives certain advantages to their development process. There is no reason, however, that decentralized systems *must* be intrinsically harder to use, we just need to focus on user experience to a degree comparable to centralized platforms: if it takes a college degree to turn the water on, that ain't infrastructure.

People are smart, they just get frustrated easily and have other things to do on a deadline. We have to raise our standards of design such that we don't expect users to have even a passing familiarity with programming, attempting to build tools that are truly general use. We can't just design a peer-to-peer system, we need to make the data ingestion and annotation process automatic, effortless, and expressive. We can't just build a system for credit assignment, it needs to happen as an automatic byproduct of using the system. We can't just make tools that *work,* they need to *feel good to use.*

Centralized systems also have intrinsic limitations that provide openings for decentralized systems, like cost, incompatibility with other systems, restrictions on independent extension, and opacity of function. The potential for decentralized systems to capture the independent development labor of all of its users, rather than just that of a core development team, is one means of competition. If a system is sufficiently easy to adopt, at least comparable to prior tooling, and gives people a satisfying means of having their work accepted and valued, the social and technical joy might be enough to outweigh the inertia of change and the convenience of centralized systems.

With these principles in mind, and drawing from other knowledge communities solving similar problems: internet infrastructure, library/information science, peer-to-peer networks, and radical organizing, I conceptualize a system of distributed infrastructure for (neuro)science as three objectives: [**shared data**](#shared-data), [**shared tools**](#shared-tools), and [**shared knowledge**](#shared-knowledge).
