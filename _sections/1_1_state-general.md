The incentive systems in science are complex, subject to infinite variation everywhere, so these are intended as general tendencies rather than statements of irrevocable and uniform truth.

### Incentivized Fragmentation

Scientific software development favors the production of many isolated, single-purpose software packages rather than cumulative work on shared infrastructure. The primary means of evaluation for a scientist is academic reputation, primarily operationalized by publications, but a software project will yield a single paper (if any). Traditional publications are static units of work that are "finished" and frozen in time, but software is never finished: the thousands of commits needed to maintain and extend the software are formally not a part of the system of academic reputation. 

Howison & Herbsleb described this dynamic in the context of BLAST[^whatisblast]

> In essence we found that BLAST innovations from those motivated to improve BLAST by academic reputation are motivated to develop and to reveal, but not to integrate their contributions. Either integration is actively avoided to maintain a separate academic reputation or it is highly conditioned on whether or not publications on which they are authors will receive visibility and citation. {% cite howisonIncentivesIntegrationScientific2013 %}


For an example in Neuroscience, one can browse the papers that cite the DeepLabCut paper {% cite mathisDeepLabCutMarkerlessPose2018 %} to find hundreds of downstream projects that make various extensions and improvements that are not integrated into the main library. While the alternative extreme of a single monolithic ur-library is also undesirable, working in fragmented islands makes infrastructure a random walk instead of a cumulative effort.

After publication, scientists have little incentive to **maintain** software outside of the domains in which the primary contributors use it, so outside of the most-used libraries most scientific software is brittle and difficult to use {% cite mangulImprovingUsabilityArchival2019 kumarBioinformaticsSoftwareBiologists2007 %}. 

Since the reputational value of a publication depends on its placement within a journal and number of citations (among other metrics), citation practices for scientific software are far from uniform and universal, and relatively few "prestige" journals publish software papers at all, the incentive to write scientific software in the first place is low compared to its near-universal use {% cite howisonSoftwareScientificLiterature2016 %}.

### Domain-Specific Silos

When funding exists for scientific infrastructure development, it typically comes in the form of side effects from, or administrative supplements to research grants. The NIH describes as much in their Strategic Plan for Data Science {% cite NIHStrategicPlan2018 %}:

> from 2007 to 2016, NIH ICs used dozens of different funding strategies to support data 
resources, most of them linked to research-grant mechanisms that prioritized innovation and hypothesis testing over user service, utility, access, or efficiency. In addition, although the need for open and efficient data sharing is clear, where to store and access datasets generated by individual laboratories—and how to make them compliant with FAIR principles—is not yet straightforward. Overall, it is critical that the data-resource ecosystem become seamlessly integrated such that different data types and information about different organisms or diseases can be used easily together rather than existing in separate data “silos” with only local utility. 

The National Library of Medicine within the NIH currently lists 122 separate databases in its [search tool](https://eresources.nlm.nih.gov/nlm_eresources/), each serving a specific type of data for a specific research community. Though their current funding priorities signal a shift away from domain-specific tools, the rest of the scientific software system consists primarily of tools and data formats purpose-built for a relatively circumscribed group of scientists. Every field has its own challenges and needs for software tools, but there is little incentive to build tools that serve as generalized frameworks to integrate them.

### "The Long Now" of Immediacy vs. Idealism

Digital infrastructure development takes place at multiple timescales simultaneously --- from the momentary work of implementing it; through longer timescales of planning, organization, and documenting; to the imagined indefinite future of its use --- what Ribes and Finholt call "The Long Now. {% cite ribesLongNowTechnology2009 %}" Infrastructural projects constitutively need to contend with the need for immediately useful results vs. general and robust systems; the need to involve the effort of skilled workers vs. the uncertainty of future support; the  balance between stability and mutability; and so on. The tension between hacking something together vs. building something sustainable for future use is well-trod territory in the hot-glue and exposed wiring of systems neuroscience rigs.

Deinfrastructuring divides the incentives and interests of junior and senior researchers. ECRs might be interested in developing tools they'll use throughout their careers, but given the pressure to establish their reputation with publications rarely have the time to develop something fully. The time pressure never ends, and established researchers also need to push enough publications through the door to be able to secure the next round of funding. The time preference of scientific software development is thus very short: hack it together, get the paper out, we'll fix it later. 

The constant need to produce software that *does something* in the context of scientific programming which largely lacks the institutional systems and expert mentorship needed for well-architected software means that most programmers *never* have a chance to learn best practices commonly accepted in software engineering. As a consequence, a lot of software tools are developed by near-amateurs with no formal software training, contributing to their brittleness {% cite altschulAnatomySuccessfulComputational2013 %}. 

The problem of time horizon in development is not purely a product of inexperience, and a longer time horizon is not uniformly better. We can look to the history of the semantic web, a project that was intended to bridge human and computer-readable content on the web, for cautionary tales. In the semantic web era, thousands of some of the most gifted programmers and some of the original architects of the internet worked with an eye to the indefinite future, but the raw idealism and neglect of the pragmatic reality of the need for software to *do something* drove many to abandon the effort (bold is mine, italics in original):

> **But there was no *use* of it.** I wasn't using any of the technologies for anything, except for things related to the technology itself. The Semantic Web is utterly inbred in that respect. The problem is in the model, that we create this metaformat, RDF, and *then* the use cases will come. But they haven't, and they won't. Even the genealogy use case turned out to be based on a fallacy. The very few use cases that there are, such as Dan Connolly's hAudio export process, don't justify hundreds of eminent computer scientists cranking out specification after specification and API after API.
>
> When we discussed this on the Semantic Web Interest Group, the conversation kept turning to how the formats could be fixed to make the use cases that I outlined happen. “Yeah, Sean's right, let's fix our languages!” But **it's not the languages which are broken,** except in as much as they are entirely broken: because **it's the *mentality* of their design which is broken.** You can't, it has turned out, make a metalanguage like RDF and then go looking for use cases. We thought you could, but you can't. It's taken eight years to realise.
> {% cite palmerDitchingSemanticWeb2008 %}

Developing digital infrastructure must be both bound to fulfilling immediate, incremental needs as well as guided by a long-range vision. The technical and social lessons run in parallel: We need software that solves problems people actually have, but can flexibly support an eventual form that allows new possibilities. We need a long-range vision to know what kind of tools we should build and which we shouldn't, and we need to keep it in a tight loop with the always-changing needs of the people it supports. 

In short, to develop digital infrastructure we need to be *strategic.* To be strategic we need a *plan.* To have a plan we need to value planning as *work.* On the valuation of this kind of work, Ribes and Finholt are instructive:

> "On the one hand, I know we have to keep it all running, but on the other, LTER is about long-term data archiving. If we want to do that, we have to have the time to test and enact new approaches. But if we’re working on the to-do lists, we aren’t working on the tomorrow-list" (LTER workgroup discussion 10/05).
>
> The tension described here involves not only time management, but also the differing valuations placed on these kinds of work. The implicit hierarchy places scientific research first, followed by deployment of new analytic tools and resources, and trailed by maintenance work. [...] While in an ideal situation development could be tied to everyday maintenance, in practice, maintenance work is often invisible and undervalued. As Star notes, infrastructure becomes visible upon breakdown, and only then is attention directed at its everyday workings (1999). Scientists are said to be rewarded for producing new knowledge, developers for successfully implementing a novel technology, but the work of maintenance (while crucial) is often thankless, of low status, and difficult to track. *How can projects support the distribution of work across research, development, and maintenance?* {% cite ribesLongNowTechnology2009 %}

### "Neatness" vs "Scruffiness"

Closely related to the tension between "Now" and "Later" is the tension between "Neatness" and "Scruffiness." Lindsay Poirier traces its reflection in the semantic web community as the way that differences in "thought styles" result in different "design logics"  {% cite poirierTurnScruffyEthnographic2017 %}. On the question of how to develop technology for representing the ontology of the web -- the system of terminology and structures with which everything should be named -- there were (very roughly) two camps. The "neats" prioritized consistency, predictability, uniformity, and coherence -- a logically complete and formally valid System of Everything. The "scruffies" prioritized local systems of knowledge, expressivity, "believing that ontologies will evolve organically as everyday webmasters figure out what schemas they need to describe and link their data. {% cite poirierTurnScruffyEthnographic2017 %}" 

This tension is as old as the internet, where amidst the [dot-com bubble](https://en.wikipedia.org/wiki/Dot-com_bubble) a telecom spokesperson lamented that the internet wasn't controllable enough to be profitable because "it was devised by a bunch of hippie anarchists." {% cite hiltzikTamingWildWild2001 %} The hippie anarchists probably agreed, rejecting "kings, presidents and voting" in favor of "rough consensus and running code." Clearly, the difference in thought styles has an unsubtle relationship with beliefs about who should be able to exercise power and what ends a system should serve {% cite larsenPoliticalNatureTCP2012 %}.

![Black and white slide with title "The last force on us - us," page 551 in https://www.ietf.org/proceedings/24.pdf full text: "The standards elephant  of  yesterday  - OSi. The standards elephant of today - its right here. As the Internet and its community grows, how do wemanage the process of change and  growth? ̄Open process - let all voices  be heard. ̄Closed  process - make progress. ̄Quick process - keep up with reality. Slow process - leave  time to think Market driven process - future is commercial. Scaling driven  process - the  future is the internet. We reject: kings, presidents and voting.We believe in: rough consensus and running code."](/infrastructure/assets/images/clark-slide.png)
*A slide from David Clark's 1992 "Views of the Future"{% cite clarkCloudyCrystalBall1992 %} that contrasts differing visions for the development process of the future of the internet. The struggle between engineered order and wild untamedness is summarized forcefully as "We reject: kings, presidents and voting. We believe in: rough consensus and running code"*

Practically, the differences between these thought communities impact the tools they build. Aaron Swartz put the approach of the "neat" semantic web architects the way he did:

> Instead of the “let’s just build something that works” attitude that made the Web (and the Internet) such a roaring success, they brought the formalizing mindset of mathematicians and the institutional structures of academics and defense contractors. They formed committees to form working groups to write drafts of ontologies that carefully listed (in 100-page Word documents) all possible things in the universe and the various properties they could have, and they spent hours in Talmudic debates over whether a washing machine was a kitchen appliance or a household cleaning device. 
> 
> With them has come academic research and government grants and corporate R&D and the whole apparatus of people and institutions that scream “pipedream.” And instead of spending time building things, they’ve convinced people interested in these ideas that the first thing we need to do is write standards. (To engineers, this is absurd from the start—standards are things you write after you’ve got something working, not before!) {% cite swartzAaronSwartzProgrammable2013 %}  

The outcomes of this cultural rift are subtle, but the broad strokes are clear: the "scruffies" largely diverged into the linked data community, which has taken some of the core semantic web technology like RDF, OWL, and the like, and developed a broad range of downstream technologies that have found purchase across information sciences, library sciences, and other applied domains[^andgoogle]. The linked data developers, starting by acknowledging that no one system can possibly capture everything, build tools that allow expression of local systems of meaning with the expectation and affordances for linking data between these systems as an ongoing social process.

The vision of a totalizing and logically consistent semantic web, however, has largely faded into obscurity. One developer involved with semantic web technologies (who requested not be named), captured the present situation in their description of a still-active developer mailing list: 

> I think that some people are completely detached from practical applications of what they propose. [...] I could not follow half of the messages. these guys seem completely removed from our plane of existence and I have no clue what they are trying to solve.

This division in thought styles generalizes across domains of infrastructure, though outside of the linked data and similar worlds the dichotomy is more frequently between "neatness" and "people doing whatever" -- with integration and interoperability becoming nearly synonymous with standardization. Calls for standardization without careful consideration and incorporation of existing practice have a familiar cycle: devise a standard that will solve everything, implement it, wonder why people aren't using it, funding and energy dissipiates, rinse, repeat[^xkcd]. The difficulty of scaling an exacting vision of how data should be formatted, the tools researchers should use for their experiments, and so on is that they require dramatic and sometimes total changes to the way people do science. The alternative is not between standardization and chaos, but a potential third way is designing infrastructures that allow the diversity of approaches, tools, and techniques to be expressed in a common framework or protocol along with the community infrastructure to allow the continual negotiation of their relationship.

[^xkcd]: There is, of course, an XKCD for that to which we make obligatory reference: [https://xkcd.com/927/](https://xkcd.com/927/)

### Taped-on Interfaces: Open-Loop User Testing

The point of most active competition in many domains of commercial software is the user interface and experience (UI/UX). To compete, software companies will exhaustively user-test and refine them with pixel precision to avoid any potential customer feeling even a thimbleful of frustration. Scientific software development is largely disconnected from usability testing, as what little support exists is rarely tied to it. This, combined with the preponderance of semi-amateurs and above incentives for developing new packages -- and thus reduplicating the work of interface development -- make it perhaps unsurprising that most scientific software is hard to use!

I intend the notion of "interface" in an expansive way: In addition to a graphical user interface (GUI) or set of functions and calling conventions exposed to the end-user, I am referring generally to all points of contact with users, developers, and other software. Interfaces are intrinsically social, and include the surrounding documentation and experience of use --- part of using software is being able to figure out how to use it! The favored design idiom of scientific software is the black box: I implemented an algorithm of some kind, here are the two or three functions needed to use it, but beneath the surface there be dragons. 

Ideally, software would be designed with developer interfaces and documentation at multiple scales of complexity to enable clean entrypoints for developers with differing levels of skill and investment to contribute. When this kind of design and documentation is underdeveloped, even widely used projects with excellent top-level interfaces like [poetry](https://python-poetry.org/) struggle to respond to the pile of issues [thousands deep](https://github.com/python-poetry/poetry/issues) as even users who have spent time reading the source have difficulty understanding what exactly needs to be fixed and maintainers have to spend their time triaging them and manually re-explaining the software hundreds of times[^poetryexample].

[^poetryexample]: For one example of many, see [Issue #3855](https://github.com/python-poetry/poetry/issues/3855), where several users try to make sense of the way poetry resolves packages from multiple sources --- a conversation that has been happening for more than a year at the time of writing across [multiple related issues](https://github.com/python-poetry/poetry/discussions/4137#discussioncomment-2320644).

Additionally, it would include interfaces for use and integration with other software --- or APIs. While the term "API" most commonly refers to [web APIs](https://en.wikipedia.org/wiki/Web_API), the term generally refers to the means by which other programs can interact with a given program. All programs have some limit to their function, the question is how other programs are expected to handle them. One particularly successful approach to program interface design is the Unix philosophy as articulated by Doug McIlroy and colleagues {% cite mcilroyUNIXTimeSharingSystem1978 %} --- which was originally designed to help build research software. Its first "make each program do one thing well" and second "expect the output of every program to become the input to another, as yet unknown, program" principles inspired a set of simple tools that can be composed together for complex tasks. When a program is monolithic and isn't designed to provide access to its component parts, it becomes difficult to reuse in downstream projects, potentially reskin with a more friendly user interface, and ultimately more likely to be a dead-end in a system of shared infrastructure. 

Without care given to any of these types of interfaces, the barrier to use is likely to remain high, the community of co-developers is likely to remain small, and the labor they expend is less likely to be useful outside that single project. This, in turn, closes the loop with incentives to develop new packages and makes another vicious cycle reinforcing fragmentation[^uxloop]. 


### Platforms, Industry Capture, and the Profit Motive

Publicly funded science is an always-irresistable golden goose for private industry. The fragmented interests of scientists and the historically light touch of funding agencies on encroaching privatization means that if some company manages to capture and privatize a corner of scientific practice they are likely to keep it. Industry capture has been thoroughly criticized in the context of the journal system (eg. recently, {% cite brembsReplacingAcademicJournals2021 %}), and that criticism should extend to the rest of our infrastructure as information companies seek to build a for-profit platform system that spans the scientific workflow (eg. {% cite ElsevierSevenBridges2017 %}). The mode of privatization of scientific infrastructure follows the broader software market as a proliferation of software as a service (SaaS), from startups to international megacorporations, that rent access to some, typically proprietary software without selling the software itself. 

While in isolation SaaS can make individual components of the infrastructural landscape easier to access --- and even free!!* --- the business model is fundamentally incompatible with integrated and accessible infrastructure. The SaaS model derives revenue from subscription or use costs, often operating as "freemium" models that make some subset of its services available for free. Even in freemium models, though, the business model requires that some functionality of the platform is enclosed and proprietary. To keep the particular domain of enclosure viable as a profit stream, the proprietor needs to actively defend against competitors as well as any technology that might fill the need for the proprietary technology[^googleantitrust] (See a more thorough treatment of platform capitalism in science in {% cite mirowskiFutureOpenScience2018 %}) 

As isolated services, one can imagine the practice of science devolving along a similar path as the increasingly-fragmented streaming video market: to do my work I need to subscribe to a data storage service, a cloud computing service, a platform to host my experiments, etc. For larger software platforms, however, vertical integration of multiple complementary services makes their impact on infrastructure more insidious. Locking users into more and more services makes for more and more revenue, which encourages platforms to be as mutually incompatible as they can get away with {% cite macinnesCompatibilityStandardsMonopoly2005 %}. To encourage adoption, platforms that can offer multiple services may offer one of the services -- say, data storage -- for free, forcing the user to use the adjoining services -- say, a cloud computing platform. 

Since these platforms are often subsidiaries of information industry monopolists, scientists become complicit in their often profoundly unethical behavior of by funneling millions of dollars into them. Longterm, unconditional funding of wildly profitable journals has allowed conglomerates like Elsevier to become sprawling surveillance companies {% cite RELXAnnualReport2020 pooleySurveillancePublishing2021  %} that are sucking as much data up as they can to market derivative products like algorithmic ranking of scientific productivity {% cite brembsAlgorithmicEmploymentDecisions2021 %} and making data sharing agreements with ICE {% cite biddleLexisNexisProvideGiant2021 %}. Or our reliance on AWS and the laundry list of human rights abuses by Amazon {% cite CriticismAmazon2021 %}. In addition to lock-in, dependence on a constellation of SaaS allows the opportunity for platform-holders to take advantage of their limitations and *sell us additional services to make up for what the other ones purposely lack* --- for example Elsevier has taken advantage of our dependence on the journal system and its strategic disorganization to sell a tool for summarizing trending research areas for tailoring maximally-fundable grants {% cite elsevierTopicProminenceSciencea %}.

Funding models and incentive structures in science are uniformly aligned towards the platformatization of scientific infrastructure. Aside from the corporate doublespeak "technology transfer" rhetoric that pervades the neoliberal university, the relative absence of major funding opportunities for scientific software developers competitive with the profit potential from "industry" often leaves it as the only viable career path. The preceding structural constraints on local infrastructural development strongly incentivize labs and researchers to rely on SaaS that provides a readymade solution to specific problems. Distressingly, rather than supporting infrastructural development that would avoid obligate payments to platform-holders, funding agencies seem all too happy to lean into them (emphases mine):

> NIH will **leverage what is available in the private sector,** either through strategic partnerships or procurement, to create a workable **Platform as a Service (PaaS)** environment. [...] NIH will partner with cloud-service providers for cloud storage, computational, and related infrastructure services needed to facilitate the deposit, storage, and access to large, high-value NIH datasets. [...]
>
>  NIH’s cloud-marketplace initiative will be the first step in a phased operational framework that **establishes a SaaS paradigm for NIH and its stakeholders.** (-NIH Strategic Plan for Data Science, 2018 {% cite NIHStrategicPlan2018 %})

The articulated plan being to pay platform holders to house data while also paying for the labor to maintain those databases veers into parody, haplessly building another triple-pay industry {% cite buranyiStaggeringlyProfitableBusiness2017 %} into the economic system of science --- one can hardly wait until they have the opportunity to rent their own data back with a monthly subscription. This isn't a metaphor: the STRIDES program, with the official subdomain [cloud.nih.gov](https://web.archive.org/web/20210729131920/https://cloud.nih.gov/), has been authorized to pay $85 million to cloud providers since 2018. In exchange, NIH hasn't received any sort of new technology, but ["extramural"](https://web.archive.org/web/20211006003547/https://cloud.nih.gov/enrollment/account-type/) scientists receive a maximum discount of 25% on cloud storage and "data egress" fees as well as plenty of training on how to give control of the scientific process to platform giants {% cite reillyNIHSTRIDESInitiative2021 %}[^STRIDESsuccess]. Without exaggeration, we are paying them to let us pay for something that makes it so we need to pay them more later.

[^STRIDESsuccess]: Their success stories tell the story of platform non-integration where scientists have to handbuild new tools to manage their data across multiple cloud environments: "We have been storing data in both cloud environments because we wanted the ecosystem we are creating to work on both clouds" {% cite STRIDESInitiativeSuccess2020 %}

It is unclear to me whether this is the result of the cultural hegemony of platform capitalism narrowing the space of imaginable infrastructures, industry capture of the decision-making process, or both, but the effect is the same in any case. 

### Protection of Institutional and Economic Power

Aside from information industries, infrastructural deficits are certainly not without beneficiaries within science --- those that have already accrued power and status. 

Structurally, the adoption of SaaS on a wide scale necessarily sacrifices the goals of an integrated mass infrastructure as the practice of research is carved into small, marketable chunks within vertically integrated technology platforms. Worse, it stands to amplify, rather than reduce, inequities in science, as the labs and institutes that are able to afford the tolls between each of the weigh stations of infrastructure are able to operate more efficiently --- one of many positive feedback loops of inequity.

More generally, incentives across infrastructures are often misaligned across strata of power and wealth. Those at the top of a power hierarchy have every incentive to maintain the fragmentation that prevents people from competing --- hopefully mostly unconsciously via uncritically participating in the system rather than maliciously reinforcing it. 

This poses an organizational problem: the kind of infrastructure that unwinds platform ownership is not only unprofitable, it's **anti-profitable** -- making it impossible to profit from its domain of use. That makes it difficult to rally the kind of development and [lobbying](https://www.snsi.info/) resources that profitable technology can, requiring organization based on ethical principles and a commitment to sacrifice control in order to serve a practical need.

The problem is not insurmountable, and there are strategic advantages to decentralized infrastructure and its development within science. Centralized technologies and companies might have more concerted power, but we have *numbers* and can make tools that let us combine small amounts of labor from many people. We often start (and end) our dreams of infrastructure with the belief that they will necessarily cost a lot of *money,* but that's propaganda. Of course development isn't *free,* but the cost of decentralized technologies is far smaller than the vast sums of money funnelled into industry profits, labor hours spent compensating for the designed inefficiencies of the platform model, and the development of a fragmented tool ecosystem built around them.

Science, as one of few domains of non-economic labor, has the opportunity to be a seed for decentralized technologies that could broadly improve not only the health of scientific practice, but the broader information ecosystem. We can develop a plan and mobilize to make use of our collective expertise to build tools that have no business model and no means of development in commercial domains --- we just need to realize what's at stake and agree that the health of science is more important than the convenience of the cloud[^awsdown] or which journal our papers go into.