> The Web is more a social creation than a technical one. I designed it for a social effect --- to help people work together --- and not as a technical toy. [...] We clump into families, associations, and companies. We develop trust across the miles and distrust around the corner. What we believe, endorse, agree with, and depend on is representable and, increasingly, represented on the Web. We all have to ensure that the society we build with the Web is of the sort we intend.
>
> Tim Berners-Lee (1999) *Weaving the Web* {% cite berners-leeWeavingWebOriginal1999 %}

The remaining set of problems implied by the infrastructural system sketched so far are the *communication* and *organization* systems that make up the interfaces to maintain and use it. We can finally return to some of the breadcrumbs laid before: the need for negotiating over distributed and conflicting data schema, for incentivizing and organizing collective labor, and for communicating information within and without academia. 

The communication systems that are needed double as *knowledge organization* systems. Knowledge organization has the rosy hue of something that might be uncontroversial and apolitical --- surely everyone involved in scientific communication wants knowledge to be organized, right? The reality of scientific practice might give a hint at our naivete. Despite being, in some sense, itself an effort to organize knowledge, *scientific results effectively have no system of explicit organization.* There is no means of, say, "finding all the papers about a research question."[^marderjoyof] The problem is so fundamental it seems natural: the usual methods of using search engines, asking around on Twitter, and chasing citation trees are flex tape slapped over the central absence of a system for formally relating our work as a shared body of knowledge. 

[^marderjoyof]: Also see Eve Marder's recent short and characteristically refreshing piece which in part discusses the problem of keeping up with scientific literature in the context of maintaining the joy of discovery {% cite marderMaintainingJoyDiscovery2022 %}.

Information capitalism, in its terrifying splendor, here too pits private profit against public good. Analogously to the necessary functional limitations of SaaS platforms, artificially limiting knowledge organization opens space for new products and profit opportunities. In their 2020 shareholder report, RELX, the parent of Elsevier, lists increasing the number of journals and papers as a primary means of increasing revenue {% cite RELXAnnualReport2020 %}. This represents a shift in their business model from subscriptions to deals like open access, which according to RELX CEO Erik Nils EngstrÃ¶m "is where revenue is priced per article on a more explicit basis" {% cite relx2020ResultsPresentation2021 %}.

In the next breath, they describe how "in databases & tools and electronic reference, representing over a third of divisional[^whatdivision] revenue, we continued to drive good growth through content development and enhanced machine learning [ML] and natural language processing [NLP] based functionality." 

What ML and NLP systems are they referring to? The 2019 report is a bit more revealing (emphases mine): 

> Elsevier looks to enhance quality by building on its premium brands and **grow article volume** through **new journal launches,** the expansion of open access journals and growth from emerging markets; and add value to core platforms by implementing capabilities such as **advanced recommendations on ScienceDirect and social collaboration through reference manager and collaboration tool Mendeley.**
>
> **In every market, Elsevier is applying advanced ML and NLP techniques** to help researchers, engineers and clinicians perform their work better. For example, in research, ScienceDirect Topics, a free layer of content that enhances the user experience, uses **ML and NLP techniques to classify scientific content and organise it thematically,** enabling users to get faster access to relevant results and related scientific topics. The feature, launched in 2017, is proving popular, generating 15% of monthly unique visitors to ScienceDirect via a topic page. **Elsevier also applies advanced ML techniques that detect trending topics per domain,** helping researchers make more informed decisions about their research. **Coupled with the automated profiling and extraction of funding body information from scientific articles,** this process supports the whole researcher journey; from planning, to execution and funding. {% cite RELXAnnualReport2019 %}

[^whatdivision]: RELX is a huge information conglomerate, and scientific publication is just one division.

Reading between the lines, it's clear that the difficulty of finding research is a feature, not a bug of their system. Their explicit business model is to increase the number of publications and sell organization back to us with recommendation services. The recommendation system might be free[^notfree], but the business is to maintain the self-reinforcing system of prestige where researchers compete for placement in highly visible journals to stand out among a wash of papers, in the process reifying the mythology {% cite brembsPrestigiousScienceJournals2018 %} of the "great journals." With semantic structure to locate papers, it becomes much more difficult to sell high citation count as a product --- people can find what they need, rather than needing to pay attention to a few high-profile journals. Without it, which papers might a paper discovery system created by a publisher recommend? The transition from a strictly journal-based discovery system to a machine learning powered search and feed model mirrors the strategic displacement of explicit organization by search in the rest of the digital economy, and presents similar opportunities for profit. Every algorithmically curated feed is an opportunity to sell ad placement[^nativeads] --- which they proudly describe as looking very similar to their research content {% cite springernatureBrandedContent elsevier360AdvertisingSolutions %}. 

[^notfree]: "free"

[^nativeads]: a strategy that the reprehensible digital marketing disciplines call "native advertising" {% cite dekeyzerProcessingNativeAdvertising2021 koutsopoulosNativeAdvertisementSelection2016 %}

The extended universe of profitmaking from knowledge disorganization gets more sinister: Elsevier sells multiple products to recommend 'trending' research areas likely to win grants, rank scientists, etc., algorithmically filling a need created by knowledge disorganization. The branding varies by audience, but the products are the same. For pharmaceutical companies ["scientific opportunity analysis"](https://www.elsevier.com/solutions/professional-services/drug-design-optimization#opportunity) promises custom reports that answer questions like "Which targets are currently being studied?" "Which experts are not collaborating with a competitor?" and "How much funding is dedicated to a particular area of research, and how much progress has been made?" {% cite elsevierDrugDesignOptimization %}. For academics, ["Topic Prominence in Science"](https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science#how) offers university administrators tools to "enrich strategic research planning with portfolio overviews of their own and peer institutions." Researchers get tools to "identify experts and potential cross-sector collaborators in specific Topics to strengthen their project teams and funding bids and identify Topics which are likely to be well funded." {% cite elsevierTopicProminenceScienceb %}  This reflects RELX's transition "from electronic reference, information reference tools, databases to [...] analytics and decision tools." {% cite relxRELX2020Results2021 %} Publishing is old news, the real money is in tools for extending control through the rest of the process of research.

These tools are, of course, designed for a race to the bottom --- if my colleague is getting an algorithmic leg up, how can I afford not to? Naturally only those labs that *can* afford them and the costs of rapidly pivoting research topics will benefit from them, making yet another mechanism that reentrenches scientific inequity for profit. Knowledge disorganization, coupled with a little surveillance capitalism that monitors the activity of colleagues and rivals {% cite brembsReplacingAcademicJournals2021 hansonUserTrackingAcademic2019 %}, has given publishers powerful control over the course of science, and they are more than happy to ride algorithmically amplified scientific hype cycles in fragmented research bubbles all the way to the bank.

One more turn of the screw: the ability of the (former) publishers to effectively invent the metrics that operationalize "prestige" in the absence of knowledge organization sytems gives them broad leverage with governments and funding agencies. In an environment of continuously dwindling budgets and legislative scrutiny, seemingly mutually beneficial platform contracts offer the sort of glossy comfort that only predictive analytics can. In 2020 the National Research Foundation of Korea (NRF) and Elsevier published a joint report that used a measurement derived from citation counts - "Field-weighted citation impact", or FWCI - to argue for the underrated research prestige of South Korea {% cite researchfoundationofkoreaSouthKoreaTechnological2020 %}. While I don't dispute the value of South Korea's research program, the apparent bargain that was struck is chilling. South Korea gets a very fancy report arguing that more scientists in other countries should work with theirs, and Elsevier gets to cement itself into the basic operation of science. Elsevier controls the journals that can guarantee high citation counts *and* the metrics built on top of them. The Brain Korea program Phase II report [^rand] {% cite seongBrainKorea212008 %}, issued just before the 2009 formation of the NRF argued that rankings and funding should be dependent on citation counts. The NRF now relies on SciVal and their FWCI measurement as a primary means of ranking researchers and determining funding, built into the Brain Korea 21 funding system {% cite elsevierCaseStudyNational2019 elsevierkoreaSciValHwalyongeulWihan2021 %}. Without exaggeration, scientific disorganization and reliance on citation counts allowed Elsevier to buy control over the course of research in South Korea.

The consequences for science are hard to overstate. In addition to literature search being an unnecessarily huge sink of time and labor,  science operates as a wash of tail-chasing results that only rarely seem to cumulatively build on one another. The need to constantly reinforce the norm that purposeful failure to cite prior work is research misconduct is itself a symptom of how engaging with a larger body of work is both extremely labor intensive and *strictly optional* in the communication regime of journal publication. The combination of more publications translating into more profit and the strategic disorganization of science contributes to conditions for scientific fraud. An entirely fraudulent paper can be undetectable even by domain experts. Since papers can effectively be islands --- given legitimacy by placement in a journal strongly incentivized to accept all comers --- and there is no good means of evaluating them in context with their immediate semantic neighbors, investigating fraud is extremely time consuming and almost entirely without reward. And since traditional peer review happens once, rather than as a continual public process, the only recourse outside of posting on PubPeer is to wait on journal editorial boards to self-police by reviewing each individual complaint. Forensic peer-reviewers have been ringing the alarm bell, saying that there is "no net" to bad research {% cite heathersRealScandalIvermectin2021 %}, and brave and highly-skilled investigators like [Elisabeth Bik](https://scienceintegritydigest.com/) have found thousands of papers with evidence of purposeful manipulation {% cite shenMeetThisSuperspotter2020 bikPrevalenceInappropriateImage2016 %}. The economic structure of for-profit journals pits their profit model against their function as providing a venue for peer review --- the one function most scientists are still sympathetic to. Trust in science is critical for addressing our most dire problems from global pandemics to climate change {% cite westMisinformationScience2021 %}, but attitudes towards scientists are lukewarm at best {% cite kennedyAmericansTrustScientists2022 %}. Even when it isn't fake news, why would anyone trust us when it's *effectively impossible*  to find or assess the quality of scientific information? {% cite krauseTrustFallacyScientists2021 %} Not even scientists can: despite the profusion of papers, by some measures progress in science has slowed to a crawl {% cite chuSlowedCanonicalProgress2021 %}. 

While Chu and Evans {% cite chuSlowedCanonicalProgress2021 %} correctly diagnose *symptoms* of knowledge disorganization like the need to "resort to heuristics to make continued sense of the field" and reliance on canonical papers, by treating the journal model as a natural phenomenon and citation as the only means of ordering research, they misattribute root *causes.* The problem is not people publishing *too many papers,* or a *breakdown of traditional publication hierarchies,* but the *staggering profitability of knowledge disorganization.* Knowledge disorganization is precisely the precondition of information-as-capital and the outcome of its concentration by our century's robber barons (see {% cite ellenwoodInformationHasValue2020 %}). Their prescription for "a clearer hierarchy of journals" misses the role of organizing scientific work in journals ranked by prestige, rather than by the content of the work, as a potentially major driver of extremely skewed citation distributions. It also misses the publisher's stated goals of *publishing more papers* within an ecosystem of algorithmic recommendations, and there is nothing recommendation algorithms love recommending more than things that are already popular. Without diagnosing knowledge disorganization as a core part of the business model of scientific publishers, we can be led to prescriptions that would make the problem worse.

It's hard to imagine an alternative to journals that doesn't look like, well, journals. While a full treatment of the journal system is outside the scope of this paper, the system we describe here renders them *effectively irrelevant* by making papers as we know them *unnecessary.* Rather than facing the massive collective action problem of asking everyone to change their publication practices on a dime, by reconsidering the way we organize the surrounding infrastructure of science we can flank journals and replace them "from below" with something qualitatively more useful. 

Beyond journals, the other technologies of communication that have been adopted out of need, though not necessarily design, serve as [desire paths](https://en.wikipedia.org/wiki/Desire_path) that trace other needs for scientific communication. As a rough sample: Researchers often prepare their manuscripts using platforms like Google Drive, indicating a need for collaborative tools in perparation of an idea. When working in teams, we often use tools like Slack to plan our work. Scientific conferences reflect the need for federated communication within subdisciplines, and we have adopted Twitter as a de facto platform for socializing and sharing our work to a broader audience. We use a handful of blogs and other sites like [OpenBehavior](https://edspace.american.edu/openbehavior/) {% cite whiteFutureOpenOpenSource2019 %}, [Open Neuroscience](https://open-neuroscience.com/), and many others to index technical knowledge and tools. Last but not finally, we use sites like [PubPeer](https://pubpeer.com) and ResearchGate for comment and criticism.

These technologies point to a few overlapping and not altogether binary axes of communication systems. 

- **Durable vs Ephemeral** - journals seek to represent information as permanent, archival-grade material, but scientific communication also necessarily exists as contextual, temporally specific snapshots.
- **Structured vs Chronological** - scientific communication both needs to present itself as a structured basis of information with formal semantic linking, but also needs the chronological structure that ties ideas to their context. This axis is a gradient from formally structured references, through intermediate systems like forums with hierarchical topic structure that embeds a feed, to the purely chronological feed-based social media systems.
- **Messaging vs Publishing** - Communication can be person-to-person, person-to-group with defined senders and recipients, or person-to-all statement to an undefined public. This ranges from private DMs through domain-specific tool indexes like OpenBehavior through the uniform indexing of Wikipedia.
- **Public vs. Private** - Who gets to read, who gets to contribute? Communication can be composed of entirely private notes to self, through communication in a lab, collaboration group, discipline, and landing in the entirely public realm of global communication. 
- **Formal vs. Informal** - Journal articles and encyclopedia-bound writing that conforms to a particular modality of expression vs. a vernacular style intended to communicate with people outside the jargon culture.
- **Push vs. Pull** - Do you go to get information from a reference location, or does information come to you as an alert or message? Or, generally, where is the information "located," is an annotation pushed and overlaid on a document, or stored elsewhere requiring the audience to explicitly pull it?

"Peer reviewed vs. unrefereed" is purposely excluded as an axis of communication tools, as the ability to review and annotate multiple versions of a document --- subject to the context of the medium --- should be a basic part of any communication system. Fear over losing the at once immutable but also paradoxically fragile ecosystem of journal-led peer review is one of the first strawmen that stops consideration of radically reorganizing scientific communication[^quittingreviewing]. The belief that peer review as we know it is an intrinsic part of science is ahistorical (eg. {% cite baldwinScientificAutonomyPublic2018 %}), and the belief that journal-led peer review is somehow a unique venue for evaluating scientific work ignores the immense quantity of criticism and discussion that happens in almost every communicative context, scientific and otherwise. The notion that the body of scientific knowledge is best curated by passing each paper through a gauntlet of three anonymous reviewers, after which it becomes Fact is ridiculous on its face. Focusing on preserving peer review is a red herring that unnecessarily constrains the possible forms of scientific communication. Instead we will try and sketch systems that address the needs for communication and knowledge organization left unmet precisely because of the primacy of peer reviewed journal publications.

[^quittingreviewing]: For a recent example, see the responses to Dan Goodman's argument why he has stopped doing pre-publication peer review altogether {% cite dangoodman[neuralreckoning]CurrentSystemJournals2022 goodmanEndingSupportLegacy2022 %}

Clearly a variety of different types of communication tools are needed, but there is no reason that each of them should be isolated and inoperable with the others. We have already seen several of the ideas that help bring an alternative into focus. Piracy communities demonstrate ways to build social systems that can sustain distributed infrastructure. Federated and protocol-based systems show us that we don't need to choose between a single monolithic system or many disconnected ones, but can have a heterogeneous space of tools linked by a basic protocol. The semantic web gives us the unfulfilled promise of triplet links as a very general means of structuring data and building interfaces for disparate systems. We can bridge these lessons with some from wiki culture to get a more practical sense of distributed governance and organization. Together with our sketches of data, analytical, and experimental tools we can start imagining a system for coordinating them --- as well as displacing some of the more intractable systems that misstructure the practice of science.
