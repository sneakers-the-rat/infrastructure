
Continuing the example of the Autopilot wiki, we could make an array of **technical knowledge wikis.** Wikis organized around individual projects could federate together to share information, and broader wikis could organize the state of our art which currently exists hollowed out in supplemental methods sections. The endless stream of posts asking around for whoever knows how to do some technique that should be basic knowledge for a given discipline illustrate the need. Across disciplines, we are drenched in widely-used instrumentation and techniques without coherent means of discussing how we use them. Organizing the technical knowledge that is mostly hard-won by early career researchers without robust training mechanisms would dramatically change their experience in science, whittling away at inequities in access to expertise. Their use only multiplies with tools that are capable of using the semantically organized information to design interface or simplify their operation as described in [experimental frameworks](#experimental-frameworks).

Technical wikis could change the character of technical work. By giving a venue for technical workers to describe their work, they would be welcomed into and broaden the base of credit currently reserved only for paper authors. Even without active contribution, they would be a way of describing the unseen iceberg of labor that science rests on. Institutional affiliations are currently just badges of prestige, but they could also represent the dependence of scientific output on the workers of that institution. If I do animal research at a university, and someone has linked to the people responsible for maintaining the animal facility, then they should be linked to all of my work. Making technical knowledge broadly available might also be a means of inverting the patronizing approach to "crowdsourcing" "citizen science" by putting it directly in the hands of nonscientists, rather than at the whim of some gamified platform (see {% cite delangeShortTimeBig2022 %}).

Technical wikis blend smoothly into **methods wikis** for cataloguing best practices in experimental design and analysis. It is a damning indictment of our systems of training or review (or, more likely, both) that it is possible to publish a paper based on badly misused t-tests, yet the scientific literature is flooded with analytical and interpretive errors {% cite strasakStatisticalErrorsMedical2007 brownIssuesDataAnalyses2018 leekStatisticsValuesAre2015 %}. Analytical errors are not just a matter of lack of education, but also a complex network of incentives and disciplinary subcultures. Having the ability to discuss and contextualize different analytical methods elevates all the exasperated methods critiques and exhortations to "not use this technique that renders meaningless results" into something *structurally expressed in the practice of science.* See the `@methodswiki` page that summarizes this general category of techniques and the discussion surrounding their application in the relevant body of research. For implementation of analytical libraries, to move beyond fragile code reduplicated in every lab we need some means of reaching fluid consensus on a set of quasi-canonical implementations of fundamental analysis operations. Given a system where analysis chains are linked to the data they are used with, that consensus might come by negotiating over a semantically dense map of the analysis paths used in a research domain. 

**Analysis wikis** would also be a natural means of organizing the previously mentioned Folding@Home-style distributed computing grids. Groups of researchers could organize computational resources and govern and document their use. For example, a tracker could implement a "compute ratio" where donated computing resources function as credit for "bounties." Analogously to private torrent trackers, where a bounty system might allow peers to trade their excess upload in exchange for someone uploading a rare album, linked tracker/wikis could translate that model to one where someone who has donated a lot of excess compute time could trade it for someone uploading or collecting a particular dataset. Since the kind of wikis we are describing combine free text with computer-readable data structures, policies for use could be directly implemented in the wiki in the same place they were discussed. This too is a means of collectivizing support for open-source initiatives that support basic infrastructure by donation and the mercy of cloud providers by integrating them in the basic social practices of science {% cite dupreAdvertisingNewInfrastructures2022 %}. 

**Review wikis** could replace journals almost as an afterthought. Though an adequate infrastructure of scientific communication immediately antiquates traditional peer review, review wikis could facilitate it without recourse to an extractive information industry. In response to the almost unique profitability of publishing, some researchers have reacted, perhaps justifiably, by demanding payment for their reviews (eg. {% cite heathers450Movement2020 %}). An alternative might be to organize review *ourselves.* Like the ratio requirements of private bittorrent trackers, we might establish a review ratio system, where for every review your work receives you need to review n other works. This would effectively function as a **reviewer co-op** that can make the implicit labor of reviewing explicit, and tie the reviews required for frequent publication with explicit norms around reciprocal reviewing. 

**Library wikis** focused on curation, contextualization, and organization of information could be one modality of resisting the neoliberal drive to reduce librarians to stewards of subscriptions and surveillance data {% cite lamdanLibrarianshipCrossroadsICE2019 quinnResistingNeoliberalismChallenge2017 %}. Knowledge organization is hard practical and theoretical work, and reimagining the space of scientific communication as one that we actively *create* instead of one that we merely *suffer through* is a wide-open invitation for the comradeship and leadership of librarians. Linked data has been a mixed blessing for librarians, its promise obscured by intellectual property oligopolies and the complexity of linked data standards (see {% cite librariaStoningGoliath2022 %}). Given fresh tooling and a path away from structuring influence of for-profit publishers, the rest of us should be prepared to learn from those that have already been doing the work of curating our archives: 

> [M]ake it easy to rely on linked data, easier than it is to rely on MARC, and the library world will shift, from the smallest and poorest libraries upwardâ€¦ and David will at last stone Goliath to death with his linked-data slingshot. 
>
> [*Stoning Goliath*](https://gavialib.com/2022/06/stoning-goliath/) (2022) The Library Loon {% cite librariaStoningGoliath2022 %}

Finally, **theory wikis** could "close the theoretical-experimental loop" to turn the buckshot of results into cumulative understanding of complex phenomena. In many (or maybe just the non-realist) scientific epistemologies, results do not directly reflect some truth about reality, but instead are embedded in a system of meaning through a process of active interpretation (eg. {% cite meehlTheoreticalRisksTabular1978 cartwrightHowLawsPhysics1983a %}). The model of grounding new research in existing understanding given by contemporary regimes of scientific communication is for each paper to synthesize and re-interpret the entire body of relevant prior research (formally, the "introduction"), which is bluntly impossible. We do the best we can alongside strong countervailing incentives to selectively engage with work in order to tell a publishable story in which we are the hero. Since the space of argumentation is built from scratch each time, cumulative progress on a shared set of theories is more of a myth for undergraduate introductions to the scientific method than a reality. Most fall far from the supposed ideal of hard refutation and can have long lives as "zombie theories." van Rooij and Baggio describe the "collecting seashells" approach of gathering many results and leaving the theory for later with an analogy:

> "In a sense, trying to build theories on collections of effects is much like trying to write novels by collecting sentences from randomly generated letter strings. Indeed, each novel ultimately consists of strings of letters, and theories should ultimately be compatible with effects. Still, the majority of the (infinitely possible) effects are irrelevant for the aims of theory building, just as the majority of (infinitely possible) sentences are irrelevant for writing a novel." {% cite vanrooijTheoryTestHow2021 %}

They and others (eg. {% cite guestHowComputationalModeling2021 %}) have argued for an iterative process of experiments informed by theory and modeling that confirm or constrain future models. Their articulation of the need for multiple registers of formality and rigidity is particularly resonant here. van Rooij and Baggio again, emphasis mine:

> **We should interpret any data in the context of our larger "web of beliefs,"** which may contain anything we know or believe about the world, including scientific or commonsense knowledge. One does not posit a function *f* in a vacuum. [...] One can either cast the net wide to capture intuitive phenomena and refine and formalize the idea in a well-defined *f* or, alternatively, make a first guess and then adjust it gradually on the basis of the constraints that one later imposes: The first sketch of an *f* need not be the final one; what matters is how the initial *f* is constrained and refined and how the rectification process can actually drive the theory forward. **Theory building is a creative process involving a dialectic of divergent and convergent thinking, informal and formal thinking.** {% cite vanrooijTheoryTestHow2021 %}

Durable but plastic, referential and dialogic, structured and free mediums like our wiki-trackers could be a practical means of integrating theory in a loop with experimentation and interpretation. Many theories are formalizable, and our linked data system is a relatively arbitrary means of expressing complex constraints and inference logics. Others are not, and our mixed-format media also supports the dialectic of informal and formal, mathematized and non-mathemetized theories. 

In the most optimistic case, where we have a full provenance chain from interpretation of analytical results back through the viscera of their acquisition, we have a living means of formally evaluating the empirical contingencies that serve as the evidence for scientific theories. For a given theory, what kinds of evidence exist? As the state of the art in analytical tooling changes, how are the interpretations of prior results changed by different analyses? How do different experimental methodologies influence the form of our theories? 

The points of conflicting evidence and unevaluated predictions of theory are then a means of distributed coordination of future experiments: guided by a distributed body of evidence and interpretation, rather than the number of papers individual researchers are able to hold in mind, what are the most informative experiments to do? This would be a fundamentally different way of approaching a new "unit" of scientific work that dissolves the scientific paper as such. Many calls for smaller units of scientific work amount to faster turnaround for shorter papers that preserve the unitary binding of an experiment, results, and interpretation. Instead new experiments could start *in medias res,* filling in some cracks in an ongoing experimental/interpretational network. A new node could be contributed already contextualized by the "introduction" of its position in a broader graph of understanding, its interpretation posed against a broader background of prior thought than the immediate data at hand. Given the means of directly applying accumulated technical knowledge, it would be possible for more than just the most resourced labs to be responsive to the nicks and burrs in the cutting edge.

The pessimistic case where we only have scientific papers in their current form to evaluate is not that much worse --- it requires the normal reading and evaluation of experimental results of a review paper, but the process of annotating the paper to describe its experimental and analytical methods as a shared body of links makes that work cumulative. Even more pessimistic, where for some reason we aren't able to formulate theories even as rough schematics but just link experimental results to rough topic domains is still vastly better than the current state of proprietary disorganization in service of a surveillance-backed analytics industry. 

A meta-organization of experimental results would change the way researchers and non-researchers alike interact with academic literature. It currently takes many years of implicit knowledge to understand any scientific subfield: finding canonical papers, knowing which researchers to follow, which keywords to search in table of contents alerts. Being able to locate a question in a continuous space of discussion, data, results, and theories --- to say nothing of building a world without paywalls --- would profoundly lower barriers to access to primary scientific knowledge for *everyone.* We might avoid the efforts to weaponize this gap into an ostensibly "helpful" algorithmic search platform that re-entrenches the very industries that make such a platform necessary by constraining the modes of our communication. We might instead arrive at a fluid, boisterous, collective project of explicitly organizing understanding. One sounds like science, the other sounds like industry capture.