> Having become a dense and consistent historical reality, language forms the locus of tradition, of the unspoken habits of thought, of what lies hidden in a people’s mind; it accumulates an ineluctable memory which does not even know itself as memory. Expressing their thoughts in words of which they are not the masters, enclosing them in verbal forms whose historical dimensions they are unaware of, men believe that their speech is their servant and do not realize that they are submitting themselves to its demands. 
>
> Michel Foucault --- *The Order of Things* {% cite foucaultOrderThings2001 %}

There is no shortage of databases for scientific data, but their traditional structure chokes on the complexity of representing multi-domain data. Typical relational databases require some formal schema to structure the data they contain, which have varying reflections in the APIs used to access them and interfaces built atop them. This broadly polarizes database design into domain-specific and domain-general[^trackeranalogy]. This design pattern results in a fragmented landscape of databases with limited interoperability. How shall we link the databases? In this section we'll consider the Icarian promise of creating the great unified database of everything as a way of motivating an alternative that blends *linked data* {% cite berners-leeLinkedData2006 %} with *federated systems* against our peer to peer backbone in the next section.

[^trackeranalogy]: To continue the analogy to bittorrent trackers, an example domain-specific vs. domain-general dichotomy might be What.cd (with its specific formatting and aggregation tools for representing artists, albums, collections, genres, and so on) vs. ThePirateBay (with its general categories of content and otherwise search-based aggregation interface)

Domain-specific databases require data to be in one or a few specific formats, and usually provide richer tools for manipulating and querying by metadata, visualization, summarization, aggregation that are purpose-built for that type of data. For example, NIH's [Gene](https://www.ncbi.nlm.nih.gov/gene/12550) tool has several visualization tools and cross-referencing tools for finding expression pathways, genetic interactions, and related sequences (Figure xx). This pattern of database design is reflected at several different scales, through institutional databases and tools like the Allen [brain atlases](https://connectivity.brain-map.org/) or [observatory](http://observatory.brain-map.org/visualcoding/), to lab- and project-specific dashboards. This type of database is natural, expressive, and powerful --- for the researchers they are designed for. While some of these databases allow open data submission, they often require explicit moderation and approval to maintain the guaranteed consistency of the database, which can hamper mass use.

![An example specialized plot of genomic regions, transcripts and products for the CDH1 gene (linked above), showing how specific tools have been built for this specific dataset](/infrastructure/assets/images/nih_gene_cdh1.png)
*NIH's Gene tool included many specific tools for visualizing, cross-referencing, and aggregating genetic data. Shown is the "genomic regions, transcripts, and product" plot for Mouse Cdh1, which gives useful, common summary descriptions of the gene, but is not useful for, say, visualizing reading proficiency data.*

General-purpose databases like [figshare](https://figshare.com/) and [zenodo](https://zenodo.org/)[^yrcool] are useful for the mass aggregation of data, typically allowing uploads from most people with minimal barriers. Their general function limits the metadata, visualization, and other tools that are offered by domain-specific databases, however, and are essentially public, versioned, folders with a DOI. Most have fields for authorship, research groups, related publications, and a single-dimension keyword or tags system, and so don't programmatically reflect the metadata present in a given dataset.

[^yrcool]: No shade to Figshare, which, among others, paved the way for open data and are a massively useful thing to have in society. 

The dichotomy of fragmented, subdomain-specific databases and general-purpose databases makes combining information from across even extremely similar subdisciplines combinatorically complex and laborious. In the absence of a formal interoperability and indexing protocol between databases, even *finding* the correct subdomain-specific database often comes down to pure luck. It also puts researchers who want to be good data stewards in a difficult position: they can hunt down the appropriate subdomain specific database and risk general obscurity; use a domain-general database and make their work more difficult for themselves and their peers to use; or spend all the time it takes to upload to multiple databases with potentially conflicting demands on format. 

What can be done? There are a few naïve answers from standardizing different parts of the process: If we had a universal data format, then interoperability becomes trivial. Conversely, we could make a single ur-database that supports all possible formats and tools. 

The notion of a universal database system almost immediately runs aground on the reality that organizing knowledge is intrinsically political. Every subdiscipline has conflicting *representational* needs, will develop different local terminology, allocate differing granularity and develop different groupings and hierarchies for the same phenomena. At their mildest, differences in representational systems can be incompatible, but at their worst they can reflect and reinforce prejudices and become the site of expression for intellectual and social power struggles {% cite joLessonsArchivesStrategies2020 selbstFairnessAbstractionSociotechnical2019 gebruDatasheetsDatasets2021 bowkerSortingThingsOut1999 %}. Every subdiscipline has conflicting *practical* needs, with infinite variation in privacy demands, different priorities between storage space, bandwidth, and computational power, and so on. In all cases the boundaries of our myopia are impossible to gauge: we might think we have arrived at a suitable schema for biology, chemistry, and physics... but what about the historians?

Matthew J Bietz and Charlotte P Lee articulate this tension in their ethnography of metagenomics databases:

> "Participants describe the individual sequence database systems as if they were shadows, poor representations of a widely-agreed-upon ideal. We find, however, that by looking across the landscape of databases, a different picture emerges. Instead, **each decision about the implementation of a particular database system plants a stake for a community boundary. The databases are not so much imperfect copies of an ideal as they are arguments about what the ideal Database should be.** [...]
>
> In the end, however, **the system was so tailored to a specific set of research questions that the collection of data, the set of tools, and even the social organization of the project had to be significantly changed.** New analysis tools were developed and old tools were discarded. Not only was the database ported to a different technology, the data itself was significantly restructured to fit the new tools and approaches. While the database development projects had begun by working together, in the end they were unable to collaborate. **The system that was supposed to tie these groups together could not be shielded from the controversies that formed the boundaries between the communities of practice.**" {% cite bietzCollaborationMetagenomicsSequence2009 %}

The pursuit of unified representation is an intimate part of the history of linked data, which relies on "ontologies" or controlled vocabularies that describe a set of objects (or classes) and the properties they can have. For example, [schema.org](https://schema.org) maintains a widely used set of hierarchical vocabularies to describe the fundamental things that exist in the world, in particular the unfamiliar world in which a [Person](https://schema.org/Person) has a [gender](https://schema.org/gender) and [net worth](https://schema.org/netWorth) but lacks a race {% cite poirierTurnScruffyEthnographic2017 %}. At one extreme in the world of ontology builders, the ideological nature of demarcating what is allowed to exist is as clear as a klaxon (emphasis in original):

> An exception is the Open Biomedical Ontologies (OBO) Foundry initiative, which accepts under its label only those ontologies that adhere to the principles of ontological realism. [...] Ontologies, from this perspective, are representational artifacts, comprising a taxonomy as their central backbone, whose representational units are intended to designate *universals* (such as *human being* and *patient role*) or *classes defined in terms of universals* (such as *patient,* a class encompassing *human beings* in which there inheres a *patient role*) and certain relations between them. [...]
>
> BFO is a realist ontology [15,16]. This means, most importantly, that representations faithful to BFO can acknowledge only those entities which exist in (for example, biological) reality; thus they must reject all those types of putative negative entities - lacks, absences, non-existents, possibilia, and the like {% cite ceustersFoundationsRealistOntology2010 %}

In practice, because of the difficulty of changing the representation and encompassing database systems on a dime, using these ontologies to link disparate datasets tends to follow the pattern of metadata *overlays* where the structure of individual databases are mapped onto one "unifying" ontology to allow for aggregation and translation. This approach appears gentler than standardization at the level of individual databases, but has the same problems kicked up one level of abstraction.

To concretize the problems with a globally unified database or metadata overlay, the remainder of this section will trace the compromises and outcomes of the The NIH's "Biomedical Data Translator" project. The Translator project was initially described in the 2016 Strategic Plan for Data Science as a means of translating between biomedical data formats:

> Through its Biomedical Data Translator program, the National Center for Advancing Translational Sciences (NCATS) is supporting research to develop ways to connect conventionally separated data types to one another to make them more useful for researchers and the public. {% cite NIHStrategicPlan2018 %}

The original [funding statement from 2016](https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator) is similarly humble, and press releases [through 2017](https://web.archive.org/web/20210709171335/https://ncats.nih.gov/pubs/features/translator) also speak mostly in terms of querying the data -- though some ambition begins to creep in. By 2019, the vision for the project had veered sharply away from anything a basic researcher might recognize as a means of translating between data types. In their piece "Toward a Universal Biomedical Translator," then in a feasibility assessment phase, the members of the Translator Consortium assert that universal translation between biomedical data is impossible[^impossibledata]{% cite consortiumUniversalBiomedicalData2019 %}. The impossibility they saw was not that of conflicting political demands on the structure of organization (as per {% cite bowkerSortingThingsOut1999 %}), but of the sheer numeracy of the data and vocabularies needed to describe them. The risk posed by a lack of a universal "language" was not being able to index all possible data, rather than inaccuracy or inequity. 

Undaunted by their stated belief in the impossibility of a universalizing ontology, the Consortium created one in their [biolink](https://biolink.github.io/biolink-model/docs/) model {% cite bruskiewichBiolinkBiolinkmodel2021 unniBiolinkModelUniversal2022 %}. Biolink consists of a hierarchy of basic classes: eg. a [BiologicalEntity](https://biolink.github.io/biolink-model/docs/BiologicalEntity.html) like a [Gene](https://biolink.github.io/biolink-model/docs/Gene.html), or a [ChemicalEntity](https://biolink.github.io/biolink-model/docs/ChemicalEntity.html) like a [Drug](https://biolink.github.io/biolink-model/docs/Drug.html). Classes can then linked by any number of properties, or "Slots," like a therapeutic procedure that [treats](https://biolink.github.io/biolink-model/docs/treats.html) a disease. 

The translator does not attempt to respond to the needs of researchers or labs who might want to link their raw data splayed out across flash drives and file structures whose chaos borders on whimsy. Instead, the Translator operates at the level of "knowledge," or "generally accepted, universal assertions derived from the accumulation of information" {% cite fechoProgressUniversalBiomedical2022 %}. Rather than translating *between data types*, the meaning of "translation" shifted to meaning *"translating data into knowledge"* {% cite consortiumUniversalBiomedicalData2019 %}. 

To feed the Translator, Biolink sits "on top of" a [collection of database APIs](http://www.smart-api.info/registry) that serve structured biomedical data, each called a "knowledge source." Individual APIs [declare](https://github.com/NCATSTranslator/ReasonerAPI) that they are able to provide data for a particular set of classes or slots, like [drugs that affect genetic expression](http://www.smart-api.info/ui/adf20dd6ff23dfe18e8e012bde686e31), and are then made browsable from the [SmartAPI Knowledge Graph](http://www.smart-api.info/portal/translator/metakg). Queries to individual APIs do not return "raw" data, but return assertions of fact in the parlance of the Biolink model: this procedure treats that disease, etc. 

Because individual researchers do not typically represent their data in the form of factual assertions, knowledge sources are constrained to "highly curated biomedical databases" or other aggregated systems. The NIH RePORTER tool [gives an overview](https://reporter.nih.gov/search/DShVUhB_ZUq0X5UWFjy5WQ/projects?shared=true) of the way these knowledge sources are prepared when none already exist for a given Biolink class or predicate: automated [text mining](https://reporter.nih.gov/project-details/10548337) tools and a series of [domain-specific data provider](https://reporter.nih.gov/project-details/10056962) projects, rather than via tools provided to researchers. 

The collection of knowledge sources, linked to nodes and edges in the Biolink model, are designed to be queried as a graph. To answer a query like "what drug treats this disease?" the translator considers the graph of entities linked to the disease: what symptoms does the disease have? what genes are linked to those symptoms? which drugs act on those genes? and so on {% cite renaissancecomputinginstituterenciBiomedicalDataTranslator2022 %}. The form of the Translator as a graph-based question answering machine bounds its application as a platform for researchers to guide their research and clinicians to guide their care {% cite hailuNIHfundedProjectAims2019 %}, rather than a tool for linking data.

One primary example currently featured by NCATS is using the translator to propose novel treatments for drug-induced liver injury (DILI) {% cite renaissancecomputinginstituterenciUseCasesShow2022 %} detailed in a 2021 conference paper {% cite goelExplanationContainerCaseBased2021 %}. To find a candidate drug, the researchers manually conducted three API queries: first they searched for phenotypes associated with DILI and selected "one of them"[^dilisearch] --- "red blood cell count". Then they queried for genes associated with red blood cell count to find telomerase reverse transcriptase (TERT), and then finally for drugs that affect TERT to find Zidovudine. The directionality of each of these relationships, high vs. low, increases vs. decreases, is unclear in each case. A more recent report on the Translator repeated this pattern of manual querying, arriving at a handful of different genes and drugs {% cite fechoProgressUniversalBiomedical2022 %}. 

While the current examples are highly manual, providing an array of results for each query along with links to associated papers on pubmed, some algorithmic system for ranking results is necessary to make use of the information in the extended knowledge graph. Rather than just the first-order connections, it should be possible to make use of second, third, and n-th order connections to weight potential results. Algorithmic medical recommendation systems have been thoroughly problematized elsewhere (eg. {% cite groteEthicsAlgorithmicDecisionmaking2020 obermeyerDissectingRacialBias2019 panchArtificialIntelligenceAlgorithmic2019 panchInconvenientTruthAI2019 %}). The primary ranking algorithm is developed by a defense contractor (CoVar) who has[^unironically] named it ROBOKOP {% cite mortonROBOKOPAbstractionLayer2019 %}[^fine]. Though ROBOKOP functions with a simple weighted graph metric based on citations and abstract text, the ranking system is intended to be extended with machine learning tools {% cite mortonROBOKOPAbstractionLayer2019 %} that can be trained based on the way the provided answers are used {% cite consortiumUniversalBiomedicalData2019 %}.
Algorithmic recommendation platforms are in a regulatory gray area {% cite ordishAlgorithmsMedicalDevices2019 el-sayedMedicalAlgorithmsNeed2021 %}, but would arguably need to have interpretable results with clear provenance to pass scrutiny. The DILI example uses a language model which explained the recommendation of Zidovudine with all the clarity of "one of 'DOWNREGULATOR,' 'INHIBITOR,' 'INDIRECT DOWNREGULATOR'." 

[^unironically]: seemingly unironically

[^fine]: which seems totally fine and normal.

The arrival at a biomedical question answering platform built atop an algorithmic ranking system for a knowledge graph that queries  200+ aggregated data sources has several qualities that should give us pause.

First, as with any machine-learning based system, the algorithm can only reflect the implicit structure of its creation, including the beliefs and values of its architects {% cite birhaneValuesEncodedMachine2022 birhaneAlgorithmicInjusticeRelational2021 %}, its training data and accompanying bias {% cite birhaneMultimodalDatasetsMisogyny2021 %}, and so on. The "mass of data" approach ML tools lend themselves to, in this case, querying hundreds of independently operated databases, makes dissecting the provenance of every entry from every data provider effectively impossible. For example, one of the providers, [mydisease.info](https://mydisease.info) was more than happy to respond to a query for the outmoded definition of "transsexualism" as a disease {% cite ramTransphobiaEncodedExamination2021 %} along with a list of genes and variants that supposedly "cause" it - [see for yourself](http://mydisease.info/v1/query?q=%22DOID%3A10919%22). At the time of the search, tracing the source of that entry first led to the disease ontology [DOID:1234](https://web.archive.org/web/20211007053446/https://www.ebi.ac.uk/ols/ontologies/doid/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FDOID_1234) which traced back into an entry in a graph aggregator [Ontobee](http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234) ([Archive Link](https://web.archive.org/web/20210923110103/http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234)), which in turn listed this [github repository](https://github.com/jannahastings/mental-functioning-ontology) **maintained by a single person** as its source[^ipredit]. This is, presumably, the fragility and inconsistency in input data that the machine learning layer is intended to putty over.

[^ipredit]: I submitted a [pull request](https://github.com/jannahastings/mental-functioning-ontology/pull/8) to remove it, but it has not been merged more than 8 months later. A teardrop in the ocean. 

If the graph encodes being transgender as a disease, it is not farfetched to imagine the ranking system attempting to "cure" it. In a seemingly prerelease version of the translator's query engine, ARAX, it does just that: in [a query for entities with a `biolink:treats` link to gender dysphoria](https://arax.rtx.ai/?r=e891e6e6-44fd-4684-9d36-f94e3e81b554)[^araxtrans], it ranks the standard therapeutics {% cite deutschOverviewFeminizingHormone2016 deutschOverviewMasculinizingHormone2016 %} Testosterone and Estradiol 6th and 10th of 11, respectively --- behind a recommendation for Lithium (4th) and Pimozide (5th) due to an automated text scrape of [two](https://pubmed.ncbi.nlm.nih.gov/2114800/) conversion therapy [papers](https://pubmed.ncbi.nlm.nih.gov/8839957/)[^dateextract]. Queries to ARAX for [treatments for gender identity disorder](https://arax.ncats.io/?r=52703) helpfullly yielded "zinc" and "water," offering a paper from the translator group that describes automated drug recommendation as the only provenance {% cite womackLeveragingDistributedBiomedical2019 %}. A query for treatments for `DOID:1233` "[transvestism](https://arax.rtx.ai/?r=81249a42-b300-4dcf-94c9-7a9fe2f78237)" was predictably troubling.


Even if the curators do their best to prevent harmful queries and block searches for "cures" to being trans, the graph-based nature of the system means that any given entry will have unpredictable consequences on recommendations made from the surrounding network of objects like genes, treatment history, and so on. If the operation of the ranking algorithm is uninterpretable, as most are, or the algorithm it itself proprietary, harmful input data could have long-range influence on both the practice of medicine as well as the course of basic research *without anyone being able to tell.* The Consortium also describes a system whereby the algorithm is continuously updated based on usage of results in research or clinical practice {% cite consortiumUniversalBiomedicalData2019 %}, which stands to magnify the problem of algorithmic bias by uncritically treating harmful treatment and research practices as training data.

[^dateextract]: as well as a recommendation for "date allergenic extract" from a misinterpretation of "to date" in the abstract of [a paper](https://pubmed.ncbi.nlm.nih.gov/24330520/) that reads "Cross-sex hormonal treatment (CHT) used for gender dysphoria (GD) could by itself affect well-being without the use of genital surgery; however, **to date,** there is a paucity of studies investigating the effects of CHT alone"

[^araxtrans]: To its credit, ARAX does transform the request for `DOID:10919` to `MONDO:0001153` - gender dysphoria.


The approach creates a fundamental tradeoff between algorithmic interpretability and the system being useful at all. The paper cited in the 2021 DILI example as evidence that the system gives plausible results is for a specific subclass of liver injuries caused by anti-tuberculosis drugs {% cite udomsinprasertLeukocyteTelomereLength2020 %}, highlighting the danger of automated recommendations from noisy data, but also calling into question what novel contribution the Translator made if telomeres were already implicated in DILI. The 2022 report gives examples where the results were already expected by the researchers, or provided a series of papers that seems difficult to imagine being much more informative than a PubMed search. If the algorithmic recommendations are unexpected --- ie. the system provides novel information --- the process of confirming them appears to be near-identical to the usual process of reading abstracts and hopping citation trees.

Perhaps most worrisome is the eventual fate of the project in the hands of the broader ecosystem of orbiting information conglomerates. Centralized infrastructure projects can be an opportunity for for-profit companies to "dance until the music stops" and then scoop up any remaining technology when the funding dries up (so far roughly [$81.6 million](https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear&sort_order=desc) since 2016 for the Translator {% cite RePORTRePORTERBiomedical2021 %}, and [$84.7 million](https://reporter.nih.gov/search/H4LxgMGK9kGw6SeWCom85Q/projects?shared=true) for the discontinued NIH Data Commons pilot which morphed into the STRIDES program). I have little doubt that the scientists and engineers working on the Translator are doing so with the best of intentions --- the real question is what happens to it after it's finished.

Knowledge graphs in particular are promising targets for platform holders. Perhaps the most well known example is Google's 2010 acquisition of Freebase (via Metaweb) {% cite subramanianGoogleBuysFreebase2010 %}, a graph of structured data with a wealth of properties for common people, places and things. Google incorporated it into their Knowledge Graph {% cite IntroducingKnowledgeGraph2012 %} to populate its factboxes and make its search results more semantically aware in its Hummingbird upgrade in 2013, the largest overhaul of its search engine since 2001 {% cite sullivanFAQAllNew2013 %}, cementing its dominance as a search engine. The connection between swallowing up knowledge organization systems into search engines is not incidental, but reflective of the broader pattern of enclosing basic digital infrastructure behind opaque platforms. Searching has a different set of cognitive expectations than browsing a database: we expect search results to be "best effort," not necessarily complete or accurate, where when browsing a database it's relatively clear when information is missing or inaccurate. For products packaged up into search platforms by for-profit companies, *it doesn't have to actually work* as long as it seems like it does.

The platformatization of the knowledge graph, along with carefully worded terms of service, is a clean means by which "good enough" results could be jackknifed into an expanded system of biomedical surveillance. Since the algorithm needs continual training, the translator has every incentive to suck up as much personal data as it can[^personalmedicaldata]. For-profit platform providers as a rule depend on developing elaborate personal profiles for targeted advertising algorithmically inferred from available data[^googlepatent], that naturally includes diagnosed or inferred disease --- a practice they explicitly describe in the patents for the targeting technology{% cite bharatGeneratingUserInformation2005 %}, have gone to court to defend {% cite SmithFacebookInc2018 krashinskyGoogleBrokeCanada2014 %}, and formed secretive joint projects with healthcare systems to pursue {% cite bourreauGoogleFitbitWill2020 %}.

[^personalmedicaldata]: A 2020 presentation in one of the Translator's [github repositories](https://github.com/NCATSTranslator/Translator-All) describes methods for mining individual clinical data {% cite translatorconsortiumClinicalDataServices2020 %}

So while an algorithmic recommendation tool may have limited use for the basic researchers it was originally intended for, it is likely to be extremely useful for the booming business of "personalized medicine.[^personalizedmed]" Linking biomedical and patient data in a single platform is a natural route towards a multisided market where records management apps are sold to patients, treatment recommendation systems are sold to clinicians, research tools and advertising opportunities are sold to pharmaceutical companies, risk metrics are sold to insurance companies, and so on. 

[^personalizedmed]: The researchers in the Translator project are clear that they do not intend it as a tool for personalized medicine, but again we are describing unintended downstream consequences by different actors than the creators.

Multiple information conglomerates are poised to capitalize on the translator project. Amazon already has a broad home surveillance portfolio {% cite bridgesAmazonRingLargest2021 %}, and has been aggressively expanding into health technology {% cite AWSAnnouncesAWS2021 %} and even literally providing [health care](https://amazon.care/) {% cite lermanAmazonBuiltIts2021 %}, which could be particularly dangerous with the uploading of all scientific and medical data onto AWS with entirely unenforceable promises of data privacy through NIH's STRIDES program {% cite quinnYouCanTrust2021 %}. 

RELX, parent of Elsevier, is as always the terrifying elephant in the room. In addition to distribution rights for a large proportion of scientific knowledge and a collection of research databases, it also sells a clinical reference platform in ClinicalKey, point of service products for planning patient care with ClinicalPath, medical education tools, and pharmaceutical advertisements designed to look like scientific papers {% cite elsevier360AdvertisingSolutions %}, among others {% cite relx2021AnnualReport2021 %}. It also is explicitly expanding into "clinical decision support applications" {% cite relx2021AnnualReport2021 %} and recently embedded its medication management product into Apple's watchOS 9 {% cite appleWatchOSDeliversNew2022 %}. Subsidiaries in RELX's "Risk" market segment sell risk profiles to insurance companies based on what they claim to be highly comprehensive profiles of harvested personal data. The Translator infrastructure is a perfect keystone to unify these products: after the NIH fronts the money to develop it and lends the credibility of basic research, RELX can cheaply expand its surveillance apparatus to enhanced medical risk profiles to insurers, priority placement in candidate drug rankings to pharmaceutical companies, and augment its ranking systems for funders and employers to include some proprietary metric of "promisingness" to encourage researchers to follow its research recommendations. This isn't speculative --- it can just strap whatever clinical data Translator gains access to into its [existing biomedical knowledge graph](https://www.elsevier.com/solutions/biology-knowledge-graph).

Even assuming the Translator works perfectly and has zero unanticipated consequences, the development strategy still reflects the inequities that pervade science rather than challenge them. Biopharmaceutical research, followed by broader biomedical research, being immediately and extremely profitable, attracts an enormous quantity of resources and develops state of the art infrastructure, while no similar infrastructure is built for the rest of science, academia, and society. 

The eventual form of the Translator follows from a series of decisions centered around the intended universality of the system. From the [funding statement](https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator) in 2016, the system was conceptualized as an "informatics platform" intended to "bring together all biomedical and health data types."  The surrounding background of cloud-based database storage imagined by the Strategic Plan for Data Science immediately constrained the design to consist of APIs that served small quantities of aggregated data, rather than potentially large quantities of raw data. Together with a platform, rather than tool-based approach, a system that allowed individual researchers to link and make sense of the subtlety of own their data was precluded from the start.

From these constraints, the form of the BioLink model comes into focus: high-level classes and logical relationships between them as asserted by a large number of separate knowledge sources. Since the data from each of these sources is heterogeneous, relatively uncurated, and potentially numerous for any given graph-based query, the need for a machine learning layer to make sense of it follows. The conceptualization of BioLink as a universal ontology seems to follow the lineage of the "neat" thought style {% cite poirierTurnScruffyEthnographic2017 %} that emphasizes "deductive inference through logical rules" {% cite unniBiolinkModelUniversal2022 %} or otherwise computing derived information from the structure of the knowledge graph rather than browsing the graph itself. Together, these constraints and design logics bring us to the form of the Translator as a graph-based query engine.

The Translator Consortium justifiably takes pride in its social organizing systems {% cite consortiumBiomedicalDataTranslator2019 %} --- coordinating 200 researchers and engineers from dozens of institutions is no small feat. This system of social organization seems to have lent itself towards developing the individual components with an eye for them to be understood by the rest of the *consortium* rather than with the intention of inviting collaboration from the broader research community[^intraconsortium]. The very notion of a platform indicates that it is something that *they build* and *we use*: There is no explicit means for proposing changes to the BioLink model, to pick and choose how answers are ranked or queries are performed, etc. This is broadly true of platform-based scientific tools, especially databases, and contributes to how they *feel*: they feel disconnected with our work, don't necessarily help us do it more easily or more effectively, and contributing to them is a burdensome act of charity (if it is possible at all).

[^intraconsortium]: The descriptions of difficulty in interfacing the components of the project internally are littered throughout their public-facing documents: eg. "A lot of the work has been about defining standards, so that the components that each of the 15 teams are building can talk to each other" {% cite renaissancecomputinginstituterenciBiomedicalDataTranslator2022 %}, "In part due to the speed with which the program has progressed, team members also have found it challenging to coordinate milestones and deliverables across teams and align the goals of the Translator program with the goals of their own nonTranslator research projects." {% cite consortiumBiomedicalDataTranslator2019 %}. These problems are, of course, completely reasonable. My comment here merely suggests that solving these problems, particularly on the self-described tight timeline of the Translator's development, may have edged out concerns for engagemenent with the broader research community.

Given the real need for *some* means of combining heterogeneous data from disparate sources, what could have been done differently?

Problematizing the need for a system intended to link *all* or even *most* biomedical data in a single mutually coherent system opens the possibility for a very different data linking infrastructure. Perhaps paradoxically, any universal, logically complete schema intended to support algorithmic inference projects a relatively circumscribed group of people for whom it would be useful: nearly all of the publicly described use-cases are oriented around finding new drugs or targets to treat disease, presumably in part because that's what preoccupies the ontology. Rather than a set of generalizable *tools* for linking data, the need for universality strongly constrains the form of data that can be represented by the system, and its platform structure constrains its uses to only those imagined by the platform designers. Every infrastructural model is an act of balancing constraints, and prioritizing "all data" seems to imply "for some people." Who is supposed to be able to upload data? change the ontology? inspect the machine learning model? Who is in charge of what? Who is a knowledge-graph query engine useful for?

Another conceptualization might be building systems for *all people* that can *embed with existing practices* and *help them do their work* which typically involves accessing *some data.* We can imagine a system designed to integrate data with schemas written in the *vernacular* of communities of knowledge work. Rather than the dichotomy of one singular database vs. many fragmented and incompatible databases, we can imagine a *pluralistic* system capable of supporting multiple overlapping and potentially conflicting representations, governable and malleable in local communities of practice. Taking seriously the notion of "translation," we could stand to learn from linguistics and translation studies: rather than attempting to project the dialects of each subdiscipline into some "true" meta-framework (a decidedly colonial project {% cite shammaTranslationColonialism2018 %}), we could resist the urge for homogenization and preserve the multiplicity of representation, embracing the imperfection of mappings between heterogeneous representational systems at multiple scales without resigning ourselves to completely isolated incompatibility.

Maybe we don't *want* a universal system that presents itself with the authority of truth to be mined and spun off into derivative platforms by information conglomerates. We might abandon the techno-utopianism of a globally consistent schema that supports arbitrary logical inference by acknowledging that those inferences would always be colored by the decisions embedded in the structure of the system, unknowable beneath the shrouding weights of its ranking model.

Instead can we imagine a properly *human* data infrastructure? One that preserves the seams and imperfections in our representational systems, that is designed to represent precisely the contingency of representation itself (eg. see {% cite birhaneImpossibilityAutomatingAmbiguity2021 fletcher-watsonDiversityComputing2018 %})? We might start with the propositional nature of links and mappings between formats --- that rather than a divine received truth, the relationships between things are contextual and created. We could find grounding in *use,* that the schemas and mappings between them should arise from the need to link representations within the context of some problem, rather than to resolve their difference.

Picking up the thread of our peer to peer data sharing backbone, we might start to imagine the boistrous multiplicity of an infrastructure based around communication and expression, rather than platformatized perfection.


