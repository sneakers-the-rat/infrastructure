
The shallowest onramp towards a generalized data infrastructure is to make use of existing discipline-specific standardized data formats. As will be discussed later, a truly universal pandisciplinary format is impossible and undesirable, but to arrive at the alternative we should first congeal the wild west of unstandardized data into a smaller number of established formats. 

Data formats consist of some combination of an abstract specification, an implementation in a particular storage medium, and an API for interacting with the format. I won't dwell on the particular qualities that a particular format needs, assuming that most that would be adopted would abide by FAIR principles.

There are a dizzying number of scientific data formats {% cite teamScientificDataFormats %}, so a comprehensive treatment is impractical here and I will use Neurodata Without Borders:N (NWB){% cite rubelNWBAccessibleData2019a %} as an example. NWB is the de facto standard for systems neuroscience, adopted by many institutes and labs, though far from universally. NWB [consists of](https://www.nwb.org/nwb-software/) a [specification language](https://schema-language.readthedocs.io/en/stable/), a [schema written in that language](https://nwb-schema.readthedocs.io/en/stable/), a [storage implementation in hdf5](https://nwb-storage.readthedocs.io/en/stable/), and an [API for interacting with the data](https://pynwb.readthedocs.io/en/stable/). They have done an admirable job of engaging with community needs {% cite rubelNeurodataBordersEcosystem2021 %} and making a modular, extensible format ecosystem. 

The major point of improvement for NWB, and I imagine many data standards, is the ease of conversion and use. The conversion API requires extensive programming, knowledge of the format, and navigation of several separate tutorial documents. This means that individual labs, if they are lucky enough to have some partially standardized format for the lab, typically need to write (or hire someone to write) their [own](https://github.com/catalystneuro/tank-lab-to-nwb) software [library](https://github.com/catalystneuro/mease-lab-to-nwb) for conversion. 

Without being prescriptive about its form, substantial interface development is needed to make mass conversion possible. It's usually untrue that unstandardized data had *no structure,* and researchers are typically able to articulate it -- "the filenames have the collection date followed by the subject id," and so on. Lowering the barriers to conversion mean designing tools that match the descriptive style of folk formats, for example by prompting them to describe where each of an available set of metadata fields are located in their data. It is not an impossible goal to imagine a piece of software that can be downloaded and with minimal recourse to reference documentation allow someone to convert their lab's data within an afternoon.

NWB also has an extension interface, which allows, for example, data from common hardware and software tools to be more easily described in the format. These are registered in an [extensions catalogue](https://nwb-extensions.github.io/), but at the time of writing it is relatively sparse. The preponderance of lab-specific conversion packages relative to extensions is indicative of an interface and community tools problem: presumably many people are facing similar conversion problems, but because there is not a place to share these techniques in a human-readable way, the effort is duplicated in dispersed codebases. We will return to some possible solutions for knowledge preservation and format extension when we discuss tools for [shared knowledge](#shared-knowledge). 

For the sake of the rest of the argument, let us assume that some relatively trivial conversion process exists to subdomain-specific data formats and we reach some reasonable penetrance of standardization. The interactions with the other pieces of infrastructure that may induce and incentivize conversion will come later.

### Peer-to-peer as a Backbone

We should adopt a *peer-to-peer* system for storing and sharing scientific data. There are, of course [many](https://www.dandiarchive.org/) [existing](https://openneuro.org/) [databases](https://www.brainminds.riken.jp/) [for](https://biccn.org/) scientific data, ranging from domain-general like [figshare](https://figshare.com/) and [zenodo](https://zenodo.org/) to the most laser-focused subdiscipline-specific. The notion of a database, like a data standard, is not monolithic. As a simplification, they consist of at least the hardware used for storage, the software implementation of read, write, and query operations, a formatting schema, some API for interacting with it, the rules and regulations that govern its use, and especially in scientific databases some frontend for visual interaction. For now we will focus on the storage software and read-write system, returning to the format, regulations, and interface later. 

Centralized servers[^cdns] are fundamentally constrained by their storage capacity and bandwidth, both of which cost money. In order to be free, database maintainers need to constantly raise money from donations or grants in order to pay for both. Funding can never be infinite, and so inevitably there must be some limit on the amount of data that someone can upload and the speed at which it can serve files[^osfspeed]. Centralized servers are also intrinsically out of the control of their users, requiring them to abide whatever terms of use the server administrators set. Even if the database is carefully backed up, it serves as a single point of infrastructural failure, where if the project lapses then at worst data will be irreversibly lost, and at best a lot of labor needs to be expended to exfiltrate, reformat, and rehost the data. The same is true of isolated, local, institutional-level servers and related database platforms, with the additional problem of skewed funding allocations making them unaffordable for many researchers. 

[^osfspeed]: As I am writing this, I am getting a (very unscientific sample of n=1) maximum speed of 5MB/s on the [Open Science Framework](https://osf.io)

[^cdns]: This applies to centrally managed traditional servers as well as rented space on larger CDNs like AWS, but in the case of the CDN the constraint is from their pricing model.

Peer-to-peer (p2p) systems solve many of these problems, and I argue are the only type of technology capable of making a database system that can handle the scale of all scientific data. They are also not new for science, used in projects like [AcademicTorrents.com](https://academictorrents.com/) {% cite cohenAcademicTorrentsCommunityMaintained2014 loAcademicTorrentsScalable2016 %} or the now defunct BioTorrents {% cite langilleBioTorrentsFileSharing2010 %}. Whether we acknowledge it or not, most scientific work is already available on p2p networks via sci-hub and library genesis {% cite himmelsteinSciHubProvidesAccess2018 sci-hubTorrentHealthTracker2022 bookwarriorLibraryGenesisDecentralized2021 %}.

There is an enormous degree of variation between p2p systems[^p2pdiscipline], but they share a set of architectural advantages. The essential quality of any p2p system is that rather than each participant in a network interacting only with a single server that hosts all the data, everyone hosts data and interacts directly with each other.

[^p2pdiscipline]: Peer to peer systems are, maybe predictably, a whole academic subdiscipline. See {% cite shenHandbookPeertoPeerNetworking2010 %} for reference.

For the sake of concreteness, we can consider a (simplified) description of Bittorrent {% cite cohenBitTorrentProtocolSpecification2017 %}, arguably the most successful p2p protocol. To share a collection of files, a user creates a `.torrent` file with their Bittorrent client which consists of a [cryptographic hash](https://en.wikipedia.org/wiki/Cryptographic_hash_function), or a string that is unique to the collection of files being shared; and a list of "trackers." A tracker, appropriately, keeps track of the `.torrent` files that have been uploaded to it, and connects users that have or want the content referred to by the `.torrent` file. The uploader (or seeder) then leaves a [torrent client](https://en.wikipedia.org/wiki/Glossary_of_BitTorrent_terms#Client) open waiting for incoming connections. Someone who wants to download the files (a leecher) will then open the `.torrent` file in their client, which will then ask the tracker for the IP addresses of the other peers who are seeding the file, directly connect to them, and begin downloading. So far so similar to standard client-server systems, but say another person wants to download the same files before the first person has finished downloading it: rather than *only* downloading from the original seeder, the new leecher downloads from *both* the original seeder and the first leecher by requesting pieces of the file from each until they have the whole thing. Leechers are incentivized to share among each other to prevent the seeders from spending time reuploading the pieces that they already have, and once they have finished downloading they become seeders themselves.

From this very simple example, we can articulate a number of attractive qualities of p2p systems:

- First, p2p systems are extremely **inexpensive to maintain** since they take advantage of the existing bandwidth and storage space of the computers in the swarm. Near the height of its popularity in 2009, The Pirate Bay, a notorious bittorrent tracker {% cite vandersarPirateBayFive2011 %}, was estimated to cost $3,000 per month to maintain while serving approximately 20 million peers {% cite roettgersPirateBayDistributing2009 %}. According to a database dump from 2013 {% cite PirateBayArchiveteam2020 %}, multiplying the size of each torrent by the number of seeders (ignoring any partial downloads from leechers), the approximate instantaneous amount of data stored by The Pirate Bay was ~26 Petabytes. The comparison to centralized services is not straightforward, since it is hard to evaluate the distributed costs of additional storage media (as well as the costs avoided by being able to take advantage of existing storage infrastructure within labs and institutes), but for the sake of illustration: hosting 26PB would cost $546,000/month with standard AWS S3 hosting ([$0.021/GB/month](https://aws.amazon.com/s3/pricing/?nc=sn&loc=4)). On AWS, downloads cost extra ([$0.05/GB](https://aws.amazon.com/s3/pricing/?nc=sn&loc=4)), so the much smaller [academictorrents.com](https://academictorrents.com) which [has served](https://github.com/academictorrents/academictorrents-docs/issues/31#issuecomment-1155917166) nearly 18PB in 1.3m downloads since 2016 would have cost $900,000 in bandwidth costs alone --- as opposed to the [literally zero dollars](https://github.com/academictorrents/academictorrents-docs/issues/31#issuecomment-1152851111) it costs to operate.
- The **speed** of a bittorrent swarm *increases,* rather than decreases, the more people are using it since it is capable of using all of the available bandwidth in the system.
- The network is extremely **resilient** since the data is shared across many independent peers in the system. If our goal is to make a resilient and robust data architecture, we would benefit by paying attention to the tools used in the broader archival community, especially the archival communities that are frequent targets of governments and intellectual property holders{% cite spiesDataIntegrityLibrarians2017 %}.  Despite more than 15 years of concerted effort by governments and intellectual property holders, The Pirate Bay is still alive and kicking[^knockin] {% cite kim15YearsPirate2019 %}. This is because even if the entire infrastructure of the tracker is destroyed, as it was in 2006, the files are distributed across all of its users, the actual database of `.torrent` metadata is quite small, and the tracker software is extraordinarily simple to rehost {% cite vandersarOpenBayNow2014 %} -- The Pirate Bay was back online in 2 days.  When another tracker, what.cd (which we will return to [soon](#archives-need-communities)) was shut down, a series of successors popped up using the open source tools [Gazelle](https://github.com/WhatCD/Gazelle) and [Ocelot](https://github.com/WhatCD/Ocelot) that what.cd developers built. Within two weeks, one successor site had recovered and reindexed 200,000 of its torrents resubmitted by former users {% cite vandersarWhatCdDead2016 %}. Bittorrent is also used by archival groups with little funding like [Archive Team](https://wiki.archiveteam.org/index.php/Main_Page), who struggled -- but eventually succeeded -- to disseminate their [geocities archive](https://wiki.archiveteam.org/index.php/GeoCities_Project) over a single "crappy cable modem" {% cite scottGeocitiesTorrentUpdate2010 %}.
- The network is extremely **scalable** since there is no cost to connecting new peers and the users of a system expand the storage capacity of the system depending on their needs. Rather than having one extremely fast data center, the model of p2p systems is to leverage many approachable peer/servers.

[^knockin]: knock on wood

[^academictorrents]: Stats are posted on the main page, this is a rough estimate using the data from [June 1st to June 10th 2022](https://web.archive.org/web/20220610083152/https://academictorrents.com/).

Peer-to-peer systems are not mutually exclusive with centralized servers: servers are peers too, after all. A properly implemented p2p system will always be *at least* as fast and have *at least* as much storage as any alternative centralized server because peers can use *both* the bandwidth of the server *and* that of any peers that have the file. In the bittorrent ecosystem large-bandwidth/storage peers are known as "seedboxes"{% cite rossiPeekingBitTorrentSeedbox2014 %} when they use the bittorrent protocol, and "web seeds"{% cite hoffmanHTTPBasedSeedingSpecification %} when they use a protocol built on top of traditional HTTP. [Archive.org](https://archive.org) has been distributing all of its materials [with bittorrent](https://archive.org/details/bittorrent) by using its servers as web seeds since 2012 and makes this point explicitly: "BitTorrent is now the fastest way to download items from the Archive, because the Bittorrent client downloads simultaneously from two different Archive servers located in two different datacenters, and from other Archive users who have downloaded these Torrents already." {% cite kahle000000Torrents2012 %}

p2p systems complement centralized servers in a number of ways beyond raw download speed, increasing the efficiency and performance of the network as a whole. Spotify began as a joint client/server and p2p system {% cite kreitzSpotifyLargeScale2010b %}, where when a listener presses play the central server provides the data until the p2p system locates peers with a cached copy to download from. The central server is able to respond quickly and reliably, and is the server of last resort in the case of rare files that aren't being shared by anyone else in the network. The p2p system alleviates pressure on the central server, improving the performance of the network and reducing server costs.

A peer to peer system is a particularly natural fit for many of the common circumstances and practices in science, where centralized server architectures seem (and prove) awkward and inefficient. Most labs, institutes, or other organized bodies of science have some form of local or institutional storage systems. In the most frequent cases of sharing data within a lab or institute, sending it back and forth to some nationally-centralized server is like walking across the lab by going the long way around the Earth. That's the method invoked by a Dropbox or AWS link, which keeps a time-tested p2p system relevant: walking a flash drive across the lab. The system makes less sense when several people in the same place need to access the same data at the same time, as is frequently the case with multi-lab collaborations, or scientific conferences and workshops. Instead of needing to wait on the 300kb/s conference wifi bandwidth as it's cheese-gratered across every machine, we instead could directly beam it between all computers in range simultaneously, full blast through the decrepit network switch that won't have seen that much excitement in years. 

If we take the suggestion of Andrey Andreev et al. and invest in server clusters within institutes {% cite andreevBiologistsNeedModern2021 charlesCommunityDrivenBigOpen2020 %}, their impact could be multiplied manyfold by fluidly combining them in a p2p swarm. While the NIH might be shy to start up another server farm for all scientific data and prefer to contract with AWS, the rest of us don't have to be. Nervous university administrators concerned about bandwidth costs should also favor p2p systems: instead of needing to serve entire datasets to each person who wants them, the load can be spread out across many institutes naturally based on the use of the file, and sharing the dataset internally would cost nothing at all.

So far I have relied on the Extraordinarily Simplified Bittorrent[^tm] depiction of a peer to peer system, but there are many improvements and variants that can address different needs for scientific data infrastructure. 

One obvious need that bittorrent can't currently support is version control[^bittorrentv2], but more recent p2p systems do. [IPFS](https://ipfs.io/) functions like "a single BitTorrent swarm, exchanging objects within one Git repository." {% cite benetIPFSContentAddressed2014 %}[^whatsgit] Dat {% cite ogdenDatDistributedDataset2017 %}, specifically designed for data synchronization and versioning, handles versioning and more. A full description of IPFS is out of scope, and it has plenty of problems {% cite patsakisHydrasIPFSDecentralised2019 %}, but for now it suffices to say p2p systems can handle version control.

[^bittorrentv2]: Though the Bittorrent V2 protocol specification {% cite cohenBEP52BitTorrent2017 %} adopts a Merkle tree data structure which could theoretically support versioned torrents, v2 torrents are still not widely supported. 

[^whatsgit]:  Git, briefly, is a version control system that keeps a history of changes of files (blobs) as a Merkle DAG: files can be updated, and different versions can be branched and reconciled.

Bittorrent swarms are vulnerable to data loss if all the peers seeding a file disconnect (though the tail is longer than typically assumed, see {% cite zhangUnravelingBitTorrentEcosystem2011 %}), but this too can be addressed with updated p2p system design. A first-order solution to this problem is a variant of IPFS' notion of 'pinning.' Since backup to lab-level or institutional servers is already commonplace, one peer could be able to 'pin' another and automatically download all the data that they share. This concept could scale to institutes and national infrastructure as scientists can request the datasets they'd like to be saved permanently be pinned. 

Another could be something akin to Freenet {% cite clarkeFreenetDistributedAnonymous2001 %}. Peers could allocate a certain amount of their unused storage space to be used to automatically download, cache, and rehost shards of other datasets. Distributing chunks and encrypting them at rest so the rehoster can't inspect their contents would make it possible to maintain privacy and network availability for sensitive data (see, for example, [ERIS](https://inqlab.net/projects/eris/)). IPFS has an analogous concept -- BitSwap -- that is makes it into a barter system. Peers who seek to download will have to 'earn' it by finding some chunk of data that the other peers want, download, and share them, though it seems like an empirical question whether or not a barter system works or is necessary.

[Solid](https://solidproject.org/) is a project that almost exactly meets all these needs {% cite capadisliSolidProtocol2020 sambraSolidPlatformDecentralized2016 SolidP2PFoundation %}. Solid allows people to share data in [Pods](https://solidproject.org/about), which let them control access and distribution across storage system with a unified identity system. It is implementation-agnostic, and so can support any peer-to-peer storage and transfer system that complies with its [protocol specification](https://solidproject.org/TR/protocol).

There are a number of additional requirements for a peer to peer scientific data infrastructure, but even these seemingly very technical problems of versioning and distributed storage show the clear need to consider the structure of the surrounding social system. What control do we give to researchers over the version history of their data? Should people that aren't the originating researcher be able to issue new versions? What structure of distributed/centralized storage works? How should we incentivize sharing of excess storage and resources? 

Even before considering additional social systems, a p2p structure in itself implies a different relationship to infrastructure. Scientists always unavoidably make their data available to at least one person: themselves; on at least one computer: theirs, and that computer is usually connected to the internet. With a p2p system that integrates metadata from domain-specific data formats, that's it, that's all, the data is already hosted by merely existing. Dust your palms off: open data achieved. A peer-to-peer backbone for scientific infrastructure realizes the unnecessarily radical notion that our infrastructure can be integrated into our daily practices, rather than existing exogenously as something "out there." It helps us internalize the slyly subversive notion that *we can build it ourselves* instead of renting something out of our control from someone else.

Scientists don't need to reinvent the notion of distributed, community curated data archives from scratch. In addition to scholarly work on the social systems of digital infrastructure, we can learn from communities of practice, and there has been no more important and impactful decentralized archival project than internet piracy.

### Archives Need Communities

Why do hundreds of thousands of people, completely anonymously, with zero compensation, spend their time to do something that is as legally risky as curating pirated cultural archives? 

Scholarly work, particularly from Economics, tends to focus on understanding piracy in order to prevent it{% cite basamanowiczReleaseGroupsDigital2011 hindujaDeindividuationInternetSoftware2008 %}, taking the moral good of intellectual property markets as an *a priori* imperative and investigating why people behave *badly* and "rend [the] moral fabric associated with the respect of intellectual property." {% cite hindujaDeindividuationInternetSoftware2008 %}. If we put the legality of piracy aside, we may find a wealth of wisdom and insight to draw from for building scientific infrastructure.

The world of digital piracy is massive, from entirely disorganized efforts of individual people on public sites to extraordinarily organized release groups {% cite basamanowiczReleaseGroupsDigital2011 %}, and so a full consideration is out of scope (see {% cite eveWarezInfrastructureAesthetics2021 %}), but many of the important lessons are taught by the structure of bittorrent trackers.

An underappreciated element of the BitTorrent protocol is the effect of the separation between the data transfer protocol and the "discovery" part of the system --- or "overlay" --- on the community structure of torrent trackers (for a more complete picture of the ecosystem, see {% cite zhangUnravelingBitTorrentEcosystem2011 %}). Many peer to peer networks like [KaZaA](https://en.wikipedia.org/wiki/Kazaa) or the [gnutella](https://en.wikipedia.org/wiki/Gnutella)-based [Limewire](https://en.wikipedia.org/wiki/LimeWire) had searching for files integrated into the transfer interface. The need for torrent trackers to share .torrent files spawned a massive community of private torrent trackers that for decades have been iterating on cultures of archival, experimenting with different community structures and incentives that encourage people to share and annotate some of the world's largest, most organized libraries.

One of these private trackers was the site of one of the largest informational tragedies of the past decade: what.cd[^whatdiss], which I will use as an example to describe some of these community systems.

[^whatdiss]: for a detailed description of the site and community, see Ian Dunham's dissertation {% cite dunhamWhatCDLegacy2018 %}

What.cd was a bittorrent tracker that was arguably the largest collection of music that has ever existed. At the time of its destruction in 2016, it was host to just over one million unique releases, and approximately 3.5 million torrents[^dbsize] {% cite dunhamWhatCDLegacy2018 %}. Every torrent was organized in a meticulous system of metadata communally curated by its roughly 200,000 global users. The collection was built by people who cared deeply about music, rather than commercial collections provided by record labels notorious for ceasing distribution of recordings that are not commercially viable --- or just losing them in a fire {% cite rosenDayMusicBurned2019 %}. Users would spend large amounts of money to find and digitize extremely rare recordings, many of which were unavailable anywhere else and are now unavailable anywhere, period. One former user describes one example:

> “I did sound design for a show about Ceaușescu’s Romania, and was able to pull together all of this 70s dissident prog-rock and stuff that has never been released on CD, let alone outside of Romania” {% cite sonnadEulogyWhatCd2016 %}

[^dbsize]: Though spotify now boasts its library having 50 million tracks, back of the envelope calculations relating number of releases to number of tracks are fraught, given the long tail of track numbers on albums like classical music anthologies with several hundred tracks on a single "release."

![A what.cd artist page (Kanye west) that shows each of his albums having perhaps a dozen different torrents: each time the album was released, on cd, vinyl, and web, each in multiple different audio formats.](/infrastructure/assets/images/kanye-what.png)
*The what.cd artist page for Kanye West (taken from [here](https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/) in the style of pirates, without permission). For the album "Yeezus," there are ten torrents, grouped by each time the album was released on CD and Web, and in multiple different qualities and formats (.flac, .mp3). Along the top is a list of the macro-level groups, where what is in view is the "albums" section, there are also sections for bootleg recordings, remixes, live albums, etc.*

What.cd was a "private" bittorrent tracker, where unlike public trackers that anyone can access, membership was strictly limited to those who were personally invited or to those who passed an interview (for more on public and private trackers, see {% cite meulpolderPublicPrivateBitTorrent %}). Invites were extremely rare, and the interview process was demanding to the point where [extensive guides](https://opentrackers.org/whatinterviewprep.com/index.html) were written to prepare for them. 

The what.cd incentive system was based on a required ratio of data uploaded vs. data downloaded {% cite jiaHowSurviveThrive2013 %}. Peer to peer systems need to overcome a free-rider problem where users might download a torrent ("leeching") and turn their computer off, rather than leaving their connection open to share it to others (or, "seeding"). In order to download additional music, then, one would have to upload more. Since downloading is highly restricted, and everyone is trying to upload as much as they can, torrents had a large number of "seeders," and even rare recordings would be sustained for years, a pattern common to private trackers {% cite liuUnderstandingImprovingRatio2010 %}. 

The high seeder/leecher ratio made it so it was extremely difficult to acquire upload credit, so users were additionally incentivized to find and upload new recordings to the system. What.cd implemented a "bounty" system, where users with a large amount of excess upload credit would be able to offer some of it to whoever was able to upload the album they wanted. To "prime the pump" and keep the economy moving, highlight artists in an album of the week, or direct users to preserve rare recordings, moderators would also use a "freeleech" system, where users would be able to download a specified set of torrents without it counting against their download quantity {% cite kashEconomicsBitTorrentCommunities2012 chenImprovingSustainabilityPrivate2011a %}.

The other half of what.cd was the more explicitly social elements: its forums, comment sections, and moderation systems. The forum was home to roiling debates that lasted years about the structure of some tagging schema, whether one genre was just another with a different name, and so on. The structure of the community was an object of constant, public negotiation, and over time the metadata system evolved to be able to support a library of the entirety of human musical culture[^subtlety]. To support the good operation of the site, the forums were also home to a huge amount of technical knowledge, like guides on how to make a perfect copy of a CD or how to detect a fake upload, that eased new users into being able to use and contribute to the system.

[^subtlety]: Though music metadata might seem like a trivial problem (just look at the fields in an MP3 header), the number of edge cases are profound. How would you categorize an early Madlib cassette mixtape remastered and uploaded to his website where he is mumbling to himself while recording some live show performed by multiple artists, but on the b-side is one of his Beat Konducta collections that mix together studio recordings from a collection of other artists? Who is the artist? How would you even identify the unnamed artists in the live show? Is that a compilation or a bootleg? Is it a cassette rip, a remaster, or a web release?

A critical problem in maintaining coherent databases is correcting metadata errors and departures from schemas. Finding errors was rewarded. Users were able to discuss and ask questions of the uploader in a comment section below each upload, which would allow "polite" resolution of low-level errors like typos. More serious problems could be reported to the moderation team, which caused the upload to be visibly marked as under review, and the report could then be discussed either in the comment sections or the forum. The system wasn't perfect: being an anonymous, gray-area community, there was of course plenty of power to be abused. Rather than being a messy hodgepodge of fake, low-quality uploads, though, what.cd was always teetering just shy of perfection.

These structural considerations do not capture the most elusive but indisputably important feature of what.cd's community infrastructure: *the sense of community*. The What.cd forums were the center of many user's relationships to music. Threads about all the finest scales of music nichery could last for years: it was a rare place people who probably cared a little bit too much about music could talk to people with the same condition. What made it more satisfying than other music forums was that no matter what music you were talking about, everyone else in the conversation would always have access to it if they wanted to hear it.  Beyond any structural incentives, people spent so much time building and maintaining what.cd because it became a source of community and a sink of personal investment.

Structural norms supported by social systems converge as a sort of *reputational* incentive. Uploading a new album to fill a bounty both makes the network more functional and complete, but also *people respect you for it* because it's prominently displayed on your profile as well as in the bounty charts and that *feels good*. Becoming known on the forums for answering questions, writing guides, or even just having a good taste in music *feels good* and also contributes to the overall health of the system. Though there are plenty of databases, and even plenty of different communication venues for scientists, there aren't any databases (to my knowledge) with integrated community systems. 

The tracker overlay model mirrors and extends some of the recommendations made by Benedikt Fecher and colleagues in their work on the reputational economy surrounding data sharing {% cite fecherReputationEconomyHow2017 %}. They give three policy recommendations: Increasing reputational benefits, reducing transaction costs, and "increasing market transparency by making open access to research data more visible to members of the research community." One way to accomplish implement them is to embed a data sharing system within a social system that is designed to reward communitarian behavior. 

Many features of what.cd's structure are undesirable for scientific infrastructure, but they demonstrate that a robust archive is not only a matter of building a database with some frontend, but also building a community {% cite brossCommunityCollaborationContribution2013 %}. Of course, we need to be careful with building the structural incentives for a data sharing system: the very last thing we want is another [coercive leaderboard](https://etiennelebel.com/cs/t-leaderboard/t-leaderboard.html) that turns what should be a collaborative effort punitive. In contrast to what.cd, for infrastructure we want extremely low barriers to entry, and be agnostic to resources --- researchers with access to huge server farms should not be unduly favored. We should think carefully about using downloading as the "cost," because downloading and analyzing huge amounts of data can be *good* and exactly what we *want* in some circumstances, but a threat to privacy and data governance in others. 

This model has its own problems, including the lack of interoperability between different trackers, the need to recreate a new set of accounts and database for each new tracker, among others. It's also been tried before: sharing data in specific formats (as our running example, Neurodata Without Borders) on indexing systems like bittorrent trackers amounts to something like BioTorrents {% cite langilleBioTorrentsFileSharing2010 %} or [AcademicTorrents](https://academictorrents.com/) {% cite cohenAcademicTorrentsCommunityMaintained2014 %}. Even with our extensions of version control and some model of automatic mirroring of data across the network, we still have some work to do. To address these and several other remaining needs for scientific data infrastructure, we can take inspiration from *federated systems.*


