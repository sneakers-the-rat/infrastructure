\documentclass[nohyper,nols,notoc,notitlepage,twoside=false,justified,nobib]{tufte-prelims}
\usepackage{url}
\usepackage[colorlinks = true,
            urlcolor = DarkOrchid,
            citecolor = DarkOrchid,
            backref= section]{hyperref}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage[default,regular,black]{sourceserifpro}
\usepackage[square,numbers]{natbib}
\usepackage[export]{adjustbox}



\makeatletter
\AtBeginDocument{%
  \def\doi#1{\url{https://doi.org/#1}}}
\makeatother

\setcounter{tocdepth}{3}
\title{Decentralized Infrastructure for (Neuro)science}
\author{Jonny L. Saunders}
\date{\today}



\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \begin{doublespace}
  \noindent\textit{#1}% epigraph
  \begin{flushright}
  \sffamily\noindent\textit{#2}% author
  \end{flushright}
  \end{doublespace}
  \end{fullwidth}
  \vspace{1cm}
}

% Options for packages loaded elsewhere

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\begin{document}

\frontmatter

\clearpage

\maketitle

\pagebreak



\openepigraph{%
If we can make something decentralised, out of control, and of great
simplicity, we must be prepared to be astonished at whatever might grow
out of that new medium.
}{Tim Berners-Lee (1998): \href{https://www.w3.org/1998/02/Potential.html}{
Realising the Full Potential of the Web}}

\openepigraph{%
A good analogy for the development of the Internet is that of constantly
renewing the individual streets and buildings of a city, rather than
razing the city and rebuilding it. The architectural principles
therefore aim to provide a framework for creating cooperation and
standards, as a small ``spanning set'' of rules that generates a large,
varied and evolving space of technology.
}{RFC 1958: \href{https://datatracker.ietf.org/doc/html/rfc1958}{Architectural Principles of the Internet}}

\openepigraph{%
In building cyberinfrastructure, the key question is not whether a
problem is a ``social'' problem or a ``technical'' one. That is putting
it the wrong way around. The question is whether we choose, for any
given problem, a primarily social or a technical solution
}{Bowker, Baker,
Millerand, and Ribes (2010): \href{https://doi.org/10.1007/978-1-4020-9789-8_5}{ Toward Information Infrastructure Studies}
\citep{bowkerInformationInfrastructureStudies2010}}

\openepigraph{%
The critical issue is, how do actors establish generative platforms by
instituting a set of control points acceptable to others in a nascent
ecosystem?
}{Tilson, Lyytinen, SÃ¸rensen (2010): \href{https://doi.org/10.1287/isre.1100.0318}{Digital Infrastructures: The Missing IS Research Agenda}\citep{tilsonDigitalInfrastructuresMissing2010}} 

\clearpage

\vspace{-10cm}

\tableofcontents

\clearpage

\mainmatter


\chapter{Introduction}



 We work in an archipelago of technical islands,
researchers, labs, consortia, and a few well-funded institutions
reinventing the wheel in parallel. Our knowledge dissemination systems
are as nimble as the static pdfs and ephemeral conference talks that
they have been for decades\footnote{(save some
  \protect\hyperlink{forums-are-just-one-point-in-a-continuous-feature-space-of-commu}{complicated
  half-in flirtation} with social media).}. Experimental instrumentation
except for that at the polar extremes of technological complexity or
simplicity is designed and built custom, locally, and
on-demand\footnote{At least in systems neuroscience, appropriate caveats
  below.}. Software for performing experiments is a patchwork of
libraries that satisfy some of the requirements of the experiment, sewn
together by some uncommented script written years ago by a grad student
who left the lab long-since. The technical knowledge to build both
instrumentation and software is fragmented and unavailable as it sifts
through the funnels of word-limited methods sections and never-finished
documentation. And O Lord Let Us Pray For The Data, born into this world
without coherent form to speak of, indexable only by passively-encrypted
notes in a paper lab notebook, dressed up for the analytical ball once
before being mothballed in ignominy on some unlabeled external drive.

The idiosyncratic and improvised ways we use and relate to computers may
be unique in each instance, but all are symptomatic of a broader deficit
in \textbf{digital infrastructure} for science. Every routine need that
requires heavy technical development or yet another platform
subscription is an indicator that infrastructural deficits \emph{define
the daily reality of science.} We \emph{should} be able to easily store,
share, and search for data; be able to organize and communicate with
each other; be able to write and review our work, but we are hemmed in
on all sides by looming tech profiteers and chasms of underdevelopment.

If the term infrastructure conjures images of highways and plumbing,
then surely digital infrastructure would be flattered at the
association. Roughly following Star and Ruhleder's (1996) dimensions
\citep{starStepsEcologyInfrastructure1996} , by analogy they
illustrate many of its promises and challenges: when designed to, it can
make practically impossible things trivial, allowing the development of
cities by catching water where it lives and snaking it through tubes and
tunnels sometimes directly into your kitchen. Its absence or failure is
visible and impactful, as in the case of power outages. There is no
guarantee that it ``optimally'' satisfies some set of needs for the
benefit of the greatest number of people, as in the case of the
commercial broadband duopolies. It exists not only as its technical
reality, but also as an embodied and shared set of social practices, and
so even when it does exist its form is not inevitable or final; as in
the case of bottled water producers competing with municipal tap water
on a behavioral basis despite being dramatically less efficient and more
costly. Finally it is not socially or ethically neutral, and the impact
of failure to build or maintain it is not equally shared, as in the
expression of institutional racism that was the Flint, Michigan water
crisis \citep{michicancivilrightscommissionFlintWaterCrisis2017} .

Infrastructural deficits are not our inevitable and eternal fate, but
the course of infrastructuring is far from certain. It is not the case
that ``scientific digital infrastructure'' will rise from the sea
monolithically as a natural result of more development time and funding,
but instead has many possible futures\citep{mirowskiFutureOpenScience2018} , each with their own advocates and
beneficiaries. Without concerted and strategic development based on a
shared and liberatory ethical framework, science will follow the same
path as other domains of digital technology down the dark road of
platform capitalism. The prize of owning the infrastructure that the
practice of science is built on is too great, and it is not hard to
imagine tech behemoths buying out the emerging landscape of small
scientific-software-as-a-service startups and selling subscriptions to
Science Prime. The possibility of future capture is still too naive a
framing: operating as obligate brokers of (usually surveillance)
data\citep{pooleySurveillancePublishing2021, zuboffBigOtherSurveillance2015, warkCapitalDeadThis2021} , prestige,
and computational resources naturally relies on \emph{displacing} the
possibility of alternative infrastructure. Our predicament is doubly
difficult: we both have digital infrastructural defits, but are also
being actively \emph{deinfrastructured.}

This paper is an argument that \textbf{decentralized} digital
infrastructure is the best means of alleviating the harms of
infrastructural deficits and building a digital landscape that supports,
rather than extracts from science. I will draw from several disciplines
and knowledge communities, across and outside academia to articulate a
vision of an infrastructure in three parts: \textbf{shared data, shared
tools, and shared knowledge.} These domains reflect three of the
dominant modes of digital enclosure prerequisite for platform capture:
\textbf{storage, computation, and communication.} The systems we will
describe are in conversation with and a continuation of a long history
of reimagining the relationship between these domains for a healthier
web (see eg. \citep{berners-leeSociallyAwareCloud2009, berners-leeWebServicesOverview2009} ). We depart from it to describe
a system of fluid, peer-to-peer social affiliation and
\protect\hyperlink{federated-systems-of-language}{folksonomic} linked
data with lessons primarily from \protect\hyperlink{the-wiki-way}{early
wikis and Wikipedia}, the fissures of the
\protect\hyperlink{neatness-vs-scruffiness}{semantic web and linked
data} communities, the social structure of
\protect\hyperlink{archives-need-communities}{private bittorrent
trackers}, and the federation system of
\protect\hyperlink{forums--feeds}{ActivityPub and the Fediverse}.
Approaching this problem from science has its constraints --- like the
structuring need to rebuild systems of
\protect\hyperlink{credit-assignment}{credit assignment} --- as well as
the powerful opportunity of one of the last systems of labor largely not
driven by profit developing technology and seeding communities that
could begin to directly address the dire, societywide need for digital
freedom.

The problems we face are different than they were at the dawn of the
internet, but we can learn from its history: we shouldn't be waiting for
a new journal-like \textbf{platform,} software package, or subscription
to save us. We need to build \textbf{protocols} for communication,
interoperability, and self-governance (see, recently \citep{brembsReplacingAcademicJournals2021} ).

I will start with a brief description of what I understand to be the
state of our digital infrastructure and the structural barriers and
incentives that constrain its development. I will then propose a set of
design principles for decentralized infrastructure and possible means of
implementing it informed by prior successes and failures at building
mass digital infrastructure. I will close with contrasting visions of
what science could be like depending on the course of our
infrastructuring, and my thoughts on how different actors in the
scientific system can contribute to and benefit from decentralization.

I insist that what I will describe is \emph{not utopian} but is
eminently practical --- the truly impractical choice is to do nothing
and continue to rest the practice of science on a pyramid scheme \citep{ponziSciencePyramidScheme2020}  of underpaid labor. With a bit
of development to integrate and improve the tools, \textbf{every class
of technology I propose here already exists and is widely used.} A
central principle of decentralized systems is embracing heterogeneity:
harnessing the power of the diverse ways we do science instead of
constraining them. Rather than a patronizing argument that everyone
needs to fundamentally alter the way they do science, the systems that I
describe are specifically designed to be easily incorporated into
existing practices and adapted to variable needs. In this way I argue
decentralized systems are \emph{more practical} than the dream that any
one system will be capable of expanding to the scale of all science ---
and as will hopefully become clear, inarguably \emph{more powerful} than
a disconnected sea of centralized platforms and services.

An easy and common misstep is to categorize this as solely a
\emph{technical} challenge. Instead the challenge of infrastructure is
also \emph{social} and \emph{cultural} --- it involves embedding any
technology in a set of social practices, a shared belief that such
technology should exist, that its form is not neutral, and a sense of
communal valuation and purpose that sustains it \citep{bietzSustainingDevelopmentCyberinfrastructure2012} .

The social and technical perspectives are both essential, but make some
conflicting demands on the construction of the piece: Infrastructuring
requires considering the interrelatedness and mutual reinforcement of
the problems to be addressed, rather than treating them as isolated
problems that can be addressed piecemeal with a new package. Such a
broad scope trades off with a detailed description of the relevant
technology and systems, but a myopic techno-zealotry that does not
examine the social and ethical nature of scientific practice risks
reproducing or creating new sources of harm. That, and
techno-solutionism never \emph{works} anyway. As a balance I will not be
proposing a complete technical specification or protocol, but describing
the general form of the tools and some existing examples that satisfy
them; I will not attempt a full history or treatment of the problem of
infrastructuring, but provide enough to motivate the form of the
proposed implementations.

My understanding of this problem is, of course, uncorrectably structured
by my training largely centered in systems neuroscience and my position
as an early career researcher (ECR). While the core of my argument is
intended to be a sketch compatible with sciences and knowledge systems
generally, my examples will sample from, and my focus will skew to my
experience. In many cases, my use of ``science'' or ``scientist'' could
be ``neuroscience'' or ``neuroscientist,'' but I will mostly use the
former to avoid the constant context switches. This document is also an
experiment in public collaboration on a living scientific document: to
try and ease our way out of disciplinary tunnelvision, we invite
annotation and contribution with no lower bound --- if you'd like to add
or correct a sentence or two (or a page or ten), you're welcome as
coauthor. I ask the reader for a measure of patience for the many ways
this argument requires elaboration and modification for distant fields.





\chapter{The State of Things}


\section{The Costs of Infrastructure
Deficits}



 An infrastructure deficit framing gives a shared etiology
to many technical and social harms in scientific work that are typically
treated separately, and allows us to problematize other symptoms have
become embedded as norms.

I will list some of the present costs to give a sense of the scale of
need, as well as scope for the problems we intend to address here. These
lists are grouped into rough and overlapping categories, but make no
pretense at completeness and have no particular order.

Impacts on the \textbf{daily experience} of researchers include:

\begin{itemize}

\item
  A prodigious duplication and dead-weight loss of labor as each lab,
  and sometimes each person within each lab, will reinvent basic code,
  tools, and practices from scratch. Literally it is the inefficiency of
  the
  \href{https://en.wikipedia.org/wiki/Deadweight_loss\#Harberger's_triangle}{Harberger's
  triangle} in the supply and demand system for scientific
  infrastructure caused by inadequate supply. Labs with enough resources
  are forced to pay from other parts of their grants to hire
  professional programmers and engineers to build the infrastructure for
  their lab\footnote{(and usually their lab or institute only)}, but
  most just operate on a purely amateur basis. Many PhD students will
  spend the first several years of their degree re-solving
  already-solved problems, chasing the tails of the wrong half-readable
  engineering whitepapers, in their 6th year finally discovering the
  technique that they actually needed all along. That's not an
  educational or training model, it's the effect of displacing the
  undone labor of unbuilt infrastructure on vulnerable graduate workers
  almost always paid poverty wages.
\item
  At least the partial cause of the phenomenon where ``every scientist
  needs to be a programmer now'' as people who aren't particularly
  interested in being programmers --- which is \emph{fine} and
  \emph{normal} --- need to either suffer through code written by some
  other unlucky amateur or learn an entire additional discipline in
  order to do the work of the one they chose.
\item
  A great deal of pain and alienation for early- career researchers not
  previously trained in programming before being thrown in the deep end.
  Learning data hygeine practices like backup, annotation, etc. ``the
  hard way'' through some catastrophic loss is accepted myth in much of
  science. At some scale all the very real and widespread pain, guilt,
  and shame felt by people who had little choice but to reinvent their
  own data management system must be recognized as an infrastructural,
  rather than a personal problem.
\item
  The high cost of ``openness'' and the dearth of data transparency. It
  is still rare to publish full, raw data and analysis code, often
  because the labor of cleaning it is too great. The ``Open science''
  movement, roughly construed, has reached a few hard limits from
  present infrastructure that have forced its energy to leak from the
  sides as bullying leaderboards or sets of symbols that are mere
  signifiers of cultural affiliation to openness. ``Openness'' is not a
  uniform or universal goal for all science, but for those for whom it
  makes sense, we need to provide the appropriate tooling before
  insisting on a change in scientific norms. We can't expect data
  transparency from researchers while it is still so \emph{hard.}
\end{itemize}

Impacts on the \textbf{system of scientific inquiry} include:

\begin{itemize}

\item
  A profoundly leaky knowledge acquisition system where entire PhDs
  worth of data can be lost and rendered useless when a student leaves a
  lab and no one remembers how to access the data or how it's formatted.
\item
  The inevitability of continual replication crises because it is often
  literally impossible to replicate an experiment that is done on a rig
  that was built one time, used entirely in-lab code, and was never
  documented
\item
  Reliance on communication platforms and knowledge systems that aren't
  designed to, and don't come close to satisfying the needs of
  scientific communication. In the absence of some generalized means of
  knowledge organization, scientists ask the void\footnote{(Twitter)}
  for advice or guidance from anyone that algorithmically stumbles by.
  Often our best recourse is to make a Slack about it, which is
  incapable of producing a public, durable, and cumulative resource: and
  so the same questions will be asked again\ldots{} and again\ldots{}
\item
  A perhaps doomed intellectual endeavor as we attempt to understand the staggering
  complexity of the brain by peering at it through the camera obscura of
  just the most recent data you or your lab have collected rather than
  being able to index across the many measurements of the same
  phenomena. The unnecessary reduplication of experiments becomes not
  just a methodological limitation, but an ethical catastrophe as
  researchers have little choice but to abandon the elemental principle
  of sacrificing as few animals as possible.
\item
  A near-absence of semantic or topical organization of research that
  makes cumulative progress in science proabilistic at best, and subject
  to the malformed incentives of publication and prestige gathering at
  worst. Since engaging with prior literature is a matter of manually
  reconstructing a caricature of a field of work in every introduction,
  continuing lines of inquiry or responding to conflicting results is
  \emph{strictly optional.}
\item
  A hierarchy of prestige that devalues the labor of many groups of
  technicians, animal care workers, and so on. Authorship is the coin of
  the realm, but many workers that are fundamental to the operation of
  science only receive the credit of an acknowledgement. We need a
  system to value and assign credit for the immense amount of technical
  and practical knowledge and labor they contribute.
\end{itemize}

Impacts on the relationship between \textbf{science and society}:

\begin{itemize}

\item
  An insular system where the inaccessibility of all the ``contextual''
  knowledge \citep{woolKnowledgeNetworksHow2020, barleyBackroomsScienceWork1994}  that doesn't have a venue for
  sharing but is necessary to perform experiments, like ``how to build
  this apparatus,'' ``what kind of motor would work here,'' etc. is a
  force that favors established and well-funded labs who can rely on
  local knowledege and hiring engineers/etc. and excludes new,
  lesser-funded labs at non-ivy institutions. The concentration of
  technical knowledge magnifies the inequity of strongly skewed funding
  distributions such that the most well-funded labs can do a completely
  different kind of science than the rest of us, turning the
  positive-feedback loop of funding begetting funding ever faster.
\item
  An absconscion with the public resources we are privileged enough to
  receive, where rather than returning the fruits of the many technical
  challenges we are tasked with solving to the public in the form of
  data, tools, collected practical knowledge, etc. we largely return
  papers. Since those papers are often impenetrable outside of their
  discipline or paywalled outside of academia, we multiply the above
  impacts of labor duplication and knowledge inaccessibility by the
  scale of society.
\item
  The complicity of scientists in rendering our collective intellectual
  heritage nothing more than another regiment in the ever-advancing
  armies of
  \protect\hyperlink{platforms-industry-capture-and-the-profit-motive}{platform
  capitalism}. If our highest aspirations are to shunt all our
  experiments, data, and analysis tools onto Amazon Web Services, our
  failure of imagination will be responsible for yet another obligate
  funnel of wealth into the system of extractive platforms that dominate
  the flow of global information. For ourselves, we stand to have the
  practice of science filleted at the seams into a series of mutually
  incompatible subscription services. For society, we squander the
  chance for one of the very few domains of non-economic labor to build
  systems to recollectivize the basic infrastrucutre of the internet:
  rather than providing an alternative to the information overlords and
  their digital enclosure movement, we will be run right into their
  arms.
\end{itemize}

Considered separately, these are serious problems, but together they are
a damning indictment of our role as stewards of our corner of the human
knowledge project.

We arrive at this situation not because scientists are lazy and
incompetent, but because we are embedded in a system of mutually
reinforcing disincentives to cumulative infrastructure development. Our
incentive systems are, in turn, coproductive with a raft of economically
powerful entities that would really prefer owning it all themselves,
thanks. Put bluntly, ``we are dealing with a massively entrenched set of
institutions, built around the last information age and fighting for its
life'' \citep{bowkerInformationInfrastructureStudies2010} 

There is, of course, an enormous amount of work being done by
researchers and engineers on all of these problems, and a huge amount of
progress has been made on them. My intention is not to shame or devalue
anyone's work, but to try and describe a path towards integrating it and
making it mutually reinforcing.

Before proposing a potential solution to some of the above problems, it
is important to motivate why they haven't already been solved, or why
their solution is not necessarily imminent. To do that, we need a sense
of the social and technical challenges that structure the development of
our tools. 




\section{(Mis)incentives in Scientific
Software}

Systems Neuro {specific problems for infrastructure}



 The incentive systems in science are complex, subject to
infinite variation everywhere, so these are intended as general
tendencies rather than statements of irrevocable and uniform truth.


\subsection{Incentivized
Fragmentation}

Scientific software development favors the production of many isolated,
single-purpose software packages rather than cumulative work on shared
infrastructure. The primary means of evaluation for a scientist is
academic reputation, primarily operationalized by publications, but a
software project will yield a single paper (if any). Traditional
publications are static units of work that are ``finished'' and frozen
in time, but software is never finished: the thousands of commits needed
to maintain and extend the software are formally not a part of the
system of academic reputation.

Howison \& Herbsleb described this dynamic in the context of
BLAST\footnote{``Basic Local Alignment Search Tool'' - a tool to compare
  genetic or protein sequences to find potential matches or analogues.}

\begin{quote}
In essence we found that BLAST innovations from those motivated to
improve BLAST by academic reputation are motivated to develop and to
reveal, but not to integrate their contributions. Either integration is
actively avoided to maintain a separate academic reputation or it is
highly conditioned on whether or not publications on which they are
authors will receive visibility and citation. \citep{howisonIncentivesIntegrationScientific2013} 
\end{quote}

For an example in Neuroscience, one can browse the papers that cite the
DeepLabCut paper \citep{mathisDeepLabCutMarkerlessPose2018a}  to
find hundreds of downstream projects that make various extensions and
improvements that are not integrated into the main library. While the
alternative extreme of a single monolithic ur-library is also
undesirable, working in fragmented islands makes infrastructure a random
walk instead of a cumulative effort.

After publication, scientists have little incentive to \textbf{maintain}
software outside of the domains in which the primary contributors use
it, so outside of the most-used libraries most scientific software is
brittle and difficult to use \citep{mangulImprovingUsabilityArchival2019, kumarBioinformaticsSoftwareBiologists2007} .

Since the reputational value of a publication depends on its placement
within a journal and number of citations (among other metrics), citation
practices for scientific software are far from uniform and universal,
and relatively few ``prestige'' journals publish software papers at all,
the incentive to write scientific software in the first place is low
compared to its near-universal use \citep{howisonSoftwareScientificLiterature2016} .


\subsection{Domain-Specific Silos}

When funding exists for scientific infrastructure development, it
typically comes in the form of side effects from, or administrative
supplements to research grants. The NIH describes as much in their
Strategic Plan for Data Science \citep{NIHStrategicPlan2018} :

\begin{quote}
from 2007 to 2016, NIH ICs used dozens of different funding strategies
to support data resources, most of them linked to research-grant
mechanisms that prioritized innovation and hypothesis testing over user
service, utility, access, or efficiency. In addition, although the need
for open and efficient data sharing is clear, where to store and access
datasets generated by individual laboratories---and how to make them
compliant with FAIR principles---is not yet straightforward. Overall, it
is critical that the data-resource ecosystem become seamlessly
integrated such that different data types and information about
different organisms or diseases can be used easily together rather than
existing in separate data ``silos'' with only local utility.
\end{quote}

The National Library of Medicine within the NIH currently lists 122
separate databases in its
\href{https://eresources.nlm.nih.gov/nlm_eresources/}{search tool}, each
serving a specific type of data for a specific research community.
Though their current funding priorities signal a shift away from
domain-specific tools, the rest of the scientific software system
consists primarily of tools and data formats purpose-built for a
relatively circumscribed group of scientists without any framework for
their integration. Every field has its own challenges and needs for
software tools, but there is little incentive to build tools that serve
as generalized frameworks to integrate them.


\subsection{``The Long Now'' of Immediacy
vs.~Idealism}

Digital infrastructure development takes place at multiple timescales
simultaneously --- from the momentary work of implementing it; through
longer timescales of planning, organization, and documenting; to the
imagined indefinite future of its use --- what Ribes and Finholt call
``The Long Now. \citep{ribesLongNowTechnology2009} ''
Infrastructural projects constitutively need to contend with the need
for immediately useful results vs.~general and robust systems; the need
to involve the effort of skilled workers vs.~the uncertainty of future
support; the balance between stability and mutability; and so on. The
tension between hacking something together vs.~building something
sustainable for future use is well-trod territory in the hot-glue and
exposed wiring of systems neuroscience rigs.

Deinfrastructuring divides the incentives and interests of junior and
senior researchers. ECRs might be interested in developing tools they'll
use throughout their careers, but given the pressure to establish their
reputation with publications rarely have the time to develop something
fully. The time pressure never ends, and established researchers also
need to push enough publications through the door to be able to secure
the next round of funding. The time preference of scientific software
development is thus very short: hack it together, get the paper out,
we'll fix it later.

The constant need to produce software that \emph{does something} in the
context of scientific programming which largely lacks the institutional
systems and expert mentorship needed for well-architected software means
that most programmers \emph{never} have a chance to learn best practices
commonly accepted in software engineering. As a consequence, a lot of
software tools are developed by near-amateurs with no formal software
training, contributing to their brittleness \citep{altschulAnatomySuccessfulComputational2013} .

The problem of time horizon in development is not purely a product of
inexperience, and a longer time horizon is not uniformly better. We can
look to the history of the semantic web, a project that was intended to
bridge human and computer-readable content on the web, for cautionary
tales. In the semantic web era, thousands of some of the most gifted
programmers and some of the original architects of the internet worked
with an eye to the indefinite future, but the raw idealism and neglect
of the pragmatic reality of the need for software to \emph{do something}
drove many to abandon the effort (bold is mine, italics in original):

\begin{quote}
\textbf{But there was no \emph{use} of it.} I wasn't using any of the
technologies for anything, except for things related to the technology
itself. The Semantic Web is utterly inbred in that respect. The problem
is in the model, that we create this metaformat, RDF, and \emph{then}
the use cases will come. But they haven't, and they won't. Even the
genealogy use case turned out to be based on a fallacy. The very few use
cases that there are, such as Dan Connolly's hAudio export process,
don't justify hundreds of eminent computer scientists cranking out
specification after specification and API after API.

When we discussed this on the Semantic Web Interest Group, the
conversation kept turning to how the formats could be fixed to make the
use cases that I outlined happen. ``Yeah, Sean's right, let's fix our
languages!'' But \textbf{it's not the languages which are broken,}
except in as much as they are entirely broken: because \textbf{it's the
\emph{mentality} of their design which is broken.} You can't, it has
turned out, make a metalanguage like RDF and then go looking for use
cases. We thought you could, but you can't. It's taken eight years to
realise. \citep{palmerDitchingSemanticWeb2008} 
\end{quote}

Developing digital infrastructure must be both bound to fulfilling
immediate, incremental needs as well as guided by a long-range vision.
The technical and social lessons run in parallel: We need software that
solves problems people actually have, but can flexibly support an
eventual form that allows new possibilities. We need a long-range vision
to know what kind of tools we should build and which we shouldn't, and
we need to keep it in a tight loop with the always-changing needs of the
people it supports.

In short, to develop digital infrastructure we need to be
\emph{strategic.} To be strategic we need a \emph{plan.} To have a plan
we need to value planning as \emph{work.} On this, Ribes and Finholt are
instructive:

\begin{quote}
``On the one hand, I know we have to keep it all running, but on the
other, LTER is about long-term data archiving. If we want to do that, we
have to have the time to test and enact new approaches. But if we're
working on the to-do lists, we aren't working on the tomorrow-list''
(LTER workgroup discussion 10/05).

The tension described here involves not only time management, but also
the differing valuations placed on these kinds of work. The implicit
hierarchy places scientific research first, followed by deployment of
new analytic tools and resources, and trailed by maintenance work.
{[}\ldots{]} While in an ideal situation development could be tied to
everyday maintenance, in practice, maintenance work is often invisible
and undervalued. As Star notes, infrastructure becomes visible upon
breakdown, and only then is attention directed at its everyday workings
(1999). Scientists are said to be rewarded for producing new knowledge,
developers for successfully implementing a novel technology, but the
work of maintenance (while crucial) is often thankless, of low status,
and difficult to track. \emph{How can projects support the distribution
of work across research, development, and maintenance?} \citep{ribesLongNowTechnology2009} 
\end{quote}


\subsection{``Neatness'' vs
``Scruffiness''}

Closely related to the tension between ``Now'' and ``Later'' is the
tension between ``Neatness'' and ``Scruffiness.'' Lindsay Poirier traces
its reflection in the semantic web community as the way that differences
in ``thought styles'' result in different ``design logics'' \citep{poirierTurnScruffyEthnographic2017} . On the question of how to
develop technology for representing the ontology of the web -- the
system of terminology and structures with which everything should be
named -- there were (very roughly) two camps. The ``neats'' prioritized
consistency, predictability, uniformity, and coherence -- a logically
complete and formally valid System of Everything. The ``scruffies''
prioritized local systems of knowledge, expressivity, ``believing that
ontologies will evolve organically as everyday webmasters figure out
what schemas they need to describe and link their data. \citep{poirierTurnScruffyEthnographic2017} ''

This tension is as old as the internet, where amidst the
\href{https://en.wikipedia.org/wiki/Dot-com_bubble}{dot-com bubble} a
telecom spokesperson lamented that the internet wasn't controllable
enough to be profitable because ``it was devised by a bunch of hippie
anarchists.'' \citep{hiltzikTamingWildWild2001}  The hippie
anarchists probably agreed, rejecting ``kings, presidents and voting''
in favor of ``rough consensus and running code.'' Clearly, the
difference in thought styles has an unsubtle relationship with beliefs
about who should be able to exercise power and what ends a system should
serve \citep{larsenPoliticalNatureTCP2012} .


\includegraphics[width=\linewidth,]{../assets/images/clark-slide.png} \emph{A
slide from David Clark's 1992 ``Views of the Future''\citep{clarkCloudyCrystalBall1992}  that contrasts differing visions for the
development process of the future of the internet. The struggle between
engineered order and wild untamedness is summarized forcefully as ``We
reject: kings, presidents and voting. We believe in: rough consensus and
running code''}

Practically, the differences between these thought communities impact
the tools they build. Aaron Swartz put the approach of the ``neat''
semantic web architects the way he did:

\begin{quote}
Instead of the ``let's just build something that works'' attitude that
made the Web (and the Internet) such a roaring success, they brought the
formalizing mindset of mathematicians and the institutional structures
of academics and defense contractors. They formed committees to form
working groups to write drafts of ontologies that carefully listed (in
100-page Word documents) all possible things in the universe and the
various properties they could have, and they spent hours in Talmudic
debates over whether a washing machine was a kitchen appliance or a
household cleaning device.

With them has come academic research and government grants and corporate
R\&D and the whole apparatus of people and institutions that scream
``pipedream.'' And instead of spending time building things, they've
convinced people interested in these ideas that the first thing we need
to do is write standards. (To engineers, this is absurd from the
start---standards are things you write after you've got something
working, not before!) \citep{swartzAaronSwartzProgrammable2013} 
\end{quote}

The outcomes of this cultural rift are subtle, but the broad strokes are
clear: the ``scruffies'' largely diverged into the linked data
community, which has taken some of the core semantic web technology like
RDF, OWL, and the like, and developed a broad range of downstream
technologies that have found purchase across information sciences,
library sciences, and other applied domains\footnote{This isn't a story
  of ``good people'' and ``bad people,'' as a lot of the linked data
  technology also serves as the backbone for abusive technology
  monopolies like google's acquisition of Freebase \citep{iainFreebaseDeadLong2019}  and the profusion of knowledge
  graph-based medical platforms.}. The linked data developers, starting
by acknowledging that no one system can possibly capture everything,
build tools that allow expression of local systems of meaning with the
expectation and affordances for linking data between these systems as an
ongoing social process.

The vision of a totalizing and logically consistent semantic web,
however, has largely faded into obscurity. One developer involved with
semantic web technologies (who requested not be named), captured the
present situation in their description of a still-active developer
mailing list:

\begin{quote}
I think that some people are completely detached from practical
applications of what they propose. {[}\ldots{]} I could not follow half
of the messages. these guys seem completely removed from our plane of
existence and I have no clue what they are trying to solve.
\end{quote}

This division in thought styles generalizes across domains of
infrastructure, though outside of the linked data and similar worlds the
dichotomy is more frequently between ``neatness'' and ``people doing
whatever'' -- with integration and interoperability becoming nearly
synonymous with standardization. Calls for standardization without
careful consideration and incorporation of existing practice have a
familiar cycle: devise a standard that will solve everything, implement
it, wonder why people aren't using it, funding and energy dissipiates,
rinse, repeat. The difficulty of scaling an exacting vision of how data
should be formatted, the tools researchers should use for their
experiments, and so on is that they require dramatic and sometimes total
changes to the way people do science. The alternative is not between
standardization and chaos, but a potential third way is designing
infrastructures that allow the diversity of approaches, tools, and
techniques to be expressed in a common framework or protocol along with
the community infrastructure to allow the continual negotiation of their
relationship.


\subsection{Taped-on Interfaces: Open-Loop User
Testing}

The point of most active competition in many domains of commercial
software is the user interface and experience (UI/UX), and to compete
software companies will exhaustively user-test and refine them with
pixel precision to avoid any potential customer feeling even a
thimbleful of frustration. Scientific software development is largely
disconnected from usability testing, as what little support exists is
rarely tied to it. This, combined with the preponderance of
semi-amateurs and above incentives for developing new packages -- and
thus reduplicating the work of interface development -- make it perhaps
unsurprising that most scientific software is hard to use!

I intend the notion of ``interface'' in an expansive way: In addition to
the graphical user interface (GUI) exposed to the end-user, I am
referring generally to all points of contact with users, developers, and
other software. Interfaces are intrinsically social, and include the
surrounding documentation and experience of use --- part of using an API
is being able to figure out how to use it! The typical form of
scientific software is a black box: I implemented an algorithm of some
kind, here is how to use it, but beneath the surface there be dragons.

Ideally, software would be designed with programming interfaces and
documentation at multiple scales of complexity to enable clean
entrypoints for developers with differing levels of skill and investment
to contribute. Additionally, it would include interfaces for use and
integration with other software. Without care given to either of these
interfaces, the community of co-developers is likely to remain small,
and the labor they expend is less likely to be useful outside that
single project. This, in turn, closes the loop with incentives to
develop new packages and makes another vicious cycle reinforcing
fragmentation\footnote{Incentivized to develop new packages
  -\textgreater{} need to reinvent interfaces -\textgreater{} hard to
  develop and extend -\textgreater{} incentivized to develop new
  packages}.


\subsection{Platforms, Industry Capture, and the Profit
Motive}

Publicly funded science is an always-irresistable golden goose for
private industry. The fragmented interests of scientists and the
historically light touch of funding agencies on encroaching
privatization means that if some company manages to capture and
privatize a corner of scientific practice they are likely to keep it.
Industry capture has been thoroughly criticized in the context of the
journal system (eg. recently, \citep{brembsReplacingAcademicJournals2021} ), and that criticism should
extend to the rest of our infrastructure as information companies seek
to build a for-profit platform system that spans the scientific workflow
(eg. \citep{ElsevierSevenBridges2017} ). The mode of privatization
of scientific infrastructure follows the broader software market as a
proliferation of software as a service (SaaS), from startups to
international megacorporations, that rent access to some, typically
proprietary software without selling the software itself.

While in isolation SaaS can make individual components of the
infrastructural landscape easier to access --- and even free!!* --- the
business model is fundamentally incompatible with integrated and
accessible infrastructure. The SaaS model derives revenue from
subscription or use costs, often operating as ``freemium'' models that
make some subset of its services available for free. Even in freemium
models, though, the business model requires that some functionality of
the platform is enclosed and proprietary. To keep the particular domain
of enclosure viable as a profit stream, the proprietor needs to actively
defend against competitors as well as any technology that might fill the
need for the proprietary technology\footnote{eg. see the complaint in
  State of Texas et al.~v. Google that alleges Google rigs ad markets
  designed to lessen its dominance and uses its control over Chrome and
  Android to create a single, always-on tracking ecosystem owned only by
  them \citep{ReGoogleDigital2021} } (See a more thorough
treatment of platform capitalism in science in \citep{mirowskiFutureOpenScience2018} )

As isolated services, one can imagine the practice of science devolving
along a similar path as the increasingly-fragmented streaming video
market: to do my work I need to subscribe to a data storage service, a
cloud computing service, a platform to host my experiments, etc. For
larger software platforms, however, vertical integration of multiple
complementary services makes their impact on infrastructure more
insidious. Locking users into more and more services makes for more and
more revenue, which encourages platforms to be as mutually incompatible
as they can get away with \citep{macinnesCompatibilityStandardsMonopoly2005} . To encourage adoption,
platforms that can offer multiple services may offer one of the services
-- say, data storage -- for free, forcing the user to use the adjoining
services -- say, a cloud computing platform.

Since these platforms are often subsidiaries of information industry
monopolists, scientists become complicit in their often profoundly
unethical behavior of by funneling millions of dollars into them.
Longterm, unconditional funding of wildly profitable journals has
allowed conglomerates like Elsevier to become sprawling surveillance
companies \citep{RELXAnnualReport2020, pooleySurveillancePublishing2021}  that are sucking as much data up
as they can to market derivative products like algorithmic ranking of
scientific productivity \citep{brembsAlgorithmicEmploymentDecisions2021}  and making data sharing
agreements with ICE \citep{biddleLexisNexisProvideGiant2021} . Or
our reliance on AWS and the laundry list of human rights abuses by
Amazon \citep{CriticismAmazon2021} . In addition to lock-in,
dependence on a constellation of SaaS allows the opportunity for
platform-holders to take advantage of their limitations and \emph{sell
us additional services to make up for what the other ones purposely
lack} --- for example Elsevier has taken advantage of our dependence on
the journal system and its strategic disorganization to sell a tool for
summarizing trending research areas for tailoring maximally-fundable
grants \citep{elsevierTopicProminenceSciencea} .

Funding models and incentive structures in science are uniformly aligned
towards the platformatization of scientific infrastructure. Aside from
the corporate doublespeak ``technology transfer'' rhetoric that pervades
the neoliberal university, the relative absence of major funding
opportunities for scientific software developers competitive with the
profit potential from ``industry'' often leaves it as the only viable
career path. The preceding structural constraints on local
infrastructural development strongly incentivize labs and researchers to
rely on SaaS that provides a readymade solution to specific problems.
Distressingly, rather than supporting infrastructural development that
would avoid obligate payments to platform-holders, funding agencies seem
all too happy to lean into them (emphases mine):

\begin{quote}
NIH will \textbf{leverage what is available in the private sector,}
either through strategic partnerships or procurement, to create a
workable \textbf{Platform as a Service (PaaS)} environment. {[}\ldots{]}
NIH will partner with cloud-service providers for cloud storage,
computational, and related infrastructure services needed to facilitate
the deposit, storage, and access to large, high-value NIH datasets.
{[}\ldots{]}

NIH's cloud-marketplace initiative will be the first step in a phased
operational framework that \textbf{establishes a SaaS paradigm for NIH
and its stakeholders.} (-NIH Strategic Plan for Data Science, 2018 \citep{NIHStrategicPlan2018} )
\end{quote}

The articulated plan being to pay platform holders to house data while
also paying for the labor to maintain those databases veers into parody,
haplessly building another triple-pay industry \citep{buranyiStaggeringlyProfitableBusiness2017}  into the economic system
of science --- one can hardly wait until they have the opportunity to
rent their own data back with a monthly subscription. This isn't a
metaphor: the STRIDES program, with the official subdomain
\href{https://web.archive.org/web/20210729131920/https://cloud.nih.gov/}{cloud.nih.gov},
has been authorized to pay \$85 million to cloud providers since 2018.
In exchange, NIH hasn't received any sort of new technology, but
\href{https://web.archive.org/web/20211006003547/https://cloud.nih.gov/enrollment/account-type/}{``extramural''}
scientists receive a maximum discount of 25\% on cloud storage and
``data egress'' fees as well as plenty of training on how to give
control of the scientific process to platform giants \citep{reillyNIHSTRIDESInitiative2021} \footnote{Their success stories tell
  the story of platform non-integration where scientists have to
  handbuild new tools to manage their data across multiple cloud
  environments: ``We have been storing data in both cloud environments
  because we wanted the ecosystem we are creating to work on both
  clouds'' \citep{STRIDESInitiativeSuccess2020} }. With platforms,
without exaggeration we pay them to let us pay for something that makes
it so we need to pay them more later.

It is unclear to me whether this is the result of the cultural hegemony
of platform capitalism narrowing the space of imaginable
infrastructures, industry capture of the decision-making process, or
both, but the effect is the same in any case.


\subsection{Protection of Institutional and Economic
Power}

Aside from information industries, infrastructural deficits are
certainly not without beneficiaries within science --- those that have
already accrued power and status.

Structurally, the adoption of SaaS on a wide scale necessarily
sacrifices the goals of an integrated mass infrastructure as the
practice of research is carved into small, marketable chunks within
vertically integrated technology platforms. Worse, it stands to amplify,
rather than reduce, inequities in science, as the labs and institutes
that are able to afford the tolls between each of the weigh stations of
infrastructure are able to operate more efficiently --- one of many
positive feedback loops of inequity.

More generally, incentives across infrastructures are often misaligned
across strata of power and wealth. Those at the top of a power hierarchy
have every incentive to maintain the fragmentation that prevents people
from competing --- hopefully mostly unconsciously via uncritically
participating in the system rather than maliciously reinforcing it.

This poses an organizational problem: the kind of infrastructure that
unwinds platform ownership is not only unprofitable, it's
\textbf{anti-profitable} -- making it impossible to profit from its
domain of use. That makes it difficult to rally the kind of development
and \href{https://www.snsi.info/}{lobbying} resources that profitable
technology can, requiring organization based on ethical principles and a
commitment to sacrifice control in order to serve a practical need.

The problem is not insurmountable, and there are strategic advantages to
decentralized infrastructure and its development within science.
Centralized technologies and companies might have more concerted power,
but we have \emph{numbers} and can make tools that let us combine small
amounts of labor from many people. We often start (and end) our dreams
of infrastructure with the belief that they will necessarily cost a lot
of \emph{money,} but that's propaganda. Of course development isn't
\emph{free,} but the cost of decentralized technologies is far smaller
than the vast sums of money funnelled into industry profits, labor hours
spent compensating for the designed inefficiencies of the platform
model, and the development of a fragmented tool ecosystem built around
them.

Science, as one of few domains of non-economic labor, has the
opportunity to be a seed for decentralized technologies that could
broadly improve not only the health of scientific practice, but the
broader information ecosystem. We can develop a plan and mobilize to
make use of our collective expertise to build tools that have no
business model and no means of development in commercial domains --- we
just need to realize what's at stake and agree that the health of
science is more important than the convenience of the cloud\footnote{Though
  the system of engineered helpless that convinces us that we're
  incapable of managing our own web infrastructure is not actually as
  reliable and seamless as it claims, as the long history of dramatic
  outages at AWS can show us \citep{lawlerAmazonServerOutage2021, hutchinsonAmazonWebServices2012} } or which journal our papers go
into. 




\section{The Ivies, Institutes, and ``The Rest of
Us''}



 Given these constraints, who can build new digital
infrastructure? Constraints, motivations, and strategies all depend on
the circumstance of those doing the development. The undone work of
infrastructure is being nibbled at around the edges\footnote{aka doing
  hard development work in sometimes adverse conditions.} by several
different kinds of organization already ranging in scale and structure.
A short survey to give us some notion of how we should seek to organize
infrastructure building:


\subsection{Institutional Core
Facilities}

Centralized ``core'' facilities are maybe the most typical form of
infrastructure development and resource sharing at the level of
departments and institutions. These facilities can range from minimal to
baroque extravagance depending on institutional resources and whatever
complex web of local history brought them about.

A
\href{https://reporter.nih.gov/project-details/9444124\#sub-Projects}{subproject}
within a
\href{https://projectreporter.nih.gov/project_info_details.cfm?aid=9444124}{PNI
Systems Core} grant echoes a lot of the thoughts here, particularly
regardling effort duplication\footnote{Thanks a lot to the one-and-only
  stunning and brilliant Dr.~Eartha Mae Guthman for suggesting looking
  at the BRAIN initiative grants as a way of getting insight on core
  facilities.}:

\begin{quote}
Creating an Optical Instrumentation Core will address the problem that
much of the technical work required to innovate and maintain these
instruments has shifted to students and postdocs, because it has
exceeded the capacity of existing staff. This division of labor is a
problem for four reasons: (1) lab personnel often do not have sufficient
time or expertise to produce the best possible results, (2) the
diffusion of responsibility leads people to duplicate one another's
efforts, (3) researchers spend their time on technical work at the
expense of doing science, and (4) expertise can be lost as students and
postdocs move on. For all these reasons, we propose to standardize this
function across projects to improve quality control and efficiency.
Centralizing the design, construction, maintenance, and support of these
instruments will increase the efficiency and rigor of our microscopy
experiments, while freeing lab personnel to focus on designing
experiments and collecting data.
\end{quote}

While core facilities are an excellent way of expanding access, reducing
redundancy, and standardizing tools within an instutition, as commonly
structured they can displace work spent on those efforts outside of the
institution. Elite institutions can attract the researchers with the
technical knowledge to develop the instrumentation of the core and
infrastructure for maintain it, but this development is only
occasionally made usable by the broader public. The Princeton data
science core is an excellent example of a core facility that does makes
its software infrastructure development
\href{https://github.com/BrainCOGS}{public}\footnote{
  "\href{https://projectreporter.nih.gov/project\_info\_description.cfm?aid=9444126\&icde=0}{Project Summary: Core 2, Data Science} {[}\ldots{]} In addition, the
  Core will build a data science platform that stores behavior, neural
  activity, and neural connectivity in a relational database that is
  queried by the DataJoint language. {[}\ldots{]} This data-science
  platform will facilitate collaborative analysis of datasets by
  multiple researchers within the project, and make the analyses
  reproducible and extensible by other researchers." {[}\ldots{]}}, which they should be applauded for, but also
illustrative of the problems with a core-focused infrastructure project.
For an external user, the documentation and tutorials are incomplete --
it's not clear to me how one would set this up for my institute, lab, or
data, and there are several places of hard-coded princeton-specific
values that I am unsure how exactly to adapt\footnote{Though again, this
  project is examplary, built by friends, and would be an excellent
  place to start extending towards global infrastructure.}. I would
consider this example a high-water mark, and the median openness of core
infrastructure falls far below it. I was unable to find an example of a
core facility that maintained publicly-accessible documentation on the
construction and operation of its experimental infrastructure or the
management of its facility.

This might be unsurprising given the economic structure of most core
facilities: an institution pays for a core to benefit the institution,
and downstream public benefits are a nice plus but not high up in the
list of concerns (if present at all). Core facilities are thus unlikely
to serve as the source of mass infrastructure, but they do serve as a
point of local coordination within institutions, and so given some
larger means of coordination may still be useful.


\subsection{Centralized Institutes}

Outside of universities, the Allen Brain Institute is perhaps the most
impactful reflection of centralization in neuroscience. The Allen
Institute has, in an impressively short period of time, created several
transformative tools and datasets, including its well-known atlases \citep{leinGenomewideAtlasGene2007}  and the first iteration of its
\href{http://observatory.brain-map.org/}{Observatory} project which
makes a massive, high-quality calcium imaging dataset of visual cortical
activity available for public use. They also develop and maintain
software tools like their
\href{https://allensdk.readthedocs.io/en/latest/}{SDK} and Brain
Modeling Toolkit \href{https://alleninstitute.github.io/bmtk/}{(BMTK)},
as well as a collection of
\href{https://portal.brain-map.org/explore/toolkit/hardware}{hardware
schematics} used in their experiments. The contribution of the Allen
Institute to basic neuroscientific infrastructure is so great that,
anecdotally, when talking about scientific infrastructure it's not
uncommon for me to hear something along the lines of ``I thought the
Allen was doing that.''

Though the Allen Institute is an excellent model for scale at the level
of a single organization, its centralized, hierarchical structure cannot
(and does not attempt to) serve as the backbone for all neuroscientific
infrastructure. Performing single (or a small number of, as in its
also-admirable
\href{https://alleninstitute.org/what-we-do/brain-science/news-press/articles/three-collaborative-studies-launch-openscope-shared-observatory-neuroscience}{OpenScope
Project}) carefully controlled experiments a huge number of times is an
important means of studying constrained problems, but is complementary
with the diversity of research questions, model organisms, and methods
present in the broader neuroscientific community.

Christof Koch, its director, describes the challenge of centrally
organizing a large number of researchers:

\begin{quote}
Our biggest institutional challenge is organizational: assembling,
managing, enabling and motivating large teams of diverse scientists,
engineers and technicians to operate in a highly synergistic manner in
pursuit of a few basic science goals \citep{grillnerWorldwideInitiativesAdvance2016} 
\end{quote}

\begin{quote}
These challenges grow as the size of the team grows. Our anecdotal
evidence suggests that above a hundred members, group cohesion appears
to become weaker with the appearance of semi-autonomous cliques and
sub-groups. This may relate to the postulated limit on the number of
meaningful social interactions humans can sustain given the size of
their brain \citep{kochBigScienceTeam2016} 
\end{quote}

These institutes too are certainly helpful in building core technologies
for the field, but they aren't necessarily organized for developing
mass-scale infrastructure. They reflect the capabilities and needs of
the institute itself, which are likely to be radically different than a
small lab. They can build technologies on a background of expensive
cloud storage and computation and rely on a team of engineers to
implement and maintain them. So while the tools they make are certainly
\emph{useful} we shouldn't count on them to build the systems we need
for scientists at large.


\subsection{Meso-scale
collaborations}

Given the diminishing returns to scale for centralized organizations,
many have called for smaller, ``meso-scale'' collaborations and
consortia that combine the efforts of multiple labs \citep{mainenBetterWayCrack2016} . The most successful consortium of this
kind has been the International Brain Laboratory \citep{abbottInternationalLaboratorySystems2017, woolKnowledgeNetworksHow2020} , a group of 22 labs spread across six countries. They have been
able to realize the promise of big team neuroscience, setting a new
standard for performing reproducible experiments performed by many labs
\citep{laboratoryStandardizedReproducibleMeasurement2020}  and
developing data management infrastructure to match \citep{laboratoryDataArchitectureLargescale2020}  (seriously, don't miss
their extremely impressive
\href{https://data.internationalbrainlab.org/}{data portal}). Their
project thus serves as the benchmark for large-scale collaboration and a
model from which all similar efforts should learn from.

Critical to the IBL's success was its adoption of a flat,
non-hierarchical organizational structure, as described by Lauren E.
Wool:

\begin{quote}
IBL's virtual environment has grown to accommodate a diversity of
scientific activity, and is supported by a flexible, `flattened'
hierarchy that emphasizes horizontal relationships over vertical
management. {[}\ldots{]} Small teams of IBL members collaborate on
projects in Working Groups (WGs), which are defined around particular
specializations and milestones and coordinated jointly by a chair and
associate chair (typically a PI and researcher, respectively). All WG
chairs sit on the Executive Board to propagate decisions across WGs,
facilitate operational and financial support, and prepare proposals for
voting by the General Assembly, which represents all PIs. \citep{woolKnowledgeNetworksHow2020} 
\end{quote}

They should also be credited with their adoption of a form of consensus
decision-making, \href{https://sociocracy.info}{sociocracy}, rather than
a majority-vote or top-down decisionmaking structure. Consensus
decision-making systems are derived from those developed by
\href{https://rhizomenetwork.wordpress.com/2011/06/18/a-brief-history-of-consenus-decision-making/}{Quakers
and some Native American nations}, and emphasize, perhaps
unsurprisingly, the value of collective consent rather than the will of
the majority.

The central lesson of the IBL, in my opinion, is that governance
matters. Even if a consortium of labs were to form on an ad-hoc basis,
without a formal system to ensure contributors felt heard and empowered
to shape the project it would soon become unsustainable. Even if this
system is not perfect, with some labor still falling unequally on some
researchers, it is a promising model for future collaborative consortia.

The infrastructure developed by the IBL is impressive, but its focus on
a single experiment makes it difficult to expand and translate to
widescale use. The hardware for the IBL experimental apparatus is
exceptionally well-documented, with a
\href{https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732}{complete
and detailed build guide} and
\href{https://figshare.com/articles/online_resource/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_CAD_files_for_behavior_rig/11639973}{library
of CAD parts}, but the documentation is not modularized such that it
might facilitate use in other projects, remixed, or repurposed. The
\href{https://github.com/int-brain-lab/iblrig}{experimental software} is
similarly single-purpose, a chimeric combination of Bonsai \citep{lopesBonsaiEventbasedFramework2015}  and
\href{https://github.com/pybpod/pybpod}{PyBpod}
\href{https://github.com/int-brain-lab/iblrig/tree/master/tasks/_iblrig_tasks_ephysChoiceWorld}{scripts}.
It unfortunately
\href{https://iblrig.readthedocs.io/en/latest/index.html}{lacks} the
API-level documentation that would facilitate use and modification by
other developers, so it is unclear to me, for example, how I would use
the experimental apparatus in a different task with perhaps slightly
different hardware, or how I would then contribute that back to the
library. The experimental software, according to the
\href{https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732}{PDF
documentation}, will also not work without a connection to an
\href{https://github.com/cortex-lab/alyx}{alyx} database. While alyx was
intended for use outside the IBL, it still has
\href{https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/management/commands/files.py\#L188}{IBL-specific}
and
\href{https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/fixtures/data.datasettype.json\#L29}{task-specific}
values in its source-code, and makes community development difficult
with a similar \href{https://alyx.readthedocs.io/en/latest/}{lack} of
API-level documentation and requirement that users edit the library
itself, rather than temporary user files, in order to use it outside the
IBL.

My intention is not to denigrate the excellent tools built by the IBL,
nor their inspiring realization of meso-scale collaboration, but to
illustrate a problem that I see as an extension of that discussed in the
context of core facilities --- designing infrastructure for one task, or
one group in particular makes it much less likely to be portable to
other tasks and groups. This argument is much more contingent on the
specific circumstances of the consortium than the prior arguments about
core facilities and institutes: when organized with mass-infrastructure
in mind, collaborations between semi-autonomous groups across
institutions could be a powerful mode of tool development.

It is also unclear how replicable these consortia are, and whether they
challenge, rather than reinforce technical inequity in science.
Participating in consortia systems like the IBL requires that labs have
additional funding for labor hours spent on work for the consortium, and
in the case of graduate students and postdocs, that time can conflict
with work on their degrees or personal research which are still far more
potent instruments of ``remaining employed in science'' than
collaboration. In the case that only the most well-funded labs and
institutions realize the benefits of big team science without explicit
consideration given to scientific equity, mesoscale collaborations could
have the unintended consequence of magnifying the skewed distribution of
access to technical expertise and instrumentation.


\subsection{The rest of us\ldots{}}

Outside of ivies with rich core facilities, institutes like the Allen,
or nascent multi-lab consortia, the rest of us are largely on our own,
piecing together what we can from proprietary and open source
technology. The world of open source scientific software has plenty of
energy and lots of excellent work is always being done, though
constrained by the circumstances of its development described briefly
above. Anything else comes down to whatever we can afford with remaining
grant money, scrape together from local knowledge, methods sections,
begging, borrowing, and (hopefully not too much) stealing from
neighboring labs.

The state of broader scientific deinfrastructuring is perhaps to be
expected given its dependence on informational monopolies that in some
part depend on it, but unlike many other industries or professions there
is reason for hope in science. Science is packed with people with an
enormous diversity of skills, resources, and perspectives. Publicly
funded science is relatively unique as a labor system that does not
strictly depend on profit. There is widespread discontent with the
systems of scientific practice, and so the question becomes how we can
organize our skill, labor, and energy to rebuild the systems that
constrain us.

A third option from the standardization offered by centralization and
the blooming, buzzing, beautiful chaos of disconnected open-source
development is that of decentralized systems, and with them we might
build the means by which the ``rest of us'' can mutually benefit by
capturing and making use of each other's knowledge and labor.





\chapter{A Draft of Decentralized Scientific
Infrastructure}



 What should we build?

The decentralized infrastructure I will describe here is similar to
previous notions of ``grass-roots'' science articulated within systems
neuroscience \citep{mainenBetterWayCrack2016} , ``small tech'' in
the web development world \citep{balkanSmallTechnologyFoundation} , and shares some of the motivations of the
\href{https://solidproject.org/}{Solid project} \citep{sambraSolidPlatformDecentralized2016} , but has broad and deep
history in many domains of computing. My intention is to provide a more
prescriptive scaffolding for its design and potential implementation as
a way of painting a picture of what science could be like. This sketch
is not intended to be final, but a starting point for further
negotiation and refinement.

Throughout this section, when I am referring to any particular piece of
software I want to be clear that I don't intend to be dogmatically
advocating that software \emph{in particular}, but software \emph{like
it} that \emph{shares its qualities} --- no snake oil is sold in this
document. Similarly, when I describe limitations of existing tools,
without exception I am describing a tool or platform I love, have
learned from, and think is valuable --- learning from something can mean
drawing respectful contrast!


\section{Design Principles}

I won't attempt to derive a definition of decentralized systems from
base principles here, but from the systemic constraints described above,
some design principles that illustrate the idea emerge naturally. For
the sake of concrete illustration, in some of these I will additionally
draw from the architectural principles of the internet protocols: the
most successful decentralized digital technology project to date.


\subsection{Protocols, not Platforms}

Much of the basic technology of the internet was developed as protocols
that describe the basic attributes and operations of a process. A simple
and common example is email over SMTP (Simple Mail Transfer Protocol)
\citep{Rfc5321SimpleMail} . SMTP describes a series of steps that
email servers must follow to send a message: the sender initiates a
connection to the recipient server, the recipient server acknowledges
the connection, a few more handshake steps ensue to describe the senders
and receivers of the message, and then the data of the message is
transferred. Any software that implements the protocol can send and and
receive emails to and from any other. The protocol basis of email is the
reason why it is possible to send an email from a gmail account to a
hotmail account (or any other hacky homebrew SMTP client) despite being
wholly different pieces of software.

In contrast, \emph{platforms} provide some service with a specific body
of code usually without any pretense of generality. In contrast to email
over SMTP, we have grown accustomed to not being able to send a message
to someone using Telegram from WhatsApp, switching between multiple
mutually incompatible apps that serve nearly identical purposes.
Platforms, despite being \emph{theoretically} more limited than
associated protocols, are attractive for many reasons: they provide
funding and administrative agencies a single point of contracting and
liability, they typically provide a much more polished user interface,
and so on. These benefits are short-lived, however, as the inevitable
toll of lock-in and shadowy business models is realized.


\subsection{Integration, not
Invention}

At the advent of the internet protocols, several different institutions
and universities had already developed existing network infrastructures,
and so the ``top level goal'' of IP was to ``develop an effective
technique for multiplex utilization of existing interconnected
networks,'' and ``come to grips with the problem of integrating a number
of separately administered entities into a common utility'' \citep{clarkDesignPhilosophyDARPA1988} . As a result, IP was developed as a
`common language' that could be implemented on any hardware, and upon
which other, more complex tools could be built. This is also a cultural
practice: when the system doesn't meet some need, one should try to
extend it rather than building a new, separate system --- and if a new
system is needed, it should be interoperable with those that exist.

This point is practical as well as tactical: to compete, an emerging
protocol should integrate or be capable of bridging with the
technologies that currently fill its role. A new database protocol
should be capable of reading and writing existing databases, a new
format should be able to ingest and export to existing formats, and so
on. The degree to which switching is seamless is the degree to which
people will be willing to switch.

This principle runs directly contrary to the current incentives for
novelty and fragmentation and the dominant economic model of software
platforms, which must be counterbalanced by design choices elsewhere.


\subsection{Embrace Heterogeneity, Be
Uncoercive}

A reciprocal principle to integration with existing systems is to design
the system to be integratable with existing practice. Decentralized
systems need to anticipate unanticipated uses, and can't rely on
potential users making dramatic changes to their existing practices. For
example, an experimental framework should not insist on a prescribed set
of supported hardware and rigid formulation for describing experiments.
Instead it should provide affordances that give a clear way for users to
extend the system to fit their needs \citep{carpenterRFC1958Architectural1996} . In addition to integrating with
existing systems, it must be straightforward for future development to
be integrated. This idea is related to ``the test of independent
invention'', summarized with the question ``if someone else had already
invented your system, would theirs work with yours?'' \citep{berners-leePrinciplesDesign1998} .

This principle also has tactical elements. An uncoercive system allows
users to gradually adopt it rather than needing to adopt all of its
components in order for any one of them to be useful. There always needs
to be a \emph{benefit} to adopting further components of the system to
encourage \emph{voluntary} adoption, but it should never be
\emph{compulsory.} For example, again from experimental frameworks, it
should be possible to use it to control experimental hardware without
needing to use the rest of the experimental design, data storage, and
interface system. To some degree this is accomplished with a modular
system design where designers are mindful of keeping the individual
modules independently useful.

A noncoercive architecture also prioritizes the ease of leaving. Though
this is somewhat tautological to protocol-driven design, specific care
must be taken to enable export and migration to new systems. Making
leaving easy also ensures that early missteps in development of the
system are not fatal to its development, preventing lock-in to a
component that needs to be restructured.


\subsection{Empower People, not
Systems}

Because IP was initially developed as a military technology by DARPA, a
primary design constraint was survivability in the face of failure. The
model adopted by internet architects was to move as much functionality
as possible from the network itself to the end-users of the network ---
rather than the network itself guaranteeing a packet is transmitted, the
sending computer will do so by requiring a response from the recipient
\citep{clarkDesignPhilosophyDARPA1988} .

For infrastructure, we should make tools that don't require a central
team of developers to maintain, a central server-farm to host data, or a
small group of people to govern. Whenever possible, data, software, and
hardware should be self-describing\footnote{AKA you shouldn't need to
  resort to some external source to understand it. Data should come
  packaged with clear metadata, software should have its own docs, etc.},
so one needs minimal additional tools or resources to understand and use
it. It should never be the case that funding drying up for one node in
the system causes the entire system to fail.

Practically, this means that the tools of digital infrastructure should
be deployable by individual people and be capable of recapitulating the
function of the system without reference to any central authority.
Researchers need to be given control over the function of
infrastructure: from controlling sharing permissions for eg. clinically
sensitive data to assurance that their tools aren't spying on them.
Formats and standards must be negotiable by the users of a system rather
than regulated by a central governance body.


\subsection{Infrastructure is
Social}

The alternative to centralized governing and development bodies is to
build the tools for community control over infrastructural components.
This is perhaps the largest missing piece in current scientific tooling.
On one side, decentralized governance is the means by which an
infrastructure can be maintained to serve the ever-evolving needs of its
users. On the other, a sense of community ownership is what drives
people to not only adopt but contribute to the development of an
infrastructure. In addition to being a source of all the warm fuzzies of
socially affiliative ``community-ness,'' any collaborative system needs
a way of ensuring that the practice of maintaining, building, and using
it is designed to \emph{visibly and tangibly benefit} those that do,
rather than be relegated to a cabal of invisible developers and
maintainers \citep{grudinGroupwareSocialDynamics1994, randallDistributedOntologyBuilding2011} .

Governance and communication tools also make it possible to realize the
infinite variation in application that infrastructures need while
keeping them coherent: tools must be built with means of bringing the
endless local conversations and modifications of use into a common space
where they can become a cumulative sense of shared memory.

I will return to this idea in
\protect\hyperlink{archives-need-communities}{Archives Need Communities}
in the context of social dynamics of private bittorrent trackers, as
well as propose a set of basic communication and governance tools in
\protect\hyperlink{rebuilding-scientific-communication}{Rebuilding
Scientific Communication}.


\subsection{Usability Matters}

It is not enough to build a technically correct technology and assume it
will be adopted or even useful, it must be developed embedded within
communities of practice and \emph{be useful for solving problems that
people actually have.} We should learn from the struggles of the
semantic web project. Rather than building a fully prescriptive and
complete system first and deploying it later, we should develop tools
whose usability is continuously improved \emph{en route} to a (flexible)
completed vision.

The adage from RFC 1958\footnote{A ``request for comment'' from the
  Network Working Group of the Internet Engineering Task Force on the
  architecture of the internet. The IETF designs many of the protocols
  that serve as the backbone of the internet.} ``nothing gets
standardized until there are multiple instances of running code'' \citep{carpenterRFC1958Architectural1996}  captures the dual nature of
the constraint well. Workable standards don't emerge until they have
been extensively tested in the field, but development without an eye to
an eventual protocol won't make one.

We should read the
\href{https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish}{gobbling
up} of open protocols into proprietary platforms that defined ``Web
2.0'' as instructive (in addition to a demonstration of the raw power of
concentrated capital) \citep{markoffTomorrowWorldWide1996} .
\emph{Why} did Slack outcompete IRC?\footnote{\href{https://en.wikipedia.org/wiki/Internet_Relay_Chat}{IRC},
  internet relay chat, was a messaging system that served many of the
  same functions as the group messaging program
  \href{https://slack.com/}{Slack} serves now. Also see its more active
  cousin \href{https://en.wikipedia.org/wiki/XMPP}{XMPP}} The answer is
relatively simple: it was relatively simple to use. Using a contemporary
example, to
\href{https://matrix-org.github.io/synapse/latest/setup/installation.html}{set
up a Synapse server} to communicate over
\href{https://matrix.org/docs/spec/}{Matrix} one has to wade through
dozens of shell commands, system-specific instructions, potential
conflicts between dependent packages, set up an SQL server\ldots{} and
that's just the backend, we don't even have a frontend client yet! In
contrast, to use Slack you download the app, give it your email, and
you're off and running.

The control exerted by centralized systems over their system design does
give certain structural advantages to their usability, and their
for-profit model gives certain advantages to their development process.
There is no reason, however, that decentralized systems \emph{must} be
intrinsically harder to use, we just need to focus on user experience to
a comparable degree that centralized platforms: if it takes a college
degree to turn the water on, that ain't infrastructure.

People are smart, they just get frustrated easily and have other things
to do on a deadline. We have to raise our standards of design such that
we don't expect users to have even a passing familiarity with
programming, attempting to build tools that are truly general use. We
can't just design a peer-to-peer system, we need to make the data
ingestion and annotation process automatic and effortless. We can't just
build a system for credit assignment, it needs to happen as an automatic
byproduct of using the system. We can't just make tools that
\emph{work,} they need to \emph{feel good to use.}

Centralized systems also have intrinsic limitations that provide
openings for decentralized systems, like cost, incompatibility with
other systems, inability for extension, and opacity of function. The
potential for decentralized systems to capture the independent
development labor of all of its users, rather than just that of a core
development team, is one means of competition. If a system is
sufficiently easy to adopt, at least comparable to prior tooling, and
gives people a satisfying means of having their work accepted and
valued, the social and technical joy might be enough to outweigh the
inertia of change and the convenience of centralized systems.

With these principles in mind, and drawing from other knowledge
communities solving similar problems: internet infrastructure,
library/information science, peer-to-peer networks, and radical
community organizers, I conceptualize a system of distributed
infrastructure for systems neuroscience as three objectives:
\protect\hyperlink{shared-data}{\textbf{shared data}},
\protect\hyperlink{shared-tools}{\textbf{shared tools}}, and
\protect\hyperlink{shared-knowledge}{\textbf{shared knowledge}}.





\section{Shared Data}


\subsection{Formats as Onramps}



 The shallowest onramp towards a generalized data
infrastructure is to make use of existing discipline-specific
standardized data formats. As will be discussed later, a truly universal
pandisciplinary format is effectively impossible, but to arrive at the
alternative we should first congeal the wild west of unstandardized data
into a smaller number of established formats.

Data formats consist of some combination of an abstract specification,
an implementation in a particular storage medium, and an API for
interacting with the format. I won't dwell on the particular qualities
that a particular format needs, assuming that most that would be adopted
would abide by FAIR principles. For now we assume that the particular
constellation of these properties that make up a particular format will
remain mostly intact with an eye towards semantically linking
specifications and unifying their implementation.

There are a dizzying number of scientific data formats \citep{teamScientificDataFormats} , so a comprehensive treatment is
impractical here and I will use the Neurodata Without Borders:N
(NWB)\citep{rubelNWBAccessibleData2019a}  as an example. NWB is
the de facto standard for systems neuroscience, adopted by many
institutes and labs, though far from universally. NWB
\href{https://www.nwb.org/nwb-software/}{consists of} a
\href{https://schema-language.readthedocs.io/en/stable/}{specification
language}, a \href{https://nwb-schema.readthedocs.io/en/stable/}{schema
written in that language}, a
\href{https://nwb-storage.readthedocs.io/en/stable/}{storage
implementation in hdf5}, and an
\href{https://pynwb.readthedocs.io/en/stable/}{API for interacting with
the data}. They have done an admirable job of engaging with community
needs \citep{rubelNeurodataBordersEcosystem2021}  and making a
modular, extensible format ecosystem.

The major point of improvement for NWB, and I imagine many data
standards, is the ease of conversion and use. The conversion API
requires extensive programming, knowledge of the format, and navigation
of several separate tutorial documents. This means that individual labs,
if they are lucky enough to have some partially standardized format for
the lab, typically need to write (or hire someone to write) their
\href{https://github.com/catalystneuro/tank-lab-to-nwb}{own} software
\href{https://github.com/catalystneuro/mease-lab-to-nwb}{library} for
conversion.

Without being prescriptive about its form, substantial interface
development is needed to make mass conversion possible. It's usually
untrue that unstandardized data had \emph{no structure,} and researchers
are typically able to articulate it -- ``the filenames have the data
followed by the subject id,'' and so on. Lowering the barriers to
conversion mean designing tools that match the descriptive style of folk
formats, for example by prompting them to describe where each of an
available set of metadata fields are located in their data. It is not an
impossible goal to imagine a piece of software that can be downloaded
and with minimal recourse to reference documentation allow someone to
convert their lab's data within an afternoon. The barriers to conversion
have to be low and the benefits of conversion have to outweigh the ease
of use from ad-hoc and historical formats.

NWB also has an extension interface, which allows, for example, common
data sources to be more easily described in the format. These are
registered in an \href{https://nwb-extensions.github.io/}{extensions
catalogue}, but at the time of writing it is relatively sparse. The
preponderance of lab-specific conversion packages relative to extensions
is indicative of an interface and community tools problem: presumably
many people are facing similar conversion problems, but because there is
not a place to share these techniques in a human-readable way, the
effort is duplicated in dispersed codebases. We will return to some
possible solutions for knowledge preservation and format extension when
we discuss tools for \protect\hyperlink{shared-knowledge}{shared
knowledge}.

For the sake of the rest of the argument, let us assume that some
relatively trivial conversion process exists to subdomain-specific data
formats and we reach some reasonable penetrance of standardization. The
interactions with the other pieces of infrastructure that may induce and
incentivize conversion will come later.


\subsection{Peer-to-peer as a
Backbone}

We should adopt a \emph{peer-to-peer} system for storing and sharing
scientific data. There are, of course
\href{https://www.dandiarchive.org/}{many}
\href{https://openneuro.org/}{existing}
\href{https://www.brainminds.riken.jp/}{databases}
\href{https://biccn.org/}{for} scientific data, ranging from
domain-general like \href{https://figshare.com/}{figshare} and
\href{https://zenodo.org/}{zenodo} to the most laser-focused
subdiscipline-specific. The notion of a database, like a data standard,
is not monolithic. As a simplification, they consist of at least the
hardware used for storage, the software implementation of read, write,
and query operations, a formatting schema, some API for interacting with
it, the rules and regulations that govern its use, and especially in
scientific databases some frontend for visual interaction. For now we
will focus on the storage software and read-write system, returning to
the format, regulations, and interface later.

Centralized servers are fundamentally constrained by their storage
capacity and bandwidth, both of which cost money. In order to be free,
database maintainers need to constantly raise money from donations or
grants\footnote{granting agencies seem to love funding new databases,
  idk.} in order to pay for both. Funding can never be infinite, and so
inevitably there must be some limit on the amount of data that someone
can upload and the speed at which it can serve files\footnote{As I am
  writing this, I am getting a (very unscientific) maximum speed of
  5MB/s on the \href{https://osf.io}{Open Science Framework}}. In the
case that a researcher never sees any of those costs, they are still
being borne by some funding agency, incurring the social costs of
funneling money to database maintainers. Centralized servers are also
intrinsically out of the control of their users, requiring them to abide
whatever terms of use the server administrators set. Even if the
database is carefully backed up, it serves as a single point of
infrastructural failure, where if the project lapses then at worst data
will be irreversibly lost, and at best a lot of labor needs to be
expended to exfiltrate, reformat, and rehost the data. The same is true
of isolated, local, institutional-level servers and related database
platforms, with the additional problem of skewed funding allocation
making them unaffordable for many researchers.

Peer-to-peer (p2p) systems solve many of these problems, and I argue are
the only type of technology capable of making a database system that can
handle the scale of all scientific data. There is an enormous degree of
variation between p2p systems\footnote{peer to peer systems are, maybe
  predictably, a whole academic subdiscipline. See \citep{shenHandbookPeertoPeerNetworking2010}  for reference.}, but they
share a set of architectural advantages. The essential quality of any
p2p system is that rather than each participant in a network interacting
only with a single server that hosts all the data, everyone hosts data
and interacts directly with each other.

For the sake of concreteness, we can consider a (simplified) description
of Bittorrent \citep{cohenBitTorrentProtocolSpecification2017} ,
arguably the most successful p2p protocol. To share a collection of
files, a user creates a \texttt{.torrent} file with their Bittorrent
client which consists of a
\href{https://en.wikipedia.org/wiki/Cryptographic_hash_function}{cryptographic
hash}, or a string that is unique to the collection of files being
shared; and a list of ``trackers.'' A tracker, appropriately, keeps
track of the \texttt{.torrent} files that have been uploaded to it, and
connects users that have or want the content referred to by the
\texttt{.torrent} file. The uploader (or seeder) then leaves a
\href{https://en.wikipedia.org/wiki/Glossary_of_BitTorrent_terms\#Client}{torrent
client} open waiting for incoming connections. Someone who wants to
download the files (a leecher) will then open the \texttt{.torrent} file
in their client, which will then ask the tracker for the IP addresses of
the other peers who are seeding the file, directly connect to them, and
begin downloading. So far so similar to standard client-server systems,
but say another person wants to download the same files before the first
person has finished downloading it: rather than \emph{only} downloading
from the original seeder, the new leecher downloads from \emph{both} the
original seeder and the first leecher by requesting pieces of the file
from each until they have the whole thing. Leechers are incentivized to
share among each other to prevent the seeders from spending time
reuploading the pieces that they already have, and once they have
finished downloading they become seeders themselves.

From this very simple example, a number of qualities of p2p systems
become clear.

\begin{itemize}

\item
  First, the system is extremely \textbf{inexpensive to maintain} since
  it takes advantage of the existing bandwidth and storage space of the
  computers in the swarm, rather than dedicated servers. Near the height
  of its popularity in 2009, The Pirate Bay, a notorious bittorrent
  tracker, was estimated to cost \$3,000 per month to maintain while
  serving approximately 20 million peers \citep{roettgersPirateBayDistributing2009} . According to a database dump
  from 2013 \citep{PirateBayArchiveteam2020} , multiplying the
  size of each torrent by the number of seeders (ignoring any partial
  downloads from leechers), the approximate instantaneous storage size
  of The Pirate Bay was \textasciitilde26 Petabytes. The comparison to
  centralized services is not straightforward, since it is hard to
  evaluate the distributed costs of additional storage media (as well as
  the costs avoided by being able to take advantage of existing storage
  infrastructure within labs and institutes), but for the sake of
  illustration: hosting 26PB would cost \$546,000/month with standard
  AWS S3 hosting (\$0.021/GB/month).
\item
  The \textbf{speed} of a bittorrent swarm \emph{increases,} rather than
  decreases, the more people are using it since it is capable of using
  all of the available bandwidth in the system.
\item
  The network is extremely \textbf{resilient} since the data is shared
  across many independent peers in the system. If our goal is to make a
  resilient and robust data architecture, we would benefit by paying
  attention to the tools used in the broader archival community,
  especially the archival communities that especially need resilience
  because their archives are frequent targets of governments and
  intellectual property holders\citep{spiesDataIntegrityLibrarians2017} . Despite more than 15 years of
  concerted effort by governments and intellectual property holders, the
  pirate bay is still alive and kicking\footnote{knock on wood} \citep{kim15YearsPirate2019} . This is because even if the entire
  infrastructure of the tracker is destroyed, as it was in 2006, the
  files are distributed across all of its users, the actual database of
  \texttt{.torrent} metadata is quite small, and the tracker software is
  extraordinarily simple to rehost \citep{vandersarOpenBayNow2014}  -- The Pirate Bay was back online in 2 days. When another
  tracker, what.cd (which we will return to
  \protect\hyperlink{archives-need-communities}{soon}) was shut down, a
  series of successors popped up using the open source tools
  \href{https://github.com/WhatCD/Gazelle}{Gazelle} and
  \href{https://github.com/WhatCD/Ocelot}{Ocelot} that what.cd
  developers built. Within two weeks, one successor site had recovered
  and reindexed 200,000 of its torrents resubmitted by former users \citep{vandersarWhatCdDead2016} . Bittorrent is also used by archival
  groups with little funding like
  \href{https://wiki.archiveteam.org/index.php/Main_Page}{Archive Team},
  who struggled -- but eventually succeeded -- to disseminate their
  \href{https://wiki.archiveteam.org/index.php/GeoCities_Project}{historic
  preservation} over a single ``crappy cable modem'' \citep{scottGeocitiesTorrentUpdate2010} .
\item
  The network is extremely \textbf{scalable} since there is no cost to
  connecting new peers and the users of a system expand the storage
  capacity of the system depending on their needs. Rather than having
  one extremely fast data center (or a privatized network designed to
  own the internet), the model of p2p systems is to leverage many
  approachable peer/servers.
\end{itemize}

Peer-to-peer systems are not mutually exclusive with centralized
servers: servers are peers too, after all. A properly implemented p2p
system will always be \emph{at least} as fast and have \emph{at least}
as much storage as any alternative centralized centralized server
because peers can use \emph{both} the bandwidth of the server \emph{and}
that of any peers that have the file. In the bittorrent ecosystem
large-bandwidth/storage peers are known as ``seedboxes''\citep{rossiPeekingBitTorrentSeedbox2014}  when they use the bittorrent
protocol, and ``web seeds''\citep{hoffmanHTTPBasedSeedingSpecification}  when they use a protocol built
on top of traditional HTTP. \href{https://archive.org}{Archive.org} has
been distributing all of its materials
\href{https://archive.org/details/bittorrent}{with bittorrent} by using
its servers as web seeds since 2012 and makes this point explicitly:
``BitTorrent is now the fastest way to download items from the Archive,
because the Bittorrent client downloads simultaneously from two
different Archive servers located in two different datacenters, and from
other Archive users who have downloaded these Torrents already.'' \citep{kahle000000Torrents2012} 

p2p systems complement centralized servers in a number of ways beyond
raw download speed, increasing the efficiency and performance of the
network as a whole. Spotify began as a joint client/server and p2p
system \citep{kreitzSpotifyLargeScale2010b} , where when a
listener presses play the central server provides the data until peers
that have the song cached are found by the p2p system to download the
rest of the song from. The central server is able to respond quickly and
reliably to so the song is played as quickly as possible, and is the
server of last resort in the case of rare files that aren't being shared
by anyone else in the network. A p2p system complements the server and
makes that possible by alleviating pressure on the server for more
predictable traffic.

A peer to peer system is a particularly natural fit for many of the
common circumstances and practices in science, where centralized server
architectures seem (and prove) awkward and inefficient. Most labs,
institutes, or other organized bodies of science have some form of local
or institutional storage systems. In the most frequent cases of sharing
data within a lab or institute, sending it back and forth to some
nationally-centralized server is like walking across the lab by going
the long way around the Earth. That's the method invoked by a Dropbox or
AWS link, but in the absence of a formal one you can always revert to a
low-fi p2p transfer: walking a flash drive across the lab. The system
makes less sense when several people in the same place need to access
the same data at the same time, as is frequently the case with multi-lab
collaborations, or scientific conferences and workshops. Instead of
needing to wait on the 300kb/s conference wifi bandwidth as it's
cheese-gratered across every machine, we instead could directly beam it
between all computers in range simultaneously, full blast through the
decrepit network switch that won't have seen that much excitement in
years.

If we take the suggestion of Andrey Andreev et al.~and invest in server
clusters within institutes \citep{andreevBiologistsNeedModern2021, charlesCommunityDrivenBigOpen2020} , their impact could be multiplied
manyfold by combining them fluidly and simultaneously in a p2p swarm for
file transfer and storage. While the NIH might be shy to start up
another server farm for all scientific data and prefer to contract with
AWS, we don't have to be. P2p systems might also alleviate any nervous
university administrators concerned about bandwidth costs: instead of
needing to serve entire datasets to each person who wants them, the load
can be spread out across many institutes naturally based on the use of
the file.

So far I have relied on the Extraordinarily Simplified
Bittorrent\footnote{â¢ï¸} depiction of a peer to peer system, but there
are many improvements and variants that can address different needs for
scientific data infrastructure.

One obvious need that bittorrent can't currently support is version
control, but more recent p2p systems do. \href{https://ipfs.io/}{IPFS}
functions like ``a single BitTorrent swarm, exchanging objects within
one Git repository.'' \citep{benetIPFSContentAddressed2014} \footnote{Git, briefly, is a version control system that keeps a
  history of changes of files (blobs) as a Merkle DAG: files can be
  updated, and different versions can be branched and reconciled.} Dat
\citep{ogdenDatDistributedDataset2017} , specifically designed for
data synchronization and versioning, handles versioning and more. A full
description of IPFS is out of scope, and it has plenty of problems \citep{patsakisHydrasIPFSDecentralised2019} , but for now sufficent to
say p2p systems can handle version control.

Bittorrent swarms are vulnerable to data loss if all the peers seeding a
file disconnect (though the tail is longer than typically assumed, see
\citep{zhangUnravelingBitTorrentEcosystem2011} ), but this too can
be addressed with updated p2p system design. A first-order solution to
this problem is a variant of IPFS' notion of `pinning.' Since backup to
lab-level or institutional servers is already commonplace, one peer
could be able to `pin' another and automatically download all the data
that they share. This concept could scale to institutes and national
infrastructure as scientists can request the datasets they'd like to be
saved permanently be pinned.

Another could be something akin to Freenet \citep{clarkeFreenetDistributedAnonymous2001} . Peers could allocate a
certain amount of their unused storage space to be used to automatically
download, cache, and rehost shards of other datasets. Distributing
chunks and encrypting them at rest so the rehoster can't inspect their
contents would make it possible to maintain privacy and network
availability for sensitive data (see, for example,
\href{https://inqlab.net/projects/eris/}{ERIS}). IPFS has an analogous
concept -- BitSwap -- that is makes it into a barter system. Peers who
seek to download will have to `earn' it by finding some chunk of data
that the other peers want, download, and share them, though it seems
like an empirical question whether or not a barter system works or is
necessary.

\href{https://solidproject.org/}{Solid} is a project that almost exactly
meets all these needs \citep{capadisliSolidProtocol2020, sambraSolidPlatformDecentralized2016, SolidP2PFoundation} . Solid
allows people to share data in
\href{https://solidproject.org/about}{Pods}, which let them control
access and distribution across storage system with a unified identity
system. It is implementation-agnostic, and so can support peer-to-peer
storage and transfer systems that comply with its
\href{https://solidproject.org/TR/protocol}{protocol specification}.

There are a number of additional requirements for a peer to peer
scientific data infrastructure, but even these seemingly very technical
problems of versioning and distributed storage show the clear need to
consider the structure of the surrounding social system. What control do
we give to researchers over the version history of their data? Should
people that aren't the originating researcher be able to issue new
versions? What structure of distributed/centralized storage works? How
should we incentivize sharing of excess storage and resources?

Even before considering additional social systems, a peer to peer
structure in itself implies a different relationship to a generalized
data infrastructure. Scientists always unavoidably make their data
available to at least one person: themselves; on at least one computer:
theirs, and that computer is usually connected to the internet. A
peer-to-peer backbone for scientific infrastructure is the unnecessarily
radical notion that everyday practices like these can make up our
infrastructure, rather than having it exist exogenously as something
``out there.'' Subtly, it's the notion that our infrastructure can
reflect and consist of \emph{ourselves} instead of something out of our
control that we need to buy from someone else.

Scientists don't need to reinvent the notion of distributed, community
curated data archives from scratch. In addition to scholarly work on the
social systems of digital infrastructure, we can learn from communities
of practice, and there has been no more important and impactful
decentralized archival project than internet piracy.


\subsection{Archives Need
Communities}

Why do hundreds of thousands of people, completely anonymously, with
zero compensation, spend their time to do something that is as legally
risky as curating pirated cultural archives?

Scholarly work, particularly from Economics, tends to focus on
understanding piracy in order to prevent it\citep{basamanowiczReleaseGroupsDigital2011, hindujaDeindividuationInternetSoftware2008} , taking the moral good
of intellectual property markets as an \emph{a priori} imperative and
investigating why people behave \emph{badly} and ``rend {[}the{]} moral
fabric associated with the respect of intellectual property.'' \citep{hindujaDeindividuationInternetSoftware2008} . If we put the legality
of piracy aside, we may find a wealth of wisdom and insight to draw from
for building scientific infrastructure.

The world of digital piracy is massive, from entirely disorganized
efforts of individual people on public sites to extraordinarily
organized release groups \citep{basamanowiczReleaseGroupsDigital2011} , and so a full consideration is out of scope, but many of the
important lessons are taught by the structure of bittorrent trackers.

An underappreciated element of the BitTorrent protocol is the effect of
the separation between the data transfer protocol and the `discovery'
part of the system --- or ``overlay'' --- on the community structure of
torrent trackers (for a more complete picture of the ecosystem, see \citep{zhangUnravelingBitTorrentEcosystem2011} ). Many peer to peer
networks like \href{https://en.wikipedia.org/wiki/Kazaa}{KaZaA} or the
\href{https://en.wikipedia.org/wiki/Gnutella}{gnutella}-based
\href{https://en.wikipedia.org/wiki/LimeWire}{Limewire} had searching
for files integrated into the transfer interface. The need for torrent
trackers to share .torrent files spawned a massive community of private
torrent trackers that for decades have been iterating on cultures of
archival, experimenting with different community structures and
incentives that encourage people to share and annotate some of the
world's largest, most organized libraries.

One of these private trackers was the site of one of the largest
informational tragedies of the past decade: what.cd\footnote{for a
  detailed description of the site and community, see Ian Dunham's
  dissertation \citep{dunhamWhatCDLegacy2018} }, which I will use
as an example to describe some of these community systems.

What.cd was a bittorrent tracker that was arguably the largest
collection of music that has ever existed. At the time of its
destruction in 2016, it was host to just over one million unique
releases, and approximately 3.5 million torrents\footnote{Though spotify
  now boasts its library having 50 million tracks, back of the envelope
  calculations relating number of releases to number of tracks are
  fraught, given the long tail of track numbers on albums like classical
  music anthologies with several hundred tracks on a single ``release.''}
\citep{dunhamWhatCDLegacy2018} . Every torrent was organized in a
meticulous system of metadata communally curated by its roughly 200,000
global users. The collection was built by people who cared deeply about
music, rather than commercial collections provided by record labels
notorious for ceasing distribution of recordings that are not
commercially viable --- or just losing them in a fire \citep{rosenDayMusicBurned2019} {[}\^{}lostartists{]}. Users would spend
large amounts of money to find and digitize extremely rare recordings,
many of which were unavailable anywhere else and are now unavailable
anywhere, period. One former user describes one example:

\begin{quote}
``I did sound design for a show about CeauÈescu's Romania, and was able
to pull together all of this 70s dissident prog-rock and stuff that has
never been released on CD, let alone outside of Romania'' \citep{sonnadEulogyWhatCd2016} 
\end{quote}

\includegraphics[width=\linewidth]{../assets/images/kanye-what.png} \emph{The
what.cd artist page for Kanye West (taken from
\href{https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/}{here}
in the style of pirates, without permission). For the album ``Yeezus,''
there are ten torrents, grouped by each time the album was released on
CD and Web, and in multiple different qualities and formats (.flac,
.mp3). Along the top is a list of the macro-level groups, where what is
in view is the ``albums'' section, there are also sections for bootleg
recordings, remixes, live albums, etc.}

What.cd was a ``private'' bittorrent tracker, where unlike public
trackers that anyone can access, membership was strictly limited to
those who were personally invited or to those who passed an interview
(for more on public and private tracker, see \citep{meulpolderPublicPrivateBitTorrent} ). Invites were extremely rare,
and the interview process was demanding to the point where
\href{https://opentrackers.org/whatinterviewprep.com/index.html}{extensive
guides} were written to prepare for them.

The what.cd incentive system was based on a required ratio of data
uploaded vs.~data downloaded \citep{jiaHowSurviveThrive2013} .
Peer to peer systems need to overcome a free-rider problem where users
might download a torrent (``leeching'') and turn their computer off,
rather than leaving their connection open to share it to others (or,
``seeding''). In order to download additional music, then, one would
have to upload more. Since downloading is highly restricted, and
everyone is trying to upload as much as they can, torrents had a large
number of ``seeders,'' and even rare recordings would be sustained for
years, a pattern common to private trackers \citep{liuUnderstandingImprovingRatio2010} .

The high seeder/leecher ratio made it so it was extremely difficult to
acquire upload credit, so users were additionally incentivized to find
and upload new recordings to the system. What.cd implemented a
``bounty'' system, where users with a large amount of excess upload
credit would be able to offer some of it to whoever was able to upload
the album they wanted. To ``prime the pump'' and keep the economy
moving, highlight artists in an album of the week, or direct users to
preserve rare recordings, moderators would also use a ``freeleech''
system, where users would be able to download a specified set of
torrents without it counting against their download quantity \citep{kashEconomicsBitTorrentCommunities2012, chenImprovingSustainabilityPrivate2011a} .

The other half of what.cd was the more explicitly social elements: its
forums, comment sections, and moderation systems. The forum was home to
roiling debates that lasted years about the structure of some tagging
schema, whether one genre was just another with a different name, and so
on. The structure of the community was an object of constant, public
negotiation, and over time the metadata system evolved to be able to
support a library of the entirety of human music output\footnote{Though
  music metadata might seem like a trivial problem (just look at the
  fields in an MP3 header), the number of edge cases are profound. How
  would you categorize an early Madlib casette mixtape remastered and
  uploaded to his website where he is mumbling to himself while
  recording some live show performed by multiple artists, but on the
  b-side is one of his Beat Konducta collections that mix together
  studio recordings from a collection of other artists? Who is the
  artist? How would you even identify the unnamed artists in the live
  show? Is that a compilation or a bootleg? Is it a cassette rip, a
  remaster, or a web release?}, and the rules and incentive structures
were made to align with building it. To support the good operation of
the site, the forums were also home to a huge amount of technical
knowledge, like guides on how to make a perfect upload, that eased new
users into being able to use the system.

A critical problem in maintaining coherent databases is correcting
metadata errors and departures from schemas. Finding errors was
rewarded. Users were able to discuss and ask questions of the uploader
in a comment section below each upload, which would allow ``polite''
resolution of low-level errors like typos. More serious problems could
be reported to the moderation team, which caused the upload to be
visibly marked as under review, and the report could then be discussed
either in the comment sections or the forum. Being an anonymous,
gray-area community, there was of course plenty of power that was
tripped on. Rather than being a messy hodgepodge of fake, low-quality
uploads, though, what.cd was always teetering just shy of perfection.

These structural considerations do not capture the most elusive but
indisputably important features of what.cd's community infrastructure:
\emph{the sense of commmunity}. The What.cd forums were the center of
many user's relationships to music. Threads about all the finest scales
of music nichery could last for years: it was a rare place people who
probably cared a little bit too much about music could talk to people
with the same condition. What made it more satisfying than other music
forums was that no matter what music you were talking about, everyone
else in the conversation would always have access to it if they wanted
to hear it. Beyond any structural incentives, people spent so much time
building and maintaining what.cd because it became a source of community
and a sink of personal investment.

Structural norms supported by social systems converge as a sort of
\emph{reputational} incentive. Uploading a new album to fill a bounty
both makes the network more functional and complete, but also
\emph{people respect you for it} because it's prominently displayed on
your profile as well as in the bounty charts and that \emph{feels good}.
Becoming known on the forums for answering questions, writing guides, or
even just having a good taste in music \emph{feels good} and also
contributes to the overall health of the system. Though there are plenty
of databases, and even plenty of different communication venues for
scientists, there aren't any databases (to my knowledge) with integrated
community systems.

The tracker overlay model mirrors and extends some of the
recommendations made by Benedikt Fecher and colleagues in their work on
the reputational economy surrounding data sharing \citep{fecherReputationEconomyHow2017} . They give three policy
recommendations: Increasing reputational benefits, reducing transaction
costs, and ``increasing market transparency by making open access to
research data more visible to members of the research community.'' One
way to accomplish implement them is to embed a data sharing system
within a social system that is designed to reward communitarian
behavior.

Many features of what.cd's structure are undesirable for scientific
infrastructure, but they demonstrate that a robust archive is not only a
matter of building a database with some frontend, but by building a
community \citep{brossCommunityCollaborationContribution2013} . Of
course, we need to be careful with building the structural incentives
for a data sharing system: the very last thing we want is another
\href{https://etiennelebel.com/cs/t-leaderboard/t-leaderboard.html}{coercive
leaderboard} that turns what should be a collaborative effort punitive.
In contrast to what.cd, for infrastructure we want extremely low
barriers to entry, and be agnostic to resources --- researchers with
access to huge server farms should not be unduly favored. We should
think carefully about using downloading as the ``cost,'' because
downloading and analyzing huge amounts of data can be \emph{good} and
exactly what we \emph{want} in some circumstances, but a threat to
privacy and data governance in others.

This model has its own problems, including the lack of interoperability
between different trackers, the need to recreate a new set of accounts
and database for each new tracker, among others. It's also been tried
before: sharing data in specific formats (as our running example,
Neurodata Without Borders) on indexing systems like bittorrent trackers
amounts to something like BioTorrents \citep{langilleBioTorrentsFileSharing2010}  or
\href{https://academictorrents.com/}{AcademicTorrents} \citep{cohenAcademicTorrentsCommunityMaintained2014} . Even with our
extensions of version control and some model of automatic mirroring of
data across the network, we still have some work to do. To address these
and several other remaining needs for scientific data infrastructure, we
can take inspiration from \emph{federated systems.}






\subsection{Linked Data or Surveillance
Capitalism?}



 There is no shortage of databases for scientific data, but
their traditional structure chokes on the complexity of representing
multi-domain data. Typical relational databases require some formal
schema to structure the data they contain, which have varying
reflections in the APIs used to access them and interfaces built atop
them. This broadly polarizes database design into domain-specific and
domain-general\footnote{To continue the analogy to bittorrent trackers,
  an example domain-specific vs.~domain-general dichotomy might be
  What.cd (with its specific formatting and aggregation tools for
  representing artists, albums, collections, genres, and so on)
  vs.~ThePirateBay (with its general categories of content and otherwise
  search-based aggregation interface)}. This design pattern results in a
fragmented landscape of databases with limited interoperability. In a
moment we'll consider \emph{federated systems} as a way to resolve this
dichotomy and continue developing the design of our p2p data
infrastructure, but for now we need a better sense of the problem.

Domain-specific databases require data to be in one or a few specific
formats, and usually provide richer tools for manipulating and querying
by metadata, visualization, summarization, aggregation that are
purpose-built for that type of data. For example, NIH's
\href{https://www.ncbi.nlm.nih.gov/gene/12550}{Gene} tool has several
visualization tools and cross-referencing tools for finding expression
pathways, genetic interactions, and related sequences (Figure xx). This
pattern of database design is reflected at several different scales,
through institutional databases and tools like the Allen
\href{https://connectivity.brain-map.org/}{brain atlases} or
\href{http://observatory.brain-map.org/visualcoding/}{observatory}, to
lab- and project-specific dashboards. This type of database is natural,
expressive, and powerful --- for the researchers they are designed for.
While some of these databases allow open data submission, they often
require explicit moderation and approval to maintain the guaranteed
consistency of the database, which can hamper mass use.

\includegraphics[width=\linewidth]{../assets/images/nih_gene_cdh1.png}
\emph{NIH's Gene tool included many specific tools for visualizing,
cross-referencing, and aggregating genetic data. Shown is the ``genomic
regions, transcripts, and product'' plot for Mouse Cdh1, which gives
useful, common summary descriptions of the gene, but is not useful for,
say, visualizing reading proficiency data.}

General-purpose databases like \href{https://figshare.com/}{figshare}
and \href{https://zenodo.org/}{zenodo}\footnote{No shade to Figshare,
  which, among others, paved the way for open data and are a massively
  useful thing to have in society.} are useful for the mass aggregation
of data, typically allowing uploads from most people with minimal
barriers. Their general function limits the metadata, visualization, and
other tools that are offered by domain-specific databases, however, and
are essentially public, versioned, folders with a DOI. Most have fields
for authorship, research groups, related publications, and a
single-dimension keyword or tags system, and so don't programmatically
reflect the metadata present in a given dataset.

The dichotomy of fragmented, subdomain-specific databases and
general-purpose databases makes combining information from across even
extremely similar subdisciplines combinatorically complex and laborious.
In the absence of a formal interoperability and indexing protocol
between databases, even \emph{finding} the correct subdomain-specific
database can be an act of raw experience or the raw luck of stumbling
across just the right blog post list of databases. It also puts
researchers who want to be good data stewards in a difficult position:
they can hunt down the appropriate subdomain specific database and risk
general obscurity; use a domain-general database and make their work
more difficult for themselves and their peers to use; or spend all the
time it takes to upload to multiple databases with potentially
conflicting demands on format.

What can be done? There are a few parsimonious answers from
standardizing different parts of the process: If we had a universal data
format, then interoperability becomes trivial. Conversely, we could make
a single ur-database that supports all possible formats and tools.

Universalizing a single part of a database system is unlikely to work
because organizing knowledge is intrinsically political. Every system of
representation is necessarily rooted in its context: one person's
metadata is another person's data. Every subdiscipline has conflicting
\emph{representational} needs, will develop different local terminology,
allocate differing granularity and develop different groupings and
hierarchies for the same phenomena. At mildest, differences in
representational systems can be incompatible, but at their worst they
can reflect and reinforce prejudices and become tools of intellectual
and social power struggles. Every subdiscipline has conflicting
\emph{practical} needs, with infinite variation in privacy demands,
different priorities between storage space, bandwidth, and computational
power, and so on. In all cases the boundaries of our myopia are
impossible to gauge: we might think we have arrived at a suitable schema
for biology, chemistry, and physics\ldots{} but what about the
historians?

Matthew J Bietz and Charlotte P Lee articulate this tension better than
I can in their ethnography of metagenomics databases:

\begin{quote}
``Participants describe the individual sequence database systems as if
they were shadows, poor representations of a widely-agreed-upon ideal.
We find, however, that by looking across the landscape of databases, a
different picture emerges. Instead, \textbf{each decision about the
implementation of a particular database system plants a stake for a
community boundary. The databases are not so much imperfect copies of an
ideal as they are arguments about what the ideal Database should be.}
{[}\ldots{]}

When the microbial ecology project adopted the database system from the
traditional genomic ``gene finders,'' they expected the database to be a
boundary object. They knew they would have to customize it to some
extent, but thought it would be able to ``travel across borders and
maintain some sort of constant identity''. In the end, however,
\textbf{the system was so tailored to a specific set of research
questions that the collection of data, the set of tools, and even the
social organization of the project had to be significantly changed.} New
analysis tools were developed and old tools were discarded. Not only was
the database ported to a different technology, the data itself was
significantly restructured to fit the new tools and approaches. While
the database development projects had begun by working together, in the
end they were unable to collaborate. \textbf{The system that was
supposed to tie these groups together could not be shielded from the
controversies that formed the boundaries between the communities of
practice.}'' \citep{bietzCollaborationMetagenomicsSequence2009} 
\end{quote}

As one ascends the scales of formalizing to the heights of the ontology
designers, the ideological nature of the project is like a klaxon
(emphasis in original):

\begin{quote}
An exception is the Open Biomedical Ontologies (OBO) Foundry initiative,
which accepts under its label only those ontologies that adhere to the
principles of ontological realism. {[}\ldots{]} Ontologies, from this
perspective, are representational artifacts, comprising a taxonomy as
their central backbone, whose representational units are intended to
designate \emph{universals} (such as \emph{human being} and
\emph{patient role}) or \emph{classes defined in terms of universals}
(such as \emph{patient,} a class encompassing \emph{human beings} in
which there inheres a \emph{patient role}) and certain relations between
them. {[}\ldots{]}

BFO is a realist ontology {[}15,16{]}. This means, most importantly,
that representations faithful to BFO can acknowledge only those entities
which exist in (for example, biological) reality; thus they must reject
all those types of putative negative entities - lacks, absences,
non-existents, possibilia, and the like \citep{ceustersFoundationsRealistOntology2010} 
\end{quote}

Aside from unilateral standardization, another formulation that doesn't
require existing server infrastructure to be dramatically changed is to
link existing databases. The problem of linking databases is an old one
with much well-trodden ground, and in the current regime of large server
farms tend to find themselves somewhere close to metadata-indexing
overlays. These overlays provide some additional tool that can translate
and combine data between databases with some mapping between the
terminology in the overlay and that of the individual databases. The NIH
articulates this as a ``Biomedical Data Translator'' in its Strategic
plan for Data Science\footnote{
  Through its Biomedical Data Translator program, the National Center
  for Advancing Translational Sciences (NCATS) is supporting research to
  develop ways to connect conventionally separated data types to one
  another to make them more useful for researchers and the public. The
  Translator aims to bring data types together in ways that will
  integrate multiple types of existing data sourcess, including
  objective signs and symptoms of disease, drug effects, and other types
  of biological data relevant to understanding the development of
  disease and how it progresses in patients. \citep{NIHStrategicPlan2018}} and NCATS elaborates it a bit more on the project
\href{https://ncats.nih.gov/translator/about}{``about''} page:

\begin{quote}
As a result of recent scientific advances, a tremendous amount of data
is available from biomedical research and clinical interactions with
patients, health records, clinical trials and adverse event reports that
could be useful for understanding health and disease and for developing
and identifying treatments for diseases. Ideally, these data would be
mined collectively to provide insights into the relationship between
molecular and cellular processes (the targets of rational drug design)
and the signs and symptoms of diseases. Currently, these very rich yet
different data sources are housed in various locations, often in forms
that are not compatible or interoperable with each other. -
https://ncats.nih.gov/translator/about
\end{quote}

The Translator is being developed by 28 institutions and nearly 200 team
members as of 2019. They credit their group structure and flexible Other
Transaction Award (OTA) funding mechanism for their successes \citep{consortiumBiomedicalDataTranslator2019} . OTA awards give the
granting agency broad flexibility in to whom and for what money can be
given, and consist of an initial competetive segment with possibility
for indefinite noncompetitive extensions at the discretion of the agency
\citep{fleisherOtherTransactionAward2019} .

The project appears to be in a relatively early phase, and so it's
difficult to figure out exactly what it is that has been built. The
\href{https://web.archive.org/web/20211210023403/https://ncats.nih.gov/translator/projects}{projects
page} is currently just a list of the leaders of different areas, but
some parts of the project are visible through a bit of searching. They
describe a registry of APIs for existing databases collected on their
platform \href{https://smart-api.info/portal/translator}{SmartAPI} that
are to be combined into a semantic knowledge graph \citep{consortiumUniversalBiomedicalData2019} . There are many kinds of
knowledge graphs, and we will return to them and other semantic web
technologies in \protect\hyperlink{shared-knowledge}{shared knowledge},
but the Translator's knowledge graph explicitly sits ``on top'' of the
existing databases as the only source of knowledge. Specifically, the
graph structure consists of the nodes and edges of the
\href{https://github.com/biolink/biolink-model}{biolink model} \citep{bruskiewichBiolinkBiolinkmodel2021} , and an edge is matched to a
corresponding API that provides data for both elements. For each edge in
the graph, then, a number of possible APIs can provide data without
necessarily making a guarantee of consistency or accuracy.

They articulate a set of beliefs about the impossibility of a unified
dataset or ontology\footnote[][-7cm]{
  "First, we assert that a single monolithic data set that directly
  connects the complete set of clinical characteristics to the complete
  set of biomolecular features, including ``-omics'' data, will never
  exist because the number of characteristics and features is constantly
  shifting and exponentially growing. [...] We also assert that there is no single language, software
  or natural, with which to express clinical and biomolecular
  observations---these observations are necessarily and appropriately
  linked to the measurement technologies that produce them, as well as
  the nuances of language. The lack of a universal language for
  expressing clinical and biomolecular observations presents a risk of
  isolation or marginalization of data that are relevant for answering a
  particular inquiry, but are never accessed because of a failure in
  translation.

  Based on these observations, our final assertion is that automating
  the ability to reason across integrated data sources and providing
  users who pose inquiries with a dossier of translated answers coupled
  with full provenance and confidence in the results is critical if we
  wish to accelerate clinical and translational insights, drive new
  discoveries, facilitate serendipity, improve clinical-trial design,
  and ultimately improve clinical care. This final assertion represents
  the driving motivation for the Translator system." \citep{consortiumUniversalBiomedicalData2019}}\citep{consortiumUniversalBiomedicalData2019} ,
although arguably create one in
\href{https://biolink.github.io/biolink-model/docs/}{biolink} that's
very similar to that described here, and this problem seems to have
driven the focus of the project away from linking data as such towards
developing a graph-powered query engine. The Translator is being
designed to use machine-learning powered ``autonomous relay agents''
that sift through the inhomogenous data from the APIs and are able to
return a human-readable response, also generated with machine-learning.
The final form of the translator is still unclear, but between
\href{https://smart-api.info/portal/translator}{SmartAPI}, a
seemingly-preliminary description of the reasoning engine \citep{goelExplanationContainerCaseBased2021} , and descriptions from
contractors \citep{ROBOKOPCoVar2021} , the machine learning
component of the system could make it quite dangerous.

The intended use of the Translator seems to not be to directly search
for and use the data itself, but to use the connected data to answer
directed questions \citep{goelExplanationContainerCaseBased2021} 
--- an example that is used repeatedly is drug discovery. For any given
query of ``drugs that could treat x disease,'' the system traces out the
connected nodes in the graph from the disease to find its phenotypes,
which are connected to genes, which might be connected to some drug, and
so on. The Translator builds on top of a large number of databases and
database aggregators, and so it then needs a way of comparing and
ranking possible answers to the question. In a simple case, a drug that
directly acted on several involved genes might be ranked higher than,
say, one that acted only indirectly on phenotypes with many off-target
effects.

As with any machine-learning based system, if the input data is biased
or otherwise (inevitably) problematic then the algorithm can only
reflect that. If it is the case that this algorithm remains proprietary
(due to, for example, it being developed by a for-profit defense
contractor that named it ROBOKOP \citep{ROBOKOPCoVar2021} )
harmful input data could have unpredictable long-range consequences on
the practice of medicine as well as the course of medical research.
Taking a very narrow sample of APIs that return data about diseases, I
queried \href{https://mydisease.info}{mydisease.info} to see if it still
had the outmoded definition of ``transsexualism'' as a disease \citep{ramTransphobiaEncodedExamination2021} . Perhaps unsurprisingly, it
did, and was more than happy to give me a list of genes and variants
that supposedly ``cause'' it -
\href{http://mydisease.info/v1/query?q=\%22DOID\%3A10919\%22}{see for
yourself}.

This is, presumably, the fragility and inconsistency the
machine-learning layer was intended to putty over: if one follows the
provenance of the entry for ``gender identity disorder'' (renamed in
DSM-V), one reaches first the disease ontology
\href{https://web.archive.org/web/20211007053446/https://www.ebi.ac.uk/ols/ontologies/doid/terms?iri=http\%3A\%2F\%2Fpurl.obolibrary.org\%2Fobo\%2FDOID_1234}{DOID:1234}
which seems to trace back into an entry in a graph aggregator
\href{http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234}{Ontobee}
(\href{https://web.archive.org/web/20210923110103/http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234}{Archive
Link}), which in turn lists this
\href{https://github.com/jannahastings/mental-functioning-ontology}{github
repository} \textbf{maintained by a single person} as its
source\footnote{I submitted a
  \href{https://github.com/jannahastings/mental-functioning-ontology/pull/8}{pull
  request} to remove it. A teardrop in the ocean.}.

If at its core the algorithm believes that being transgender is a
disease, could it misunderstand and try to ``cure'' it? Even if it
doesn't, won't it influence the surrounding network of entities with its
links to genes, prior treatment, and so on in unpredictable ways?
Combined with the online training that updates the search model based on
how it is used \citep{consortiumUniversalBiomedicalData2019} ,
socially problematic treatment and research practices could be built
into our data infrastructure \emph{without any way of knowing their
effect on unrelated treatment.} In the long-run, an effort towards
transparency could have precisely the opposite effect by being run
through a series of black boxes.

A larger problem is reflected in the scope and evolving direction of the
Translator when combined with the preceding discussion of putting all
data in the hands of cloud platform holders. There is mission creep from
the original NIH initiative language that essentially amounts to a way
to connect different data sources --- what could have been as simple as
a translation table between different data standards and formats. The
original
\href{https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator}{funding
statement from 2016} is similarly humble, and press releases
\href{https://web.archive.org/web/20210709171335/https://ncats.nih.gov/pubs/features/translator}{through
2017} also speak mostly in terms of querying the data -- though some
ambition begins to creep in.

That is remarkably different than what is articulated in 2019 \citep{consortiumUniversalBiomedicalData2019}  to be much more focused on
\emph{inference} and \emph{reasoning} from the graph structure of the
linked data for the purpose of \emph{automating drug discovery.} It
seems like the original goal of making a translator in the sense of
``translating data between formats'' has morphed into ``translating data
to language,'' with ambitions of providing a means of making algorithmic
predictions for drug discovery and clinical practice rather than linking
data \citep{hailuNIHfundedProjectAims2019}  Tools like these have
been thoroughly problematized elsewhere, eg. \citep{groteEthicsAlgorithmicDecisionmaking2020, obermeyerDissectingRacialBias2019, panchArtificialIntelligenceAlgorithmic2019, panchInconvenientTruthAI2019} .

As of September 2021, it appears there is still some work left to be
done to make the Translator functional, but the early example
illustrates some potential risks (emphases mine):

\begin{quote}
The strategy used by the Translator consortium in this case is to 1)
identify phenotypes that are associated with {[}Drug-Induced Liver
Injury{]} DILI, then 2) find genes which are correlated with these
presumably pathological phenotypes, and then 3) identify drugs which
target those genes' products. The rationale is that drugs which target
gene products associated with phenotypes of DILI may possibly serve as
candidates for treatment options.

\textbf{We constructed a series of three queries,} written in the
Translator API standard language and submitted to xARA to select
appropriate KPs to collect responses (Figure 4). \textbf{From each
response, an exemplary result is selected and used in the query for the
next step.}

The results of the first query produced several phenotypes, one of them
was ''Red blood cell count'' (EFO0004305). When using this phenotype in
the second step to query for genes, we identified one of the results as
the telomerase reverse transcriptase (TERT) gene. This was then used in
the third query (Figure 4) to identify targeting drugs, which included
the drug Zidovudine.

xARA use this result to call for an explanation. The xcase retrieved
uses a relationship extraction algorithm {[}6{]} fine-tuned using
BioBert {[}7{]}. The explanation solution seeks previously pre-processed
publications where both biomedical entities (or one of its synonyms) is
found in the same article within a distance shorter than 10 sentences.
The excerpt of entailing both terms is then used as input to the
relationship extraction method. When implementing this solution for the
gene TERT (NCBIGene:7015) and the chemical substance Zidovudine
(CHEBI:10110), the solution was able to identify corroborating evidence
of this drug-target interaction with the relationship types being one
of: ''DOWNREGULATOR,'' ''INHIBITOR,'' or ''INDIRECT DOWNREGULATOR'' with
respect to TERT. \citep{goelExplanationContainerCaseBased2021} 
\end{quote}

As a recap, since I'm not including the screenshots of the queries, the
researchers searched first for a phenotypic feature of DILI, then
selected ``one of them'' --- red blood cell count --- to search for
genes that affect the phenotype, and eventually find a drug that effects
that gene: all seemingly manually (an additional \$1.4 million has been
allocated to unify them \citep{haendelCommonDialectInfrastructure2021} ). Zidovudine, as a nucleoside reverse transcriptase inhibitor, does
inhibit telomerase reverse transcriptase \citep{hukezalieVitroExVivo2012} , but can also cause anemia and lower red
blood cell counts \citep{ZidovudinePatientNIH}  -- so through the
extended reasoning chain the system has made a sign flip and recommended
a drug that will likely make the identified phenotype (low red blood
cell count) worse? The manual input will then be used to train the
algorithm for future results, though how data from prior use and data
from graph structure will be combined in the ranking algorithm --- and
then communicated to the end user --- is still unclear.

Contrast this with the space-age and chromed-out description from CoVar:

\begin{quote}
ROBOKOP technology scours vast, diverse databases to find answers that
standard search technologies could never provide. It does much more than
simple web-scraping. It considers inter-relationships between entities,
such as colds cause coughs. Then it searches for new connections between
bits of knowledge it finds in a wide range of data sources and generates
answers in terms of these causal relationships, on-the-fly.

Instead of providing a simple list of responses, ROBOKOP ranks answers
based on various criteria, including the amount of supporting evidence
for a claim, how many published papers reference a given fact, and the
specificity of any particular relationship to the question.
\end{quote}

For-profit platform holders are not incentivized to do responsible
science, or even really make something that works, provided they can get
access to some of the government funding that pours out for projects
that are eventually canned -
\href{https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear\&sort_order=desc}{\$75.5
million} so far since 2016 for the Translator \citep{RePORTRePORTERBiomedical2021} . As exemplified by the trial and
discontinuation of the NIH Data Commons after
\href{https://reporter.nih.gov/search/H4LxgMGK9kGw6SeWCom85Q/projects?shared=true}{\$84.7
million}, centralized infrastructure projects can be an opportunity to
``dance until the music stops.'' Again, it is relatively difficult to
see from the outside what work is going on and how it all fits together,
but judging from RePORTER there seem to be a
\href{https://reporter.nih.gov/project-details/10332268}{profusion} of
\href{https://reporter.nih.gov/project-details/10333468}{projects} and
\href{https://reporter.nih.gov/project-details/10333460}{components} of
the \href{https://reporter.nih.gov/project-details/10330627}{system}
with unclear functional overlap, and the model seems to have developed
into allocating funding to develop each separate knowledge source.

The risk with this project is very real because of the context of its
development. After 5 years, it still seems like the the Translator is
relatively far from realizing the vision of biopolitical control through
algorithmic predictions, but combined with Amazon's aggressive expansion
into health technology \citep{AWSAnnouncesAWS2021}  and even
literally providing \href{https://amazon.care/}{health care} \citep{lermanAmazonBuiltIts2021} , and the uploading of all scientific and
medical data onto AWS with entirely unenforceable promises of data
privacy \citep{quinnYouCanTrust2021}  --- the notion of spending
public money to develop a system for aggregating patient data with
scientific and clinical data becomes dangerous. It doesn't require
takeover by Amazon to become dangerous --- once you introduce the need
for data to train an algorithm, you need to feed it data, and so the
translator gains the incentive to suck up as much personal and other
data as it can.

Even assuming the Translator works perfectly and has zero unanticipated
consequences, the development strategy still reflects the inequities
that pervade science rather than challenge them. Biopharmaceutical
research, followed by broader biomedical research, being immediately and
extremely profitable, attracts an enormous quantity of resources and
develops state of the art infrastructure, while no similar
infrastructure is built for the rest of science, academia, and society.

I have no doubt that everyone working on the Translator is doing so for
good reasons, and they have done useful work. Forming a consortium and
settling on a development model is hard work and this group should be
applauded for that. Unifying APIs with Smart-API, drafting an ontology,
and making a knowledge graph, are all directly useful to reducing
barriers to desiloing data and shared in the vision articulated here.

The problems here come in a few mutually reinforcing flavors, I'll group
them crudely into the constraints of existing infrastructure,
centralized models of development, and a misspecification of what the
purpose of the infrastructure should be.

Navigating a relationship with existing technology in new development is
tricky, but there is a distinction between integrating with it and
embodying its implications. Since the other projects spawned from the
Data Science Initiative embraced the use of cloud storage, the
constraint of using centralized servers with the need for a linking
overlay was baked in the project from the beginning. From this decision
immediately comes the impossibility of enforcing privacy guarantees and
the rigidity of database formats and tooling. Since the project started
from a place of presuming that the data would be hosted ``out there''
where much of its existence is prespecified, building the Translator
``on top'' of that system is a natural conclusion. Further, since the
centralized systems proposed in the other projects don't aim to provide
a means of standardization or integration of scientific data that
doesn't already have a form, the reliance on APIs for access to
structured data follows as well.

Organizing the process as building a set of tools as a relatively large,
but nonetheless centralized and demarcated group pose additional
challenges. I won't speculate on the incentives and personal dynamics
that led there, but I also believe this development model comes from
good intention. While there is clearly a lot of delegation and
distributed work, the project in its different teams takes on specific
tools that \emph{they} build and \emph{we} use. This is broadly true of
scientific tools, especially databases, and contributes to how they
\emph{feel}: they feel disconnected with our work, don't necessarily
help us do it more easily or more effectively, and contributing to them
is a burdensome act of charity.

This is reflected in the form of the biolink ontology, where rather than
a tool for scientists to \emph{build} ontologies, it is intended to be
\emph{built towards.} There is tension between the articulated
impossibility of a grand unified ontology and the eventual form of the
algorithm that depends on one that, in their words, motivated the turn
to machine learning to reconcile that impossibility. The compromise
seems to be the use of a quasi-``neutral'' meta-ontology that
instantiates its different abstract objects depending on the contents of
its APIs. A ranking algorithm to parse the potentially infinite results
follows, and so too does the need for feedback and training and the
potential for long-lived and uninterrogatable algorithmic bias.

These all contribute to the misdirection in the goal of the project.
Linking \emph{all} or \emph{most} biomedical data in single mutually
coherent system drifted into an API-driven knowledge-graph for
pharmaceutical and clinical recommendations. Here we meet a bit of a
reprise of the \protect\hyperlink{neatness-vs-scruffiness}{\#neat}
mindset, which emphasizes global coherence as a basis for reasoning
rather than providing a means of expressing the natural connections
between things in their local usage. Put another way, the emphasis is on
making something logically complete for some dream of
algorithmically-perfect future rather than to be useful to do the things
researchers at large want to do but find difficult. The press releases
and papers of the Translator project echo a lot of the heady days of the
semantic web\footnote{not to mention a sort of enlightenment-era
  diderot-like quest for the encyclopedia of everything} and its attempt
to link everything --- and seems ready to follow the same path of the
fledgling technologies being gobbled up by technology giants to finish
and privatize.

I think the problem with the initial and eventual goals of the
translator can be illustrated by problematizing the central focus on
linking ``all data,'' or at least ``all biomedical data.'' Who is a
system of ``all (biomedical) data'' for? Outside of metascientists and
pharmaceutical companies, I think most people are interested primarily
in the data of their colleagues and surrounding disciplines. Every
infrastructural model is an act of balancing constraints, and
prioritizing ``all data'' seems to imply ``for some people.'' Who is
supposed to be able to upload data? change the ontology? inspect the
machine learning model? Who is in charge of what? Who is a
knowledge-graph query engine useful for?

Another prioritization might be building systems for \emph{all people}
that can \emph{embed with existing practices} and \emph{help them do
their work} which typically involves accessing \emph{some data.} The
system needs to not only be designed to allow anyone to integrate their
data into it, but also to be integrated into how researchers collect and
use their data. It needs to give them firm, verifiable, and fine-grained
control over who has access to their data and for what purpose. It needs
to be \emph{pluralistic,} capable of representing multiple potentially
conflicting representations, governable and malleable in local
communities of practice. Through the normal act of making my data
available to my colleague and vice versa, we build on a cumulative and
negotiable understanding of the relationship between our work and its
meaning.

Without too much more prefacing, let's return to the scheduled
programming.

TODO: Elsevier {has even scarier plans}

Trans health {example of potential harms}






\subsection{Federated Systems (of
Language)}



 When last we left it, our peer-to-peer system needed some
way of linking data together. Instead of a big bucket of files as is
traditional in torrents and domain-general databases, we need some way
of exposing the metadata of disparate data formats so that we can query
for and find the particular range of datasets appropriate to our
question. We'll be playing in the world of linked data and the semantic
web, but thinking about using those tools for a fluid means of
expression more akin to natural language than to an engineering
specification.

Each format has a different metadata structure with different names, and
even within a single format we want to support researchers who extend
and modify the core format. Additionally, each format has a different
implementation, eg. as an hdf5 file, binary files in structured
subdirectories, SQL-like databases.

That's a lot of heterogeneity to manage, but fret not: there is hope.
Researchers navigate this variability manually as a standard part of the
job, and we can make that work cumulative by building tools that allow
researchers to communally describe and negotiate over the structure of
their data and the local relationships to other data structures. We can
extend our peer-to-peer system to be a \emph{federated database}
system\footnote{There is a lot of subtlety to the terminology
  surrounding ``federated'' and the typology of distributed systems
  generally, I am using it in the federated messaging sense of forming
  groups of people, rather than the strict term ``federated databases''
  which do imply a standardized schema across a federation. The
  conception of distributed, autonomous databases described by the
  DataLad team \citep{hankeDefenseDecentralizedResearch2021}  is a
  bit closer to my meaning. In the ActivityPub world, federations refer
  to a single homeserver under which many people can sign up. We mean
  something similar but distinct: people that have autonomous
  ``homeservers'' in a peer to peer system, typically multiple
  identities for a single person rather than many people on a single
  server, that can combine into federations with particular governance
  structures and technological systems attached.}.

Federated systems consist of \emph{distributed}, \emph{heterogeneous},
and \emph{autonomous} agents that implement some minimal agreed-upon
standards for mutual communication and (co-)operation. Federated
databases were proposed in the early 1980's \citep{heimbignerFederatedArchitectureInformation1985}  and have been
developed and refined in the decades since as an alternative to either
centralization or non-integration \citep{litwinInteroperabilityMultipleAutonomous1990, kashyapSemanticSchematicSimilarities1996, hullManagingSemanticHeterogeneity1997} . Their application to the
dispersion of scientific data in local filesystems is not new \citep{busseFederatedInformationSystems1999, djokic-petrovicPIBASFedSPARQLWebbased2017, hasnainBioFedFederatedQuery2017} , but their implementation is more
challenging than imposing order with a centralized database or punting
the question into the unknowable maw of machine learning.

Amit Sheth and James Larson, in their reference description of federated
database systems, describe \textbf{design autonomy} as one critical
dimension that characterizes them:

\begin{quote}
Design autonomy refers to the ability of a component DBS to choose its
own design with respect to any matter, including

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  The \textbf{data} being managed (i.e., the Universe of Discourse),
\item
  The \textbf{representation} (data model, query language) and the
  \textbf{naming} of the data elements,
\item
  The conceptualization or \textbf{semantic interpretation} of the data
  (which greatly contributes to the problem of semantic heterogeneity),
\item
  \textbf{Constraints} (e.g., semantic integrity constraints and the
  serializability criteria) used to manage the data,
\item
  The \textbf{functionality} of the system (i.e., the operations
  supported by system),
\item
  The \textbf{association and sharing with other systems}, and
\item
  The \textbf{implementation} (e.g., record and file structures,
  concurrency control algorithms).
\end{enumerate}
\end{quote}

Susanne Busse and colleagues add an additional dimension of
\textbf{evolvability,} or the ability of a particular system to adapt to
inevitable changing uses and requirements \citep{busseFederatedInformationSystems1999} .

In order to support such radical autonomy and evolvability, federated
systems need some means of translating queries and representations
between heterogeneous components. The typical conceptualization of
federated databases have five layers that implement different parts of
this reconciliation process \citep{shethFederatedDatabaseSystems1990} :

\begin{itemize}

\item
  A \textbf{local schema} is the representation of the data on local
  servers, including the means by which they are implemented in binary
  on the disk
\item
  A \textbf{component schema} serves to translate the local schema to a
  format that is compatible with the larger, federated schema
\item
  An \textbf{export schema} defines permissions, and what parts of the
  local database are made available to the federation of other servers
\item
  The \textbf{federated schema} is the collection of export schemas,
  allowing a query to be broken apart and addressed to different export
  schemas. There can be multiple federated schemas to accomodate
  different combinations of export schemas.
\item
  An \textbf{external schema} can further be used to make the federated
  schema better available to external users, but in this case since
  there is no notion of ``external'' it is less relevant.
\end{itemize}

This conceptualization provides a good starting framework and isolation
of the different components of a database system, but a peer-to-peer
database system has different constraints and opportunities \citep{bonifatiDistributedDatabasesPeertopeer2008} . In the strictest,
``tightly coupled'' federated systems, all heterogeneity in individual
components has to be mapped to a single, unified federation-level
schema. Loose federations don't assume a unified schema, but settle for
a uniform query language, and allow multiple translations and views on
data to coexist. A p2p system naturally lends itself to a looser
federation, and also gives us some additional opportunities to give
peers agency over schemas while also preserving some coherence across
the system. I will likely make some database engineers cringe, but the
emphasis for us will be more on building a system to support distributed
social control over the database, rather than guaranteeing consistency
and transparency between the different components.

Though there are hundreds of subtleties and choices in implementation
beneath the level of detail I'll reach here, allow me to illustrate the
system by example:

Let us start with the ability for a peer to choose who they are
associated with at multiple scales of organization: a peer can directly
connect with another peer, but peers can also federate into groups,
groups can federate into groups of groups, and so on. Within each of
these grouping structures, the peer is given control over what data of
theirs is shared.

Clearly, we need some form of \emph{identity} in the system, let's make
it simple and flat and denote that in pseudocode as
\texttt{@username}\footnote{In reality, without any form of distributed
  uniqueness checking, we would need to have some notion of where this
  username is ``from,'' so let's say we actually have a system like
  \texttt{username@name-provider} but for this example assume a single
  name provider, we'll return to identity provision.}. Someone would
then be able to use their \texttt{@name}space as a root, under which
they could refer to their data, schemas, and so on, which will be
denoted \texttt{@name:subobject} (see this notion of personal namespaces
for knowledge organization discussed in early wiki culture here \citep{MeatballWikiPersonalCategories} ). Let us also assume that there
is no categorical difference between \texttt{@usernames} used by
individual researchers, institutions, consortia, etc. --- everyone is on
the same level.

We pick up where we left off earlier with a peer who has their data in
some discipline-specific format, which let us assume for the sake of
concreteness has a representation as an
\href{https://www.w3.org/OWL/}{OWL} schema.

That schema could be ``owned'' by the \texttt{@username} corresponding
to the standard-writing group --- eg \texttt{@nwb} for neurodata without
borders. In a \href{https://www.w3.org/TR/turtle/}{turtle-ish}
pseudocode\footnote{We could use actual turtle or JSON-LD syntax
  throughout, but I am using a simplified pseudocode to a) make it a bit
  more readable for the sake of illustration and b) be very explicit
  that this is not intended to be a proposal for a protocol or
  specification, but demonstrative of its qualities.}, then, our dataset
might look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#cool{-}dataset\textgreater{}}
\NormalTok{  a @nwb:NWBFile}
\NormalTok{  @nwb:general:experimenter @jonny}
\NormalTok{  @nwb:ElectricalSeries}
\NormalTok{    .electrodes [1, 2, 3]}
\NormalTok{    .rate 30000}
\NormalTok{    .data [...]}
\end{Highlighting}
\end{Shaded}

This indicates that me, \texttt{@jonny} collected
\texttt{a\ @nwb:NWBFile} dataset named
\texttt{\textless{}\#cool-dataset\textgreater{}}, making it available as
\texttt{@jonny:cool-dataset} that consisted of an
\texttt{@nwb:ElectricalSeries} and the relevant attributes (where a
leading \texttt{.} is a shorthand for the parent schema element, so
\texttt{@nwb:ElectricalSeries:electrodes}).

I have some custom field for my data, though, which I extend the format
specification to represent. Say I have invented some new kind of
solar-powered electrophysiological device --- the SolarPhys2000 --- and
want to annotate its specs alongside my data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#SolarEphys\textgreater{}}
\NormalTok{  extends @nwb:NWBContainer}
    
\NormalTok{  UsedWith @hw:SolarPhys2000}

\NormalTok{  ManufactureDate}
\NormalTok{    a @schema:Date}

\NormalTok{  InputWattageSeries}
\NormalTok{    extends @nwb:ElectricalSeries}

\NormalTok{    sunIntensity}
\NormalTok{      a @nwb:TimeSeries}
\end{Highlighting}
\end{Shaded}

Here I create a new extension \texttt{@jonny:SolarEphys} that
\texttt{extends} the \texttt{@nwb:NWBContainer} schema. We use
\texttt{extends} rather than \texttt{a} because we are adding something
new to the \emph{description} of the container rather than \emph{making}
a container to store data. I declare that this container is
\texttt{UsedWith} our SolarPhys2000 which we have registered in some
general \texttt{@hw} hardware registry. I then add two new fields,
\texttt{ManufactureDate} and \texttt{InputWattageSeries}, declaring
types from, for example
\href{https://schema.org/Date}{\texttt{@schema:Date}} and \texttt{@nwb}.

There are many strategies for making my ontology extension available to
others in a federated network. We could use a distributed hash table, or
\href{https://en.wikipedia.org/wiki/Distributed_hash_table}{\textbf{DHT}},
like bittorrent, which distributes references to information across a
network of peers (eg. \citep{pirroDHTbasedSemanticOverlay2012} ).
We could use a strategy like the
\href{https://matrix.org/}{\textbf{Matrix} messaging protocol}, where
users belong to a single home server that federates with other servers.
Each server is responsible for keeping a synchronized copy of the
messages sent on the servers and rooms it's federated with, and each
server is capable of continuing communication if any of the others
failed. We could use
\href{https://www.w3.org/TR/2018/REC-activitypub-20180123/}{\textbf{ActivityPub}
(AP)} \citep{Webber:18:A} , a publisher-subscriber model where
users affiliated with a server post messages to their `outbox' and are
sent to listening servers (or made available to HTTP GET requests). AP
uses \href{https://json-ld.org/}{JSON-LD} \citep{spornyJSONLDJSONbasedSerialization2020} , so is already capable of
representing linked data, and the related ActivityStreams vocabulary
\citep{snellActivityStreams2017}  also has plenty of relevant
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#activity-types}{action
types} for
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-create}{creating},
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-question}{discussing},
and
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-tentativeaccept}{negotiating}
over links (also see
\href{https://github.com/openEngiadina/cpub}{cpub}). We'll return to
ActivityPub later, but for now the point is to let us assume we have a
system for distributing schemas/extensions/links associated with an
identity publicly or to a select group of peers.

For the moment our universe is limited only to other researchers using
NWB. Conveniently, the folks at NWB have set up a federating group so
that everyone who uses it can share their format extensions. Since our
linking system for manipulating schemas is relatively general, we can
use it to ``formalize'' a basic configuration for a federating group
that automatically \texttt{Accept}s request to \texttt{Join} and allows
any schema that inherits from their base \texttt{@nwb:NWBContainer}
schema. Let's say \texttt{@fed} defines some basic properties of our
federating system --- it constitutes our federating ``protocol'' --- and
loosely use some terms from the
\href{https://www.w3.org/ns/activitystreams\#class-definitions}{ActivityStreams}
vocabulary as \texttt{@as}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#nwbFederation\textgreater{}}
\NormalTok{  a @fed:Federation}
\NormalTok{  onReceive}
\NormalTok{    @as:Join @as:Accept}
\NormalTok{  allowSchema}
\NormalTok{    extensionOf @nwb:NWBContainer}
\end{Highlighting}
\end{Shaded}

Now anyone that is a part of the \texttt{@nwbFederation} would be able
to see the schemas we have submitted, sort of like a beefed up,
semantically-aware version of the existing
\href{https://nwb-extensions.github.io/}{neurodata extensions catalog}.
In this system, many overlapping schemas could exist simultaneously
under different namespaces, but wouldn't become a hopeless clutter
because similar schemas could be compared and reconciled based on their
semantic properties.

So far we have been in the realm of metadata, but how would my computer
know how to read and write the data to my disk so i can use it? In a
system with heterogeneous data types and database implementations, we
need some means of specifying different programs to use to read and
write, different APIs, etc. Why not make that part of the file schema as
well? Suppose the HDF5 group (or anyone, really!) has a namespace
\texttt{@hdf} that defines the properties of an \texttt{@hdf:HDF5} file,
basic operations like \texttt{Read}, \texttt{Write}, or \texttt{Select}.
NWB could specify that in their definition of \texttt{@nwb:NWBFile}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@nwb.NWBFile}
\NormalTok{  a @hdf:HDF5}
\NormalTok{    isVersion "x.y.z"}
\NormalTok{    hasDependency "libhdf5"=="x.y.z"}
\NormalTok{  usesContainer @nwb:NWBContainer}
\end{Highlighting}
\end{Shaded}

The abstraction around the file implementation makes it easier for
others to consume my data, but it also makes it easier for \emph{me} to
use and contribute to the system. Making an extension to the schema
wasn't some act of charity, it was the most direct way for me to use the
tool to do what I wanted. Win-win: I get to use my fancy new instrument
and store its data by extending some existing format standard, and in
the process make the standard more complete and useful. We are able to
make my work useful by \emph{aligning the modalities of use and
contribution.}

Now that I've got my schema extension written and submitted to the
federation, time to submit my data! Since it's a p2p system, I don't
need to manually upload it, but I do want to control who gets it. By
default, I have all my NWB datasets set to be available to the
\texttt{@nwbFederation} , and I list all my metadata on, say the Society
for Neuroscience's \texttt{@sfnFederation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#globalPermissions\textgreater{}}
\NormalTok{  a @fed:Permissions}
\NormalTok{  permissionsFor @jonny}

\NormalTok{  federatedWith }
\NormalTok{    name @nwbFederation}
\NormalTok{    @fed:shareData }
\NormalTok{      is @nwb:NWBFile}

\NormalTok{  federatedWith}
\NormalTok{    name @sfnFederation}
\NormalTok{    @fed:shareMetadata}
\end{Highlighting}
\end{Shaded}

Let's say this dataset in particular is a bit sensitive --- say we apply
a set of permission controls to be compliant with \texttt{@hhs.HIPAA}
--- but we do want to make use of some public server space run by our
Institution, so we let it serve an encrypted copy that those I've shared
it with can decrypt. Since we've applied the \texttt{@hhs.HIPAA}
ruleset, we would be able to automatically detect if we have any
conflicting permissions, but we're doing fine in this example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#datasetPermissions\textgreater{}}
\NormalTok{  a @fed:Permissions}
\NormalTok{  permissionsFor @jonny:cool{-}dataset}

\NormalTok{  accessRuleset @hhs:HIPAA}
\NormalTok{    .authorizedRecipient \textless{}\#hash{-}of{-}patient{-}ids\textgreater{}}
  
\NormalTok{  federatedWith}
\NormalTok{    name @institutionalCloud}
\NormalTok{    @fed:shareEncrypted}
\end{Highlighting}
\end{Shaded}

Now I want to make use of some of my colleagues data. Say I am doing an
experiment with a transgenic dragonfly and collaborating with a chemist
down the hall. This transgene, known colloquially in our discipline as
\texttt{"@neuro:superstar6"} (which the chemists call
\texttt{"@chem:SUPER6"}) fluoresces when the dragonfly is feeling
bashful, and we have plenty of photometry data stored as
\texttt{@nwb:Fluorescence} objects. We think that its fluorescence is
caused by the temperature-dependent conformational change from blushing.
They've gathered NMR and Emission spectroscopy data in their
chemistry-specific format, say \texttt{@acs:NMR} and
\texttt{@acs:Spectroscopy}.

We get tired of having our data separated and needing to maintain a
bunch of pesky scripts and folders, so we decide to make a bridge
between our datasets. We need to indicate that our different names for
the gene are actually the same thing and relate the spectroscopy data.

Let's make the link explicit, say we use an already-existing vocabulary
like the ``simple knowledge organization system'' for describing logical
relationships between concepts:
\href{https://www.w3.org/2009/08/skos-reference/skos.html}{\texttt{@skos}}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#super{-}link{-}6\textgreater{}}
\NormalTok{  a @fed:Link}
  
\NormalTok{  from @neuro:superstar6}
\NormalTok{  to @chem:SUPER6}
\NormalTok{  link @skos:exactMatch}
\end{Highlighting}
\end{Shaded}

Our \texttt{@nwb:Fluorescence} data has the emission wavelength in its
\texttt{@nwb:Fluorescence:excitation\_lambda} property\footnote{not
  really where it would be in the standard, but go with it plz}, which
is the value of their \texttt{@acs:Spectroscopy} data at a particular
value of its \texttt{wavelength}. Unfortunately, \texttt{wavelength}
isn't metadata for our friend, but does exist as a column in the
\texttt{@acs:Spectroscopy:readings} table, so for now the best we can do
is indicate that \texttt{excitation\_lambda} is one of the values in
\texttt{wavelength} and pick it up in our analysis tools.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#imaging\textgreater{}}
\NormalTok{ a @fed:Link}
 
\NormalTok{ from @nwb:Fluorescence:excitation\_lambda}
\NormalTok{ to @acs:Spectroscopy:readings}
\NormalTok{ link @fed:Subset}
\NormalTok{   valueIn "wavelength"}
\end{Highlighting}
\end{Shaded}

This makes it much easier for us to index our data against each other
and solves a few real practical problems we were facing in our
collaboration. We don't need to do as much cleaning when it's time to
publish the data since it can be released as a single linked entity.

Rinse and repeat our sharing and federating process from our previous
schema extension, add a little bit of extra federation with the
\texttt{@acs} namespace, and in the normal course of our doing our
research we've contributed to the graph structure linking two common
data formats. Ours is one of many, with idiosyncratic names like
\texttt{@jonny:super-link-6}\footnote{we'll return to credit assignment,
  don't worry! I wouldn't leave a friend out to dry.}. We might not have
followed the exact rules, and we only made a few links rather than a
single authoratative mapping, but if someone is interested in compiling
one down the line they'll start off a hell of a lot further than if we
hadn't contributed it!

With a protocol for how queries can be forwarded and transformed between
users and federations, one could access the same kind of complex query
structure as traditional databases with
\href{https://www.w3.org/TR/sparql11-federated-query/}{SPARQL} \citep{SPARQLFederatedQuery2013}  as has been proposed for biology many
times before \citep{simaEnablingSemanticQueries2019, djokic-petrovicPIBASFedSPARQLWebbased2017, hasnainBioFedFederatedQuery2017} . Some division in the way that data
and metadata are handled is necessary for the network to work in
practice, since we can't expect a search to require terabytes of data
transfer. A natural solution to this is to have metadata query results
point to
\href{https://en.wikipedia.org/wiki/Content-addressable_storage}{content
addressed} identifiers that are served peer to peer. A
mutable/changeable/human-readable name and metadata system that points
to a system of permanent, unique identifiers has been one need that has
hobbled IPFS, and is the direction pointed to by DataLad\footnote{\href{https://www.datalad.org/}{DataLad}
  \citep{halchenkoDataLadDistributedSystem2021, hankeDefenseDecentralizedResearch2021}  and its application in
  Neuroscience as \href{https://dandiarchive.org}{DANDI} are two
  projects that are \emph{very close} to what I have been describing
  here --- developing a p2p backend for datalad might even be a
  promising development path towards it.} \citep{hankeDefenseDecentralizedResearch2021} . A
\href{https://mastodon.social/@humanetech/107155144840782386}{parallel}
\href{https://web.archive.org/web/20211024082055/https://socialhub.activitypub.rocks/t/which-links-between-activitypub-and-solid-project/529}{set}
of
\href{https://web.archive.org/web/20211024080845/https://socialhub.activitypub.rocks/t/how-solid-and-activitypub-complement-each-other-best/727}{conversations}
has been
\href{https://web.archive.org/web/20211024081238/https://forum.solidproject.org/t/discussion-solid-vs-activitypub/2685}{happening}
in the broader linked data community with regard to using ActivityPub as
a way to index data on Solid.

In this example I have been implicitly treating the
\texttt{@nwbFederation} users like bittorrent
\protect\hyperlink{trackers--wikis}{trackers}, keeping track of
different datasets in their federation and caching for faster quaeries.
There is no reason why queries couldn't themselves be distributed across
the participating peers, though I believe tracker-like federations at
various intermediary scales of organization are useful and might emerge
naturally. A system like this doesn't need the radical zero trust design
of, for example, some distributed ledgers, and an overlapping array of
institutional, disciplinary, interest, and so on federations would be a
good means of realizing the evolvable community structure needed for
sustained archives.

Extend this practice across the many overlapping gradients of
cooperation and collaboration in science, and on a larger scale a system
like this could serve as a way to make explicit the organic, continual
negotiation over meaning and practice that centralized ontologies can
only capture as a snapshot. I don't happen to know where the physicists
store their data or what format it's in, \emph{but the chemists might,}
and the best way to get there from here might be a dense, multiplicative
web of actual practical knowledge instead of some sparsely used
corporate API. We don't make the same guarantees of consistency or
support for algorithmic reasoning as a top-down system would in theory,
but it would give us agency over the structure of our information and
have the potential to be useful for a far broader base of researchers.

As will be developed through the rest of the piece, this system
effectively functions as a \emph{protocol for protocols.} The
\texttt{@fed:Federation} notion of federation defines its own set of
properties and requirements that let the people who implement it
implement a variety of federation types and relationships with each
other, but it is one of many. By building on the ``meta-protocol'' of a
peer-to-peer linked data platform we can change the problem from needing
to agree on a single protocol for scientific communication, data
indexing, and so on to making a means of creating \emph{many} protocols
as fluidly as is needed. Are the rules of \texttt{@fed:Federation} too
limited? Extend it to suit your needs.

Like the preceding description of the basic peer-to-peer system, this
joint metadata/p2p system could be fully compatible with existing
systems. Translating between a metadata query and a means of accessing
it on heterogeneous databases is a requisite part of the system, so, for
example, there's no reason that an HTTP-based API like SmartAPI couldn't
be queried.


\section{Shared Tools}



 Straddling our system for sharing data are the tools to
gather and analyze it --- two examples of the more general need for
computational resources. Experimental and analytical tools are the
natural point of extension for collectively developed scientific digital
infrastructure, and considering them together shows the combinatoric
power of integrating interoperable domains of scientific practice. In
particular, in addition to benefits from their development in isolation,
we can ask how a more broadly integrated system helps problems like
adoption and incentives for distributed work, enables a kind of deep
provenance from experiment to results, and further builds us towards
reimagine the form of the community and communication tools for science.

This section will be relatively short compared to
\protect\hyperlink{shared-data}{shared data}. We have already
introduced, motivated, and exemplified many of the design practices of
the broader infrastructural system. There is much less to argue against
or ``undo'' in the spaces of analytical and experimental tools because
so much more work has been done, and so much more power has been accrued
in the domain of data systems. Distributed computing does have a dense
history, with huge numbers of people working on the problem, but its
dominant form is much closer to the system articulated below than
centralized servers are to federated semantic p2p systems. I also have
written extensively about
\protect\hyperlink{experimental-frameworks}{experimental frameworks}
before \citep{saundersAutopilotAutomatingBehavioral2019} , and
develop \href{https://docs.auto-pi-lot.com/en/latest/}{one of them} so I
will be brief at risk of repeating myself or appearing self-serving.

Integrated scientific workflows have been written about many times
before, typically in the context of the ``open science'' movement. One
of the founders of the Center for Open Science, Jeffrey Spies, described
a similar ethic of toolbuilding as I have in a 2017 presentation:

\begin{quote}
Open Workflow: 1. Meet users where they are 2. Respect current
incentives 3. Respect current workflow

\begin{itemize}

\item
  We could\ldots{} demonstrate that it makes research more efficient, of
  higher quality, and more accessible.
\item
  Better, we could\ldots{} demonstrate that researchers will get
  published more often.
\item
  Even better, we could\ldots{} make it easy.
\item
  Best, we could\ldots{} make it automatic \citep{spiesWorkflowCentricApproachIncreasing2017} 
\end{itemize}
\end{quote}

To build an infrastructural system that enables ``open'' practices,
\emph{convincing} or \emph{mandating} a change are much less likely to
be successful and sustainable than focusing on building them to make
doing work easier and openness automatic. To make this possible, we
should focus on developing \emph{frameworks to build} experimental and
analysis tools, rather than developing more tools themselves.


\subsection{Analytical Frameworks}

The first natural companion of shared data infrastructure is a shared
analytical framework. A major driver for the need for everyone to write
their own analysis code largely from scratch is that it needs to account
for the idiosyncratic structure of everyone's data. Most scientists are
(blessedly) not trained programmers, so code for loading and negotiating
loading data is often intertwined with the code used to analyze and plot
it. As a result it is often difficult to repurpose code for other
contexts, so the same analysis function is rewritten in each lab's local
analysis repository. Since sharing raw data and code is still a
(difficult) novelty, on a broad scale this makes results in scientific
literature as reliable as we imagine all the private or semi-private
analysis code to be.

Analytical tools (anecdotally) make up the bulk of open source
scientific software, and range from foundational and general-purpose
tools like numpy \citep{harrisArrayProgrammingNumPy2020}  and
scipy \citep{virtanenSciPyFundamentalAlgorithms2020} , through
tools that implement a class of analysis like DeepLabCut \citep{mathisDeepLabCutMarkerlessPose2018a}  and scikit-learn \citep{pedregosaScikitlearnMachineLearning2011} , to tools for a specific
technique like MoSeq \citep{wiltschkoRevealingStructurePharmacobehavioral2020}  and DeepSqueak
\citep{coffeyDeepSqueakDeepLearningbased2019} . The pattern of
their use is then to build them into a custom analysis system that can
then in turn range in sophistication from a handful of
flash-drive-versioned scripts to automated pipelines.

Having tools like these of course puts researchers miles ahead of where
they would be without them, and the developers of the mentioned tools
have put in a tremendous amount of work to build sensible interfaces and
make them easier to use. No matter how much good work might be done,
inevitable differences between APIs is a relatively sizable technical
challenge for researchers --- a problem compounded by the incentives for
fragmentation described previously. For toolbuilders, many parts of any
given tool from architecture to interface have to be redesigned each
time with varying degrees of success . For science at large, with few
exceptions of well-annotated and packaged code, most results are only
replicable with great effort.

Discontinuity between the behavior and interface of different pieces of
software is, of course, overwhelming norm. Negotiating boundaries
between (and even within) software and information structures is an
elemental part of computing. The only time it becomes a conceivable
problem to ``solve'' interoperability is when the problem domain
coalesces to the point where it is possible to articulate its abstract
structure as a protocol, and the incentives are great enough to adopt
it. Thankfully that's what we're trying to do here.

It's unlikely that we will solve the problem of data analysis being
complicated, time consuming, and error prone by teaching every scientist
to be a good programmer, but we can build experimental frameworks that
make analysis tools easier to build and use.

Specifically, a shared analytical framework should be

\begin{itemize}

\item
  \textbf{Modular} - Rather than implementing an entire analysis
  pipeline as a monolith, the system should be broken into minimal,
  composable modules. The threshold of what constitutes ``minimal'' is
  of course to some degree a matter of taste, but the framework doesn't
  need to make normative decisions like that. The system should support
  modularity by providing a clear set of hooks that tools can provide:
  eg. a clear place for a given tool to accept some input, parameters,
  and so on. Since data analysis can often be broken up into a series of
  relatively independent stages, a straightforward (and common) system
  for modularity is to build hooks to make a directed acyclic graph
  (DAG) of data transformation operations. This structure naturally
  lends itself to many common problems: caching intermediate results,
  splitting and joining multiple inputs and outputs, distributing
  computation over many machines, among others. Modularity is also
  needed within the different parts of the system itself -- eg. running
  an analysis chain shouldn't require a GUI, but one should be
  available, etc.
\item
  \textbf{Pluggable} - The framework needs to provide a clear way of
  incorporating external analysis packages, handling their dependencies,
  and exposing their parameters to the user. Development should ideally
  not be limited to a single body of code with a single mode of
  governance, but should instead be relatively conservative about
  requirements for integrating code, and liberal with the types of
  functionality that can be modified with a plugin. Supporting plugins
  means supporting people developing tools for the framework, so it
  needs to make some part of the toolbuilding process easier or
  otherwise empower them relative to an independent package. This
  includes building a visible and expressive system for submitting and
  indexing plugins so they can be discovered and credit can be given to
  the developers. Reciprocal to supporting plugins is being
  interoperable with existing and future systems, which the reader may
  have assumed was a given by now.
\item
  \textbf{Deployable} - For wide use, the framework needs to be easy to
  install and deploy locally and on computing clusters. A primary
  obstacle is dependency management, or making sure that the computer
  has everything needed to run the program. Some care needs to be taken
  here, as there are multiple emphases in deployability that can be in
  conflict. Deployable for who? A system that can be relatively
  challenging to use for routine exploratory data analysis but can
  distribute analysis across 10,000 GPUs has a very circumscribed set of
  people it is useful for. This is a matter of balancing design
  constraints, but we should prioritize broad access, minimal
  assumptions of technological access, and ease of use over being able
  to perform the most computationally demanding analyses possible when
  in conflict. Containerization is a common, and the most likely
  strategy here, but the interface to containers may need a lot of care
  to make accessible compared to opening a fresh .py file.
\item
  \textbf{Reproducible} - The framework should separate the
  \emph{parameterization} of a pipeline, the specific options set by the
  user, and its \emph{implementation}, the code that constitutes it. The
  parameterization of a pipeline or analysis DAG should be portable such
  that it, for example, can be published in the supplementary materials
  of a paper and reproduced exactly by anyone using the system. The
  isolation of parameters from implementation is complementary to the
  separation of metadata from data and if implemented with semantic
  triplets would facilitate a continuous interface from our data to
  analysis system. This will be explored further below and in
  \protect\hyperlink{shared-knowledge}{shared knowledge}
\end{itemize}

Thankfully a number of existing projects that are very similar to this
description are actively being built. One example is
\href{https://datajoint.io/}{DataJoint} \citep{yatsenkoDataJointSimplerRelational2018} , which recently expanded its
facility for modularity with its recent
\href{https://github.com/datajoint/datajoint-elements}{Elements} project
\citep{yatsenkoDataJointElementsData2021} . Datajoint is a system
for creating analysis pipelines built from a graph of processing stages
(among
\href{https://docs.datajoint.org/python/v0.13/intro/01-Data-Pipelines.html\#what-is-datajoint}{other
features}). It is designed around a refinement on traditional relational
data models, which is reflected throughout the system as most operations
being expressed in its particular schema, data manipulation, and query
languages. This is useful for operations that are expressed in the
system, but makes it harder to integrate external tools with their
dependencies ---
\href{https://github.com/datajoint/element-array-ephys/blob/1fdbcf12d1a518e686b6b79e9fbe77b736cb606a/Background.md}{at
the moment} it appears that spike sorting (with
\href{https://github.com/MouseLand/Kilosort}{Kilosort} \citep{pachitariuKilosortRealtimeSpikesorting2016} ) has to happen outside
of the extracellular electrophysiology elements pipeline.

Kilosort is an excellent and incredibly useful tool, but its idiomatic
architecture designed for standalone use is illustrative of the
challenge of making a general-purpose analytic framework that can
integrate a broad array of existing tools. It is built in MATLAB, which
requires a paid license, making arbitrary deployment difficult, and
MATLAB's flat path system requires careful and usual manual
orchestration of potentially conflicting names in different packages.
Its parameterization and use are combined in a
``\href{https://github.com/MouseLand/Kilosort/blob/db3a3353d9a374ea2f71674bbe443be21986c82c/main_kilosort3.m}{main}''
script in the repository root that creates a MATLAB struct and runs a
series of functions --- requiring some means for a wrapping framework to
translate between input parameters and the representation expected by
the tool. Its preprocessing script combines
\href{https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/datashift.m\#L74-L77}{I/O},
preprocessing, and
\href{https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/datashift.m\#L57-L68}{plotting},
and requires data to be
\href{https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/preprocessDataSub.m\#L82-L84}{loaded
from disk} rather than passed as arguments to preserve memory --- making
chaining in a pipeline difficult.

This is not a criticism of Datajoint or Kilosort, which were both
designed for different uses and with different philosophies (that are of
course, also valid). I mean this as a brief illustration of the design
challenges and tradeoffs of these systems.

We can start getting a better picture for the way a decentralized
analysis framework might work by considering the separation between the
metadata and code modules, hinting at a protocol as in the federated
systems sketh above. Since we're considering modular analysis elements,
each module would need some elemental properties like the parameters
that define it, its inputs, outputs, dependencies, as well as some
additional metadata about its implementation (eg. this one takes
\emph{numpy arrays} and this one takes \emph{matlab structs}). The
precise implementation of a modular protocol also depends on the graph
structure of the analysis system. We invoked DAGs before, but analysis
graph structure of course has its own body of researchers refining them
into eg. \href{https://en.wikipedia.org/wiki/Petri_net}{Petri nets}
which are graphs whose nodes necessarily alternate between ``places''
(eg. intermediate data) and ``transitions'' (eg. an analysis operation),
and their related workflow markup languages (eg.
\href{https://openwdl.org/}{WDL} or \citep{vanderaalstYAWLAnotherWorkflow2005} ). In that scheme, a framework
could provide tools for converting data between types, caching
intermediate data, etc. between analysis steps, as an example of how
different graph structures might influence its implementation.

Say we use \texttt{@analysis} as the namespace for our analysis
protocol, and \texttt{\textasciitilde{}someone\textasciitilde{}} has
provided mappings to objects in \texttt{numpy}. We can assume they are
provided by the package maintainers, but that's not necessary: this is
my node and it takes what I want it to!

In pseudocode, I could define some analysis node for, say, converting an
RGB image to grayscale under my namespace as \texttt{@jonny:bin-spikes}
like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#bin{-}spikes\textgreater{}}
\NormalTok{  a @analysis:node}
\NormalTok{    Version "\textgreater{}=1.0.0"}

\NormalTok{  hasDescription}
\NormalTok{    "Convert an RGB Image to a grayscale image"}

\NormalTok{  inputType}
\NormalTok{    @numpy:ndarray}
\NormalTok{      \# ... some spec of shape ...}

\NormalTok{  outputType}
\NormalTok{    @numpy:ndarray}
\NormalTok{      \# ... some spec of shape ...}
\end{Highlighting}
\end{Shaded}

I have abbreviated the specification of shape to not overcomplicate the
pseudocode example, but say we successfully specify a 3 dimensional
(width x height x channels) array with 3 channels as input, and a a 2
dimensional (width x height) array as output.

The code doesn't run on nothing! We need to specify our node's
dependencies, say in this case we need to specify an operating system
image \texttt{ubuntu}, a version of \texttt{python}, a system-level
package \texttt{opencv}, and a few python packages on \texttt{pip}. We
are pinning specific versions with \href{https://semver.org/}{semantic
versioning}, but the syntax isn't terribly important. Then we just need
to specify where the code for the node itself comes from:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  dependsOn}
\NormalTok{    @ubuntu:"\^{}20.*":x64}
\NormalTok{    @python:"3.8"}
\NormalTok{    @apt:opencv:"\^{}4.*.*"}
\NormalTok{    @pip:opencv{-}python:"\^{}4.*.*"}
\NormalTok{    @pip:numpy:"\^{}14.*.*"}

\NormalTok{  providedBy}
\NormalTok{    @git:repository }
\NormalTok{      .url "https://mygitserver.com/binspikes/fast{-}binspikes.git"}
\NormalTok{      .hash "fj9wbkl"}
\NormalTok{    @python:class "/main{-}module/binspikes.py:Bin\_Spikes"}
\end{Highlighting}
\end{Shaded}

Here we can see the advantage of being able to mix and match different
namespaces in a practical sense. Our \texttt{@analysis.node} protocol
gives us several slots to connect different tools together, each in turn
presumably provides some minimal functionality expected by that slot:
eg. \texttt{inputType} can expect \texttt{@numpy:ndarray} to specify its
own dependencies, the programming language it is written in, shape, data
type, and so on. Coercing data between chained nodes then becomes a
matter of mapping between the \texttt{@numpy} and, say a \texttt{@nwb}
namespace of another format. In the same way that there can be multiple,
potentially overlapping between data schemas, it would then be possible
for people to implement mappings between intermediate data formats
as-needed.

This node also becomes available to extend, say someone wanted to add an
additional input format to my node:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}@friend\#bin{-}spikes\textgreater{}}
\NormalTok{  extends @jonny:bin{-}spikes}

\NormalTok{  inputType}
\NormalTok{    @pandas:DataFrame}

\NormalTok{  providedBy}
\NormalTok{    ...}
\end{Highlighting}
\end{Shaded}

They don't have to interact with my potentially messy codebase at all,
but it is automatically linked to my work so I am credited. One could
imagine a particular analysis framework implementation that would then
search through extensions of a particular node for a version that
supports the input/output combinations appropriate for their analysis
pipeline, so the work is cumulative. This functions as a dramatic
decrease in the size of a unit of work that can be shared.

This also gives us healthy abstraction over implementation. Since the
functionality is provided by different, mutable namespaces, we're not
locked into any particular piece of software --- even our
\texttt{@analysis} namespace that gives the \texttt{inputType} etc.
slots could be forked. We could implement the dependency resolution
system as, eg. a docker container, but it also could be just a check on
the local environment if someone is just looking to run a small analysis
on their laptop with those packages already installed.

We use providedBy to indicate a python class which implements the node
in code. We could use an \texttt{Example\_Framework} that provides a set
of classes and methods to implement the different parts of the node (a
la \href{https://luigi.readthedocs.io/en/stable/tasks.html}{luigi}). Our
\texttt{Bin} class inherits from \texttt{Node}, and we implement the
logic of the function by overriding its \texttt{run} method and specify
an output file to store intermediate data (if requested by the pipeline)
with an \texttt{output} method. We also specify a \texttt{bin\_width} as
a \texttt{Param}eter for our node, as an example of how a lightweight
protocol could be bidirectionally specified as an
\protect\hyperlink{shared-knowledge}{interface} to the linked data
format: we could receive a parameterization from our pseudocode
specification, or we could write a framework with a
\texttt{Bin.export\_schema()} that constructs the pseudocode
specification from code.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ Example\_Framework }\ImportTok{import}\NormalTok{ Node, Param, Target}

\KeywordTok{class}\NormalTok{ Bin(Node):}
\NormalTok{  bin\_width }\OperatorTok{=}\NormalTok{ Param(dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{, default}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

  \KeywordTok{def}\NormalTok{ output(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ Target:}
    \ControlFlowTok{return}\NormalTok{ Target(}\StringTok{\textquotesingle{}temporary\_data.pck\textquotesingle{}}\NormalTok{)}

  \KeywordTok{def}\NormalTok{ run(}\VariableTok{self}\NormalTok{, }\BuiltInTok{input}\NormalTok{:}\StringTok{\textquotesingle{}numpy.ndarray\textquotesingle{}}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \StringTok{\textquotesingle{}numpy.ndarray\textquotesingle{}}\NormalTok{:}
    \CommentTok{\# do some stuff}
    \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

Now that we have a handful of processing nodes, we could then describe
some \texttt{@workflow}, taking some \texttt{@nwb:NWBFile} as input, and
then returning some output as a \texttt{:processed} child beneath its
existing namespace. We'll only make a linear pipeline with two stages,
but there's no reason more complex branching and merging couldn't be
described as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#my{-}analysis\textgreater{}}
\NormalTok{  a @analysis:workflow}

\NormalTok{  inputType }
\NormalTok{    @jonny:bin{-}spikes:inputType}

\NormalTok{  outputName}
\NormalTok{    .inputType:processed}

\NormalTok{  step Step1 @jonny:bin{-}spikes}
\NormalTok{  step Step2 @someone{-}else:another{-}step}
\NormalTok{    input Step1:output}
\end{Highlighting}
\end{Shaded}

Having kept the description of our data in particular abstract from the
implementation of the code and the workflow specification, the only
thing left is to apply it to our data! Since the parameters are linked
from the analysis nodes, we can specify them here (or in the workflow).
Assuming literally zero abstraction and using the tried-and-true
``hardcoded dataset list'' pattern, something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#my{-}project\textgreater{}}
\NormalTok{  a @analysis:project}

\NormalTok{  hasDescription}
\NormalTok{    "I gathered some data, and it is great!"}

\NormalTok{  researchTopic}
\NormalTok{    @neuro:systems:auditory:speech{-}processing}
\NormalTok{    @linguistics:phonetics:perception:auditory{-}only}

\NormalTok{  inPaper}
\NormalTok{    @doi:10.1121:1.5091776 }

\NormalTok{  workflow Analysis1 @jonny:my{-}analysis}
\NormalTok{    globalParams}
\NormalTok{      .Step1:params:bin\_width 10}

\NormalTok{    datasets}
\NormalTok{      @jonny.mydata1:v0.1.0:raw}
\NormalTok{      @jonny.mydata2:\^{}0.2.*:raw}
\NormalTok{      @jonny.mydata3:\textgreater{}=0.1.1:raw}
\end{Highlighting}
\end{Shaded}

And there we are! The missing parameters like \texttt{outputName} from
our workflow can be filled in from the defaults filled in the workflow
node. We get some inkling of where we're going later by also being able
to specify the paper this data is associated with, as well as some broad
categories of research topics so that our data as well as the results of
the analysis can be found.

So that's useful, but comparable to existing technologies. The important
part is in the way this hypothetical analysis framework and markup
interact with our data system and emerging federated metadata system ---
The layers of abstraction here are worth unpacking, but we'll hold until
the end of the shared tools section and we have a chance to consider
what this system might look like for experimental tools to contrast
abstraction across the two domains.






\subsection{Experimental Frameworks}



 Data that is to be analyzed has to be collected somehow.
Tools to bridge the body of experimental practice are a different
challenge than analyzing data, or at least so an anecdotal census of
scientific software tools would suggest. \emph{Everyone needs completely
different things!} As practiced, we might imagine the practice of
science as a cone of complexity: We can imagine the relatively few
statistical outcomes from a family of tests and models. For every test
statistic we can imagine a thousand analysis scripts, for every analysis
script we might expect a hundred thousand data formats, and so the
french-horn-bell convexity of complexity of experimental tools used to
collect the data feels \ldots{} different.

Beyond a narrow focus of the software for performing experiments itself,
the contextual knowledge work that surrounds it largely lacks a means of
communication and organization. Scientific papers have increasingly
marginalized methods sections, being pushed to the bottom, abbreviated,
and relegated to supplemental material. The large body of work that is
not immediately germane to experimental results, like animal care,
engineering instruments, lab management, etc. have effectively no formal
means of communication --- and so little formal means of credit
assignment.

Extending our ecosystem to include experimental tools has a few
immediate benefits: bridging the gap between collection and sharing of
data would resolve the need for format conversion as a prerequisite for
inclusion in the linked system, allowing the expression of data to be a
fluid part of the experiment itself. It would also serve as a concrete
means of implementing and building a body of cumulative contextual
knowledge in a creditable system.

I have previously written about the design of a generalizable,
distributed experimental framework in section 2, and about one modular
implementation in section 3 of \citep{saundersAutopilotAutomatingBehavioral2019} , so to avoid repeating
myself, and since many of the ideas from the section on analysis tools
apply here as well, I will be relatively brief.

We don't have the luxury of a natural formalism like a DAG to structure
our experimental tools. Some design constraints on experimental
frameworks might help explain why:

\begin{itemize}

\item
  They need to support a wide variety of instrumentation, from
  \textbf{off-the-shelf parts,} to \textbf{proprietary instruments} as
  are common in eg. microscopy, to \textbf{custom, idiosyncratic
  designs} that might make up the existing infrastructure in a lab.
\item
  To be supportive, rather than constraining, they need to be able to
  \textbf{flexibly perform many kinds of experiments} in a way that is
  \textbf{familiar to patterns of existing practice.} That effectively
  means being able to coordinate heterogeneous instruments in some
  ``task'' with a flexible syntax.
\item
  They need to be \textbf{inexpensive to implement,} in terms of both
  money and labor, so it can't require buying a whole new set of
  hardware or dramatically restructuring existing research practices.
\item
  They need to be \textbf{accessible and extensible,} with many
  different points of control with different expectations of expertise
  and commitment to the framework. It needs to be useful for someone who
  doesn't want to learn it to its depths, but also have a comprehensible
  codebase at multiple scales so that reasearchers can \textbf{easily
  extend} it when needed.
\item
  They need to be designed to support \textbf{reproducibility and
  provenance,} which is a significant challenge given the heterogeneity
  inherent in the system. On one hand, being able to produce \emph{data
  that is clean at the time of acquisition} simplifies automated
  provenance, but enabling experimental replication requires multiple
  layers of abstraction to keep the idiosyncracies of an experiment
  separable from its implementation: it shouldn't require building
  \emph{exactly} the same apparatus with \emph{exactly} the same parts
  connected in \emph{exactly} the same way to replicate an experiment.
\item
  Ideally, they need to support \textbf{cumulative labor and knowledge
  organization,} so an additional concern with designing abstractions
  between system components is allowing work to be made portable and
  combinable with others. The barriers to contribution should be
  extremely minimal, not requiring someone to be a professional
  programmer to make a pull request to a central library, and
  contributions should come in many modes --- code is not the only form
  of knowing and it's far from the only thing needed to perform an
  experiment.
\end{itemize}

Here, as in the domains of data and analysis, the temptation to be
universalizing is strong, and the parts of the problem that are
emphasized influence the tools that are produced. A common design tactic
for experimental tools is to design them as state machines, a system of
states and transitions not unlike the analysis DAGs above. One such
nascent project is
\href{https://archive.org/details/beadl-xml-documentation-v-0.1/mode/2up}{BEADL}
\citep{wulfBEADLXMLDocumentation2020}  from a Neurodata Without
Borders
\href{https://archive.org/details/nwb-behavioral-task-wg}{working
group}. BEADL is an XML-based markup for standardizing a behavioral task
as an abstraction of finite state machines called
\href{https://statecharts.github.io/}{statecharts}. Experiments are
fully abstract from their hardware implementation, and can be formally
validated in simulations. The working group also describes creating a
standardized ontology and metadata schema for declaring all the many
variable parameters for experiments, like reward sizes, stimuli, and
responses \citep{nwbbehavioraltaskwgNWBBehavioralTask2020} . This
group, largely composed of members from the Neurodata Without Borders
team, understandably emphasize systematic description and uniform
metadata as a primary design principle.

Personally, I \emph{like} statecharts. The problem is that it's not
necessarily natural to express things as statecharts as you would want
to, or in the way that your existing, long-developed local experimental
code does. There are only a few syntactical features needed to
understand the following statechart: blocks are states, they can be
inside each other. Arrows move between blocks depending on some
condition. Entering and exiting blocks can make things happen. Short
little arrows from filled spots are where you start in a block, and when
you get to the end of the chart you go back to the first one. See the
following example of a statechart for controlling a light, described in
the \href{https://statecharts.dev/on-off-statechart.html}{introductory
documentation} and summarized in the figure caption:

\includegraphics[width=\linewidth]{../assets/images/on-off-delayed-exit-1.png}
\emph{``When you flick a lightswitch, wait 0.5 seconds before turning
the light on, then once it's on wait 0.5 seconds before being able to
turn it back off again. When you flick it off, wait 2 seconds before you
can turn it on again.}

They have an extensive set of documents that defend the consistency and
readability of statecharts on their
\href{https://statecharts.dev/}{homepage}, and my point here is not to
disagree with them. My point is instead that tools that aspire to the
status of generalized infrastructure can't ask people to dramatically
change the way they think about and do science. There are many possible
realizations of this task, and each is more or less natural to every
person.

The problem here is really one of emphasis, BEADL seeks to solve
problems with inconsistencies in terminology by standardizing them, and
in order to do that seeks to standardize the syntax for specifying
experiments.

This means of standardization has many attractive qualities and is being
led by very capable researchers, but I think the project is illustrative
of how the differing constraints of different systems and differing
goals of different approaches influence the possible space of tooling.
Analysis tasks are often asynchronous, where the precise timing of each
node's completion is less important than the path dependencies between
different nodes be clearly specified. Analysis tasks often have a
clearly defined set of start, end, and intermediate cache points, rather
than branching or cyclical decision paths that change over multiple
timescales. Statecharts are a hierarchical abstraction of finite state
machines, the primary advantage of which is that they are better able to
incorporate continuous and history-dependent behavior, which causes
state explosion in traditional finite-state machines.

\href{https://docs.auto-pi-lot.com}{Autopilot} \citep{saundersAutopilotAutomatingBehavioral2019}  approaches the problem
differently by avoiding standardizing \emph{experiments} themselves,
instead providing smaller building blocks of experimental tools like
hardware drivers, data transformations, etc. and emphasizing
understanding their use in \emph{context.} This approach sacrifices some
of the qualities of a standardized system like being a logically
complete or having guaranteed interoperability of terms in order to
better support integrating with existing work patterns and making work
cumulative. It is a bit more humble: because we can't possibly predict
the needs and limitations of a totalizing system, we split the problem
along the different domains of tools and give facility for describing
how they are used together.

For concrete example, we might imagine the lightswitch in an
autopilot-like framework like this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ autopilot.hardware.gpio }\ImportTok{import}\NormalTok{ Digital\_Out}
\ImportTok{from}\NormalTok{ time }\ImportTok{import}\NormalTok{ sleep}
\ImportTok{from}\NormalTok{ threading }\ImportTok{import}\NormalTok{ Lock}

\KeywordTok{class}\NormalTok{ Lightswitch(Digital\_Out):}
  \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{,}
\NormalTok{    off\_debounce: }\BuiltInTok{float} \OperatorTok{=} \DecValTok{2}\NormalTok{,}
\NormalTok{    on\_delay:     }\BuiltInTok{float} \OperatorTok{=} \FloatTok{0.5}\NormalTok{,}
\NormalTok{    on\_debounce:  }\BuiltInTok{float} \OperatorTok{=} \FloatTok{0.5}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{      off\_debounce (float): }
\CommentTok{        Time (s) before light can be turned back on}
\CommentTok{      on\_delay (float): }
\CommentTok{        Time (s) before light is turned on}
\CommentTok{      on\_debounce (float): }
\CommentTok{        Time (s) after turning on that light can\textquotesingle{}t be turned off}
\CommentTok{    """}
    \VariableTok{self}\NormalTok{.off\_debounce }\OperatorTok{=}\NormalTok{ off\_debounce}
    \VariableTok{self}\NormalTok{.on\_delay     }\OperatorTok{=}\NormalTok{ on\_delay}
    \VariableTok{self}\NormalTok{.on\_debounce  }\OperatorTok{=}\NormalTok{ on\_debounce}

    \VariableTok{self}\NormalTok{.on }\OperatorTok{=} \VariableTok{False}
    \VariableTok{self}\NormalTok{.lock }\OperatorTok{=}\NormalTok{ Lock()}

  \KeywordTok{def}\NormalTok{ switch(}\VariableTok{self}\NormalTok{):}
    \CommentTok{\# use a lock to make sure if}
    \CommentTok{\# called while waiting, we ignore it}
    \ControlFlowTok{if} \KeywordTok{not} \VariableTok{self}\NormalTok{.lock.acquire():}
      \ControlFlowTok{return}

    \CommentTok{\# if already on, switch off}
    \ControlFlowTok{if} \VariableTok{self}\NormalTok{.on: }
      \VariableTok{self}\NormalTok{.on }\OperatorTok{=} \VariableTok{False}
\NormalTok{      sleep(}\VariableTok{self}\NormalTok{.off\_debounce)}

    \CommentTok{\# otherwise switch on}
    \ControlFlowTok{else}\NormalTok{: }
\NormalTok{      sleep(}\VariableTok{self}\NormalTok{.on\_delay)}
      \VariableTok{self}\NormalTok{.on }\OperatorTok{=} \VariableTok{True}
\NormalTok{      sleep(}\VariableTok{self}\NormalTok{.on\_debounce)}

    \VariableTok{self}\NormalTok{.lock.release()}
\end{Highlighting}
\end{Shaded}

The terms \texttt{off\_debounce}, \texttt{on\_delay}, and
\texttt{on\_debounce} are certainly not part of a controlled ontology,
but we have described how they are used in the docstring and how they
are used is inspectable in the class itself.

The difficulty of a controlled ontology for experimental frameworks is
perhaps better illustrated by considering a full experiment. In
Autopilot, a full experiment can be parameterized by the \texttt{.json}
files that define the task itself and the system-specific configuration
of the hardware. An
\href{https://gist.github.com/sneakers-the-rat/eebe675326a157df49f66f62c4e33a6e}{example
task} from our lab consists of 7 behavioral shaping stages of increating
difficulty that introduce the animal to different features of a fairly
typical auditory categorization task. Each stage includes the parameters
for at most 12 different stimuli per stage, probabilities for presenting
lasers, bias correction, reinforcement, criteria for advancing to the
next stage, etc. So just for one relatively straightforward experiment,
in one lab, in one subdiscipline, there are \textbf{268 parameters} --
excluding all the default parameters encoded in the software.

The way Autopilot handles various parameters are part of set of layers
of abstraction that separate idiosyncratic logic from the generic form
of a particular \texttt{Task} or \texttt{Hardware} class. The general
structure of a two-alternative forced choice task is shared across a
number of experiments, but they may have different stimuli, different
hardware, and so on. Autopilot \texttt{Task}s use abstract references to
classes of hardware components that are required to run them, but
separates their implementation as a system-specific configuration so
that it's not necessary to have \emph{exactly the same} components
plugged into \emph{exactly the same} GPIO pins, etc. Task parameters
like stimuli, reward timings, etc. are similarly split into a separate
task parameterization that both allow \texttt{Task}s to be generic and
make provenance and experimental history easier to track. \texttt{Task}
classes can be subclasses to add or modify logic while being able to
reuse much of the structure and maintain the link between the root task
and its derivatives --- for example
\href{https://github.com/auto-pi-lot/autopilot-plugin-wehrlab/blob/9cfffcf5fe1886d25658d4f1f0c0ffe41c18e2cc/gap/nafc_gap.py\#L13-L49}{one
task we use} that starts a continuous background sound but otherwise is
the same as the root \texttt{Nafc} class. The result of these points of
abstraction is to allow exact experimental replication on inexactly
replicated experimental apparatuses.

In contrast, workflows in Bonsai \citep{lopesBonsaiEventbasedFramework2015a, lopesNewOpenSourceTools2021} ,
another very popular and very capable experimental tool,
\href{https://github.com/bonsai-rx/bonsai-examples/blob/cbc2c1decc11e1dc1df920421ef88a16fd2e184c/RoiTrigger/RoiTrigger.bonsai}{combine
the pattern of nodes} that constitute an experiment with idiosyncratic
parameters like a
\href{https://github.com/bonsai-rx/bonsai-examples/blob/cbc2c1decc11e1dc1df920421ef88a16fd2e184c/RoiTrigger/RoiTrigger.bonsai\#L76-L85}{crop
bounding box}. To be clear, I love Bonsai, and this kind of workflow
reproducibility is a huge step up from the more common practice of
totally lab-specific code. The flat design of Bonsai is extremely useful
for prototyping and extends through to complex experiments, but would
have a hard time being able to support generalizable and reusable
software classes for basic experimental operations, as well as creation
and negotiation over experimental terminology.

We can imagine extending the abstract specification of experimental
parameters, hardware requirements, and so on to work with our federated
naming system to overcome the challenges to standardizing. First, we can
make explicit declarations about the relationship between our
potentially very local vocabulary and other vocabularies at varying
degrees of generality. Here we can declare our \texttt{Lightswitch}
object and 1) link its \texttt{on\_delay} to our friend
\texttt{@rumbly}'s object that implements the same thing as
\texttt{on\_latency}, and 2) link it to a standardized \texttt{Latency}
term from
\href{https://scicrunch.org/scicrunch/interlex/view/ilx_0106040\#annotations}{interlex},
but since that term is for time elapsed between a stimulus and
behavioral response in a psychophysical context, it's only a partial
match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#Lightswitch\textgreater{}}
\NormalTok{  a @autopilot.hardware.Digital\_Out}

\NormalTok{  param on\_delay}
\NormalTok{    @skos:exactMatch @rumbly:LED:on\_latency}
\NormalTok{    @skos:nearMatch @interlex:Latency}

\NormalTok{  providedBy}
\NormalTok{    @git:repository ...}
\NormalTok{    @python:class ...}
\end{Highlighting}
\end{Shaded}

Further, since our experimental frameworks are intended to handle off
the shelf parts as well as our potentially idiosyncratic lightbulb
class, we can link many implementations of a hardware controlling class
to the product itself. Take for example the
\href{https://docs.auto-pi-lot.com/en/latest/hardware/i2c.html\#autopilot.hardware.i2c.I2C_9DOF}{I2C\_9DOF}
class that controls a 9 degree of freedom motion sensor from
\href{https://www.sparkfun.com/products/13944}{Sparkfun} where we both
indicate the specific part itself as well as the generic \texttt{ic}
that it uses:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#I2C\_9DOF\textgreater{}}
\NormalTok{  @autopilot.controlsHardware}
\NormalTok{    @sparkfun:13944}
\NormalTok{    @ic:LSM9DS1}
\end{Highlighting}
\end{Shaded}

This hints at the first steps of a system that would make our technical
work more cumulative, as it is then easy to imagine being able to search
for all the different implementations for a given piece of hardware.
Since the \texttt{@sparkfun:13944} element can in turn specify
properties like being an inertial motion sensor, this kind of linking
becomes powerful very quickly to make bridges that allow similar work to
be discovered and redeployed quickly.

We can also extend our previous connection between a dataset and the
results of its analysis to also include the tools that were used to
collect it. Say we want to declare the
\href{https://gist.github.com/sneakers-the-rat/eebe675326a157df49f66f62c4e33a6e}{example
experiment} above, and then extend our
\texttt{\textless{}\#project-name\textgreater{}} project to reference
it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}\#example{-}experiment\textgreater{}}
\NormalTok{  a @autopilot:protocol}

\NormalTok{  level @autopilot:freeWater}
\NormalTok{    reward}
\NormalTok{      type @si:mL}
\NormalTok{      value 5}
\NormalTok{    graduation }
\NormalTok{      a @autopilot:graduation:ntrials}
\NormalTok{      n\_trials 200}

\NormalTok{  level @autopilot:Nafc}
\NormalTok{    stim}
\NormalTok{      @autopilot:stim:sound:Tone}
\NormalTok{        frequency 5000}
\NormalTok{        duration 100}

\NormalTok{  ...}

\NormalTok{  @autopilot:prefs}
\NormalTok{    @jonny:Lightswitch}
\NormalTok{      on\_delay 1}

\NormalTok{\textless{}\#project{-}name\textgreater{}}
\NormalTok{  a @jonny:project{-}name}
\NormalTok{  collectedBy @jonny:example{-}experiment}
\end{Highlighting}
\end{Shaded}

So while we sacrifice the direct declaration of standardized terminology
and syntax, we gain the potential for a much denser and richer
expressive structure for our experiments. Instead of a single
authoritative dictionarylike meaning for a term, we instead appreciate
it in the context of its use, linked to the code that implements it as
well as the data it produces and the kinds of arguments that are made
with different analysis chains. Of course there is no intrinsic conflict
with this kind of freewheeling system and controlled vocabularies and
syntaxes: in this system, they can be one of many means of expression
rather than need to be singular sources of truth that depend on wide
adoption. While individual instances of uncontrolled vocabularies might
mean chaos, when they are integrated in a system of practice we get
something much wilder but also more intricate, beautiful, and useful.

As in the case of analytical tools, the role of the experimental
frameworks is also to make interacting with the rest of the system
easier and doesn't involve manually editing a lot of metadata. For
example, currently autopilot \texttt{Task}s ask users to declare
collected data as a \href{https://www.pytables.org/}{pytables} \citep{altedPyTablesProcessingAnalyzing2003}  datatypes like
\texttt{target\ =\ tables.StringCol(1)} to record whether a target is
\texttt{\textquotesingle{}L\textquotesingle{}} or
\texttt{\textquotesingle{}R\textquotesingle{}}. If instead it was
capable of specifying a Neurodata Without Borders data type like
\texttt{target\ =\ \textquotesingle{}@nwb:behavior:BehavioralEvents\textquotesingle{}},
then it would be possible to directly output to a standardized format,
potentially also automatically creating a
\href{https://pynwb.readthedocs.io/en/stable/pynwb.behavior.html\#pynwb.behavior.BehavioralEpochs}{\texttt{BehavioralEpochs}}
container or other data that are implied but otherwise have to be
explicitly created. Autopilot already automatically tracks the entire
behavioral history of an experimental subject, so we can also imagine it
being able to automatically create a \texttt{@analysis:project} object
described above that groups together multiple datasets that connected
them to an analysis pathway. So in this example the elusive workflow
where experimental data is automatically scooped up and incrementally
analyzed that is typically a hard-won engineering battle within a single
lab would become the normal mode of using the system.

The experimental framework described so far could solve some of the
software challenges of doing experiments by providing a system for
extending a set of reusable classes that can be combined into
experiments and linked together, but we haven't described anything to
address the rest of the contextual knowledge of practical scientific
work. We also haven't described any sort of governance or development
system that makes these packages anything more than ``some repository on
GitHub somewhere'' with all the propensity to calcify into fiefdoms that
those entail. This leads us back to a system of communication, the
central piece of missingness that we have been circling around the whole
piece. If you'll allow me one more delay, I want to summarize the system
so far before finally arriving there.






\subsection{Abstraction \& Interfaces}


Though there are many similarities between the three domains of data,
analytical, and experimental tools, the different constraints each
impose on a generalizable framework for integration and interoperability
are instructive. Each requires a careful consideration of the
\emph{layers of abstraction} needed to maintain the modularity of the
system --- this is an elemental feature of any protocol design. What are
the minimal affordances needed to implement a wide array of systems and
technologies within each domain? By being careful with specifying
abstraction, when considered together, the linked system described so
far represents a powerful step towards \emph{collectivizing the
scientific state of the art.}

There are three primary layers of abstraction in the analysis system
described: the interface between the metadata description of a node and
the code that implements it, the separation of individual nodes and a
notion of a combined workflow, and perhaps more subtly the separation of
the data applied to the workflow and the workflow itself.

\begin{itemize}
\item
  First, the markup description of the node gives us abstraction from
  programming language and implementation. This lets us do stuff like
  use multiple tools with competing environmental needs, adapt to
  multiple versions of the code markup as it develops, etc. Note the
  interaction with the rest of the metadata system: because we required
  a particular type of data file, and that link should provide us some
  means of opening/instantiating the file with dependencies, we didn't
  need to write loading code. Since it's in a linked system, someone
  could override the implementation of my node -- say someone comes up
  with a faster means of binning, then they just inherit from my node
  and replace the reference to the code. Boom we have cumulative and
  linked development.
\item
  The separation of the node from the workflow means that the node can
  be shared and swapped and reintegrated easily, dramatically reducing
  the brittleness of the systme. Since there is no restriction on what
  constitutes a node, though, there's no reason that nodes can't be
  either made massive, like putting a whole library in the process
  method, or be packaged up together. If we made the argument and method
  names recursive between the workflow and the node objects then tooling
  could automatically traverse multiple layers of node/workflow
  combinations at different levels of abstraction. This being a
  schematic description means that there can be multiple ``workflow
  runner'' packages that eg. distribute the task across a billion
  supercomputers or not.
\item
  Finally, the separation between the data applied and the workflow
  itself is very powerful indeed given our linked and namespaced system. My
  workflow effectively constitutes ``an unit of analysis.'' I have
  linked my data to this unit of analysis. Play out the permutations:

  \begin{itemize}
  
  \item
    I can see all the analyses that this particular pipeline has been
    applied to. Since it is embedded within the same federated system as
    our schema system, I can draw and connect semantic links to similar
    analysis pipelines as well as pipeline/data combinations.
  \item
    I can see all the different analyses that have been applied to my
    data: if my data is analyzed a zillion different times, in a zillion
    different combinations of data, I effectively get a ``multiverse
    analysis'' and we get to measure robustness of my data
    for free. 
  \item
    Being able to look across the landscape\ldots{} we start being able
    to actually really make cumulative progress on best practices. A
    common admonishment in cryptographically-adjacent communities is to
    ``never roll your own crypto,'' because your homebrew crypto library
    will never be more secure than reference implementations that have
    an entire profession of people trying to expose and patch their
    weaknesses. Bugs in analysis code that produce inaccurate results
    are inevitable and rampant \citep{millerScientistNightmareSoftware2006, soergelRampantSoftwareErrors2015, eklundClusterFailureWhy2016a, bhandarineupaneCharacterizationLeptazolinesPolar2019} , but
    impossible to diagnose when every paper writes its own pipeline. A
    common analysis framework would be a single point of inspection for
    bugs, and facilitate re-analysis and re-evaluation of affected
    results after a patch.
  \end{itemize}
\end{itemize}

This is all extraordinarily reproducible because even though I have
my portable markup description of the analysis, I can just refer to it
by name in my paper. 

Since we have a bunch of p2p systems all hooked up with
constantly-running daemons, to compete with the compute side of cloud
technology we also should implement a voluntary compute grid akin to
\href{https://foldingathome.org/}{Folding@Home}.

The category distinction between experimental and analytical tools is,
of course, a convenient ordering fiction for the purpose of this piece.
Autopilot is designed to make it easy to integrate other tools, and \citep{kaneRealtimeLowlatencyClosedloop2020} 

These are examples of what happens when you relax the demanding parts
of an exact ontology/knowledge graph -- we don't guarantee computability
across the graph itself, there's no way to automatically whiz
uncritically across all datasets in the system, but as we have seen
that's also not really true of the other systems either, to the degree
that it's desirable at all. Instead of having formal guarantees on the
graph, we can design tools that automate certain parts of the
interaction with the system to actually make our jobs easier. By being
very permissive, we let the desire paths of tool use form. This is a
very literal example of the `empower people, not systems' principle.

Reciprocally, we can also imagine the reverse: being able to develop
metadata structures that are then code generators for tools that have a
sufficiently sophisticated API -- for example remember how we said
Bonsai might have a hard time making generalizable behavioral tasks/etc?
Imagine if someone made a code compilation tool that allowed people to
declare abstract structures that could then be reusably reparameterzied
that autocreated a bonsai workflow? In the same way that the metadata
system can be used for \emph{storage} of existing work, it can also be
used to create abbreviate and abstract constructs for \emph{use} with
other tools.

To take stock:

We have described a system of three component modalities: \textbf{data,
analytical tools, and experimental tools} connected by a \textbf{linked
data} layer. We started by describing the need for a
\textbf{peer-to-peer} data system that makes use of \textbf{data
standards} as an onramp to linked metadata. To interact with the system,
we described an identity-based linked data system that lets individual
people declare linked data resources and properties that link to
\textbf{content addressed} resources in the p2p system, as well as
\textbf{federate} into multiple larger organizations. We described the
requirements for \textbf{DAG-based analytical frameworks} that allow
people to declare individual nodes for a processing chain linked to
code, combine them into workflows, and apply them to data. Finally, we
described a design strategy for \textbf{component-based experimental
frameworks} that lets people specify experimental metadata, tools, and
output data.

This system as described is a two-layer system, with a few different
domains linked by a flexible metadata linking layer. The metadata system
as described is not merely \emph{inert} metadata, but metadata linked to
code that can \emph{do something} --- eg. specify access permissions,
translate between data formats, execute analayis workflows, parameterize
experiments, etc. Put another way, we have been attempting to describe a
system that \emph{embeds the act of sharing and curation in the practice
of science.} Rather than a thankless post-hoc process, the system
attempts to provide a means for aligning the daily work of scientists so
that it can be cumulative and collaborative. To do this, we have tried
to avoid rigid specifications of system structure, and instead described
a system that allows researchers to pluralistically define the structure
themselves.


\section{Shared Knowledge}



 The remaining set of problems implied by the
infrastructural system sketched in the preceding sections is the
\emph{communication} and \emph{organization} systems that make up the
interfaces to maintain and use it. We can finally return to some of the
breadcrumbs laid before: the need for negotiating over distributed and
conflicting data schema, for incentivizing and organizing collective
labor, and ultimately for communicating scientific results.

The communication systems that are needed double as \emph{knowledge
organization} systems. Knowledge organization has the rosy hue of
something that might be uncontroversial and apolitical --- surely
everyone involved in scientific communication wants knowledge to be
organized, right? The reality of scientific practice might give a hint
at our naivete. Despite being, in some sense, itself an effort to
organize knowledge, \emph{scientific results effectively have no system
of explicit organization.} There is no means of, say, ``finding all the
papers about a research question.'' The problem is so fundamental it
seems natural: the usual methods of using search engines, asking around
on Twitter, and chasing citation trees are flex tape slapped over the
central absence of a system for formally relating our work as a shared
body of knowledge.

Information capitalism, in its terrifying splendor, here too pits
private profit against public good. Analogously to the necessary
functional limitations of SaaS platforms, artificially limiting
knowledge organization opens space for new products and profit
opportunities. In their 2020 shareholder report, RELX, the parent of
Elsevier, lists increasing the number of journals and papers as a
primary means of increasing revenue \citep{RELXAnnualReport2020} .
This represents a shift in their business model from subscriptions to
deals like open access, which according to RELX CEO Erik Nils EngstrÃ¶m
``is where revenue is priced per article on a more explicit basis'' \citep{relx2020ResultsPresentation2021} .

In the next breath, they describe how ``in databases \& tools and
electronic reference, representing over a third of divisional\footnote{RELX
  is a huge information conglomerate, and scientific publication is just
  one division.} revenue, we continued to drive good growth through
content development and enhanced machine learning {[}ML{]} and natural
language processing {[}NLP{]} based functionality.''

What ML and NLP systems are they referring to? The 2019 report is a bit
more revealing (emphases mine):

\begin{quote}
Elsevier looks to enhance quality by building on its premium brands and
\textbf{grow article volume} through \textbf{new journal launches,} the
expansion of open access journals and growth from emerging markets; and
add value to core platforms by implementing capabilities such as
\textbf{advanced recommendations on ScienceDirect and social
collaboration through reference manager and collaboration tool
Mendeley.}

\textbf{In every market, Elsevier is applying advanced ML and NLP
techniques} to help researchers, engineers and clinicians perform their
work better. For example, in research, ScienceDirect Topics, a free
layer of content that enhances the user experience, uses \textbf{ML and
NLP techniques to classify scientific content and organise it
thematically,} enabling users to get faster access to relevant results
and related scientific topics. The feature, launched in 2017, is proving
popular, generating 15\% of monthly unique visitors to ScienceDirect via
a topic page. \textbf{Elsevier also applies advanced ML techniques that
detect trending topics per domain,} helping researchers make more
informed decisions about their research. \textbf{Coupled with the
automated profiling and extraction of funding body information from
scientific articles,} this process supports the whole researcher
journey; from planning, to execution and funding. \citep{RELXAnnualReport2019} 
\end{quote}

Reading between the lines, it's clear that the difficulty of finding
research is a feature, not a bug of their system. Their explicit
business model is to increase the number of publications and sell
organization back to us with recommendation services. The recommendation
system might be free\footnote{``free''}, but the business is to develop
dependence to sell ad placement --- which they proudly describe as
looking very similar to their research content \citep{springernatureBrandedContent, elsevier360AdvertisingSolutions} .

It gets more sinister: Elsevier sells multiple products to recommend
`trending' research areas likely to win grants, rank scientists, etc.,
algorithmically filling a need created by knowledge disorganization. The
branding varies by audience, but the products are the same. For
pharmaceutical companies
\href{https://www.elsevier.com/solutions/professional-services/drug-design-optimization\#opportunity}{``scientific
opportunity analysis''} promises custom reports that answer questions
like ``Which targets are currently being studied?'' ``Which experts are
not collaborating with a competitor?'' and ``How much funding is
dedicated to a particular area of research, and how much progress has
been made?'' \citep{elsevierDrugDesignOptimization} . For
academics,
\href{https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science\#how}{``Topic
Prominence in Science''} offers university administrators tools to
``enrich strategic research planning with portfolio overviews of their
own and peer institutions.'' Researchers get tools to ``identify experts
and potential cross-sector collaborators in specific Topics to
strengthen their project teams and funding bids and identify Topics
which are likely to be well funded.'' \citep{elsevierTopicProminenceScienceb} 

These tools are, of course, designed for a race to the bottom --- if my
colleague is getting an algorithmic leg up, how can I afford not to?
Naturally only those labs that \emph{can} afford them and the costs of
rapidly pivoting research topics will benefit from them, making yet
another mechanism that reentrenches scientific inequity for profit.
Knowledge disorganization, coupled with a little surveillance capitalism
that monitors the activity of colleagues and rivals \citep{brembsReplacingAcademicJournals2021} , has given publishers powerful
control over the course of science, and they are more than happy to ride
algorithmically amplified scientific hype cycles in fragmented research
bubbles all the way to the bank.

One more turn of the screw: limiting the system of scientific
organization to only citations allows publishers to effectively invent
the metrics that operationalize ``prestige'' and derivative products
that cement their position with researchers, governments, and funding
agencies. The promise of placement in a high prestige journal is high
citation count, and highly citated research explitly or implicitly is
seen as prestigious. With semantic structure to locate papers, it
becomes much more difficult to sell high citation count as a product ---
people can find what they need, rather than needing to pay attention to
a few high-profile journals.

Scientists aren't the only customers of citation prestige: in 2020 the
National Research Foundation of Korea (NRF) and Elsevier published a
joint report that used a measurement derived from citation counts -
``Field-weighted citation impact'', or FWCI - to argue for the
underrated research prestige of South Korea \citep{researchfoundationofkoreaSouthKoreaTechnological2020} . While I don't
dispute the value of South Korea's research program, the apparent
bargain that was struck is chilling. South Korea gets a very fancy
report arguing that more scientists in other countries should work with
theirs, and Elsevier gets to cement itself into the basic operation of
science. Elsevier controls the journals that can guarantee high citation
counts \emph{and} the metrics built on top of them. The Brain Korea
program Phase II report \footnote{the result of another corporate
  collaboration with the Rand corporation.} \citep{seongBrainKorea212008} , issued just before the 2009 formation of the
NRF argued that rankings and funding should be dependent on citation
counts. The NRF now relies on SciVal and their FWCI measurement as a
primary means of ranking researchers and determining funding, built into
the Brain Korea 21 funding system \citep{elsevierCaseStudyNational2019, elsevierkoreaSciValHwalyongeulWihan2021} . Without exaggeration, scientific disorganization and reliance on
citation counts allowed Elsevier to buy control over the course of
research in South Korea.

The consequences for science are hard to overstate. In addition to
literature search being an unnecessarily huge sink of time and labor,
science operates as a wash of tail-chasing results that only rarely seem
to cumulatively build on one another. The need to constantly reinforce
the norm that purposeful failure to cite prior work is research
misconduct is itself a symptom of how engaging with a larger body of
work is both extremely labor intensive and \emph{strictly optional} in
the communication regime of journal publication. The combination of more
publications translating into more profit and the strategic
disorganization of science contributes to conditions for scientific
fraud. An entirely fraudulent paper with nonsensical but carefully
crafted methods and results can be undetectable outside of domain
experts. Since papers can effectively be islands, given legitimacy by
placement in a journal strongly incentivized to accept all comers, and
there is no good means of evaluating them in context with their
immediate semantic neighbors, investigating fraud is extremely time
consuming and almost entirely without reward. And since traditional peer
review happens once, rather than as a continual public process, the only
recourse outside of posting on PubPeer is to wait on journal editorial
boards to review each individual complaint. Forensic peer-reviewers have
been ringing the alarm bell, saying that there is ``no net'' to bad
research \citep{heathersRealScandalIvermectin2021} , and brave and
highly-skilled investigators like
\href{https://scienceintegritydigest.com/}{Elisabeth Bik} have found
thousands of papers with evidence of purposeful manipulation \citep{shenMeetThisSuperspotter2020, bikPrevalenceInappropriateImage2016} .
The economic structure of for-profit journals pits their profit model
against their function as providing a venue for peer review --- the one
function most scientists are still sympathetic to. Despite the profusion
of papers, by some measures progress in science has slowed to a crawl
\citep{chuSlowedCanonicalProgress2021} .

While Chu and Evans correctly diagnose \emph{symptoms} of knowledge
disorganization like the need to ``resort to heuristics to make
continued sense of the field'' and reliance on canonical papers, by
treating the journal model as a natural phenomenon and citation as the
only means of ordering research, they misattribute root \emph{causes.}
The problem is not people publishing \emph{too many papers,} or a
\emph{breakdown of traditional publication hierarchies,} but the
\emph{profitability of knowledge disorganization.} Their prescription
for ``a clearer hierarchy of journals'' misses the role of organizing
scientific work in journals ranked by prestige, rather than by the
content of the work, as a potentially major driver of extremely skewed
citation distributions. It also misses the publisher's stated goals of,
well \emph{publishing more papers,} and pushing algorithmic paper
recommendations, as there is nothing recommendation algorithms love
recommending more than things that are alreaady popular. Without
diagnosing knowledge disorganization as a core part of the business
model of scientific publishers, we can be led to prescriptions that
would make the problem worse.

It's hard to imagine an alternative to journals that doesn't look like,
well, journals. While a full treatment of the journal system is outside
the scope of this paper, the system we describe here renders them
\emph{effectively irrelevant} by making papers as we know them
\emph{unnecessary.} Rather than facing the massive collective action
problem of asking everyone to change their publication practices head
on, by reconsidering the way we organize the surrounding infrastructure
of science we can flank journals and replace them ``from below'' with
something qualitatively more useful.

Beyond journals, the other technologies of communication that have been
adopted out of need, though not necessarily design, serve as
\href{https://en.wikipedia.org/wiki/Desire_path}{desire paths} that
trace other needs for scientific communication. As a rough sample:
Researchers often prepare their manuscripts using platforms like Google
Drive, indicating a need for collaborative tools in perparation of an
idea. When working in teams, we often use tools like Slack to plan our
work. Scientific conferences reflect the need for federated
communication within subdisciplines, and we have adopted Twitter as a de
facto platform for socializing and sharing our work to a broader
audience. We use a handful of blogs and other sites like
\href{https://edspace.american.edu/openbehavior/}{OpenBehavior} \citep{whiteFutureOpenOpenSource2019} ,
\href{https://open-neuroscience.com/}{Open Neuroscience}, and many
others to index technical knowledge and tools. Finally we use sites like
\href{https://pubpeer.com}{PubPeer} and ResearchGate for comment and
criticism.

These technologies point to a few overlapping and not altogether binary
axes of communication systems.

\begin{itemize}

\item
  \textbf{Durable vs Ephemeral} - journals seek to represent information
  as permanent, archival-grade material, but scientific communication
  also necessarily exists as contextual, temporally specific snapshots.
\item
  \textbf{Structured vs Chronological} - scientific communication both
  needs to present itself as a structured basis of information with
  formal semantic linking, but also needs the chronological structure
  that ties ideas to their context. This axis is a gradient from formal
  ontologies, through intermediate systems like forums with hierarchical
  topic structure that embeds a feed, to the purely chronological
  feed-based social media systems.
\item
  \textbf{Messaging vs Indexing} - Communication can be person-to-person
  or person-to-group messaging with defined senders and recipients, or
  intended as a generalizable category of objects. This ranges from
  entirely-specific DMs through domain-specific tool indexes like
  OpenBehavior through the uniform indexing of Wikipedia.
\item
  \textbf{Public vs.~Private} - Who gets to read, who gets to
  contribute? Communication can be composed of entirely private notes to
  self, through communication in a lab, collaboration group, discipline,
  and landing in the entirely public realm of global communication.
\item
  \textbf{Formal vs.~Informal} - Journal articles and encyclopedia-bound
  writing that conforms to a particular modality of expression vs.~a
  vernacular style intended to communicate with people outside the
  jargon culture.
\item
  \textbf{Push vs.~Pull} - Do you go to get information from a reference
  location, or does information come to you as an alert or message?
\end{itemize}

Clearly a variety of different types of communication tools are needed,
but there is no reason that each of them should be isolated and
inoperable with the others. We have already seen several of the ideas
that help bring an alternative into focus. Piracy communities
demonstrate ways to build social systems that can sustain
infrastructure. Federated and protocol-based systems show us that we
don't need to choose between a single monolithic system or many
disconnected ones, but can have a heterogeneous space of tools linked by
a basic protocol. The semantic web and linked data people showed us the
power of triplet links as a very general means of linking disparate
systems. We can bridge these lessons with some from the early wiki
movement to get a more practical sense of what it takes to give people
total control over the structure of their communication and knowledge
systems. Together with our sketches of data, analytical, and
experimental tools we can start imagining a system for coordinating them
--- as well as displacing some of the more intractable systems that
misstructure the practice of science. 




\subsection{The Wiki Way}



 \textgreater{} If we take radical collaboration as our
core, then it becomes clear that extending Wikipedia's success doesn't
simply mean installing more copies of wiki software for different tasks.
It means figuring out the key principles that make radical collaboration
work. What kinds of projects is it good for? How do you get them
started? How do you keep them growing? What rules do you put in place?
What software do you use? \citep{swartzMakingMoreWikipedias2006} 

\begin{quote}
So that's it --- insecure but reliable, indiscriminate and subtle, user
hostile yet easy to use, slow but up to date, and full of difficult,
nit-picking people who exhibit a remarkable community camaraderie.
Confused? Any other online community would count each of these
``negatives'' as a terrible flaw, and the contradictions as impossible
to reconcile. Perhaps wiki works because the other online communities
don't. \citep{leufWikiWayQuick2001a, -l, 329}  and in
\href{http://wiki.c2.com/?WhyWikiWorks}{WhyWikiWorks}\footnote{Interestingly,
  this quote is almost, but not exactly the same as that on
  \href{http://wiki.c2.com/?WhyWikiWorks}{Ward's wiki}: \textgreater{}
  So that's it - insecure, indiscriminate, user-hostile, slow, full of
  difficult, nit-picking people, and frivolous. Any other online
  community would count each of these strengths as a terrible flaw.
  Perhaps wiki works because the other online communities do not.

  I can't tell if Ward Cunningham wrote the original entry in the wiki,
  but in any case seems to have found a bit of optimism in the book.}
\end{quote}

Aside from maybe the internet itself, there is no larger public digital
knowledge organization effort than Wikipedia. While there are many
lessons to be learned from Wikipedia itself, it emerged from a prior
base of thought and experimentation in radically permissive,
self-structuring read/write --- sometimes called ``peer production''
\citep{hillWikipediaEndOpen2019}  --- communities. Wikis are now
quasi-ubiquitous today\footnote{though their corporate manifestations
  would probably be unrecognizable to the project early wiki users
  imagined.}, largely thanks to Wikipedia, but its specific history and
intent to be an \emph{encyclopedia} entwines it with a very particular
technological and social system that obscures some of the broader dreams
of early wikis.

Aaron Swartz recounts a quote from Jimmy Wales, co-founder of Wikipedia:

\begin{quote}
``I'm not a wiki person who happened to go into encyclopedias,'' Wales
told the crowd at Oxford. ``I'm an encyclopedia person who happened to
use a wiki.'' \citep{swartzWhoWritesWikipedia2006} 
\end{quote}

And further describes how this origin and mission differentiates it from
other internet communities:

\begin{quote}
But Wikipedia isn't even a typical community. Usually Internet
communities are groups of people who come together to discuss something,
like cryptography or the writing of a technical specification. Perhaps
they meet in an IRC channel, a web forum, a newsgroup, or on a mailing
list, but the focus is always something ``out there'', something outside
the discussion itself.

But \textbf{with Wikipedia, the goal is building Wikipedia.} It's not a
community set up to make some other thing, it's a community set up to
make itself. And since Wikipedia was one of the first sites to do it, we
know hardly anything about building communities like that. \citep{swartzMakingMoreWikipedias2006} 
\end{quote}

We know a lot more now than in 2006, of course, but Wikipedia still has
outsized structuring influence on our beliefs about what Wikis can be.
Wikipedia has since spawned a
\href{https://meta.wikimedia.org/wiki/Complete_list_of_Wikimedia_projects}{huge
number} of technologies and projects like
\href{https://meta.wikimedia.org/wiki/MediaWiki}{MediaWiki} and
\href{https://meta.wikimedia.org/wiki/Wikidata}{Wikidata}, each with
their own long and occasionally torrid histories. I won't dwell on the
obvious and massive feat of collective organzation that the greater
Wikipedia project represents --- learning from its imperfections is more
useful to us here, especially for things that aren't encyclopedias. The
dream of a centralized, but mass-edited ``encyclopedia of everything''
seems to be waning, and its slow retreat from wild openness has run
parallel to a long decline in contributors \citep{hillWikipediaEndOpen2019, halfakerRiseDeclineOpen2013} . Throughout
that time, there has been a separate (and largely skeptical) set of wiki
communities holding court on what radically open web communities can be
like, inventing their worlds in realtime. These thought communities have
histories that are continuous with one another, and in their mutual
reaction and inspiration sometimes teach similar lessons from across the
divides of their very different structure.

The first wiki was launched in 1995\footnote{it's complicated:
  http://wiki.c2.com/?WardsWikiTenthAnniversary}
(\href{http://wiki.c2.com/}{still up}) and came to be known as Ward's
wiki after its author
\href{http://wiki.c2.com/?WardCunningham}{WardCunningham}. Technically,
it was extremely simple: a handful of
\href{http://wiki.c2.com/?TextFormattingRules}{TextFormattingRules} and
use of \href{http://wiki.c2.com/?WikiCase}{WikiCase} where if you
\href{http://wiki.c2.com/?JoinCapitalizedWords}{JoinCapitalizedWords}
you create a link to a (potentially new)
\href{http://wiki.c2.com/?WikiPage}{WikiPage} --- and the ability for
anyone to edit any page. These very simple
\href{http://wiki.c2.com/?WikiDesignPrinciples}{WikiDesignPrinciples}
led to a sprawling and continuous conversation that spanned more than a
\href{http://wiki.c2.com/?WardsWikiTenthAnniversary}{decade} and
thousands\footnote{\href{http://c2.com/wiki/history/}{23,244} unique
  page names according to the edit history, but the edit history was
  also purposely pruned from time to time.} of pages that, because of
the nature of the medium, is left fully preserved in amber. Those
conversations are a history of thought on what makes wiki communities
work (eg. \href{http://wiki.c2.com/?WhyWikiWorks}{WhyWikiWorks},
\href{http://wiki.c2.com/?WhyWikiWorksNot}{WhyWikiWorksNot}), and what
is needed to sustain them.

One tension that emerged early and was never fully resolved by these
wikis is the balance between
``\href{http://wiki.c2.com/?DocumentMode}{DocumentMode}'' writing that
serves as linearly-readable reference material, similar to that of
Wikipedia, and ``\href{http://wiki.c2.com/?ThreadMode}{ThreadMode}''
writing that is a nonlinear representation of a conversation. Order vs
contemporaneousness is a fundamental challenge of inventing culture in
plaintext. The purpose of using a wiki as opposed to other technologies
that existed at the time like bulletin boards, newsgroups, IRC, etc. was
that it provided a means of structure\footnote{Giving a means of
  organizing the writing of the Portland Pattern Repository was the
  reason for creating Ward's Wiki in the first place.}. The parallel
need to communicate and attribute work made it a seeming inevitability
that even if you went out of your way to restructure a lot of writing
into a sensible DocumentMode page, someone would soon after create a new
horizontal divider and start a fresh ThreadMode section.

Ward Cunningham and other more organizationally-oriented contributors
opposed ThreadMode (eg.
\href{http://wiki.c2.com/?ThreadModeConsideredHarmful}{ThreadModeConsideredHarmful},
\href{http://wiki.c2.com/?InFavorOfDissertation}{InFavorOfDissertation})
for a number of reasons, largely due to the
\href{http://wiki.c2.com/?ThreadMess}{ThreadMess} and
\href{http://wiki.c2.com/?WikiChaos}{WikiChaos} it had the potential of
creating.

\begin{quote}
I occasionally suggest how this site should be used. My
\href{http://wiki.c2.com/?GoodStyle}{GoodStyle} suggestions have been
here since the beginning and are linked from the edit page should anyone
forget. I have done my best to discourage dialog
\href{http://wiki.c2.com/?InFavorOfDissertation}{InFavorOfDissertation}
which offers a better fit to this medium. I've been overruled. I will
continue to make small edits to pages for the sake of brevity. --
\href{http://wiki.c2.com/?WardCunningham}{WardCunningham} \citep{C2wikiWikiHistory} 
\end{quote}

Most pages are thus a combination of both, usually with some
DocumentMode text at the top with ThreadMode conversations interspersed
throughout without necessarily having any clean delineation between the
two. Far from just being raw disorder, this mixed mode of writing gave
it a peculiar character of being \emph{both} a folk reference for a
library of concepts \emph{as well as} a history of discussion that made
the contingency of that reference material plain. Beka Valentine put it
well:

\begin{quote}
c2wiki is an exercise in dialogical methods. of laying bare the fact
that knowledge and ideas are not some truth delivered from On High, but
rather a social process, a conversation, a dialectic, between various
views and interests \citep{valentineC2wikiExerciseDialogical2021} 
\end{quote}

This tension and its surrounding discussions point to the need for
multiple representations of a single idea: that both the social and
reference representations of a concept are valuable, but aren't
necessarily best served by being represented in the same place. There
was relatively common understanding that the intended order of things
was to have many ThreadMode conversations that would gradually be
converted to DocumentMode in a process of
\href{http://wiki.c2.com/?BrainStormFirstCleanLater}{BrainStormFirstCleanLater}.
Many \href{http://wiki.c2.com/?ConvertThreadModeToDocumentMode}{proposed
solutions} orbit around making parallel pages with similar names (like
\textless pagename\textgreater Discussion) to clean up a document while
preserving the threads (though there were plenty of interesting
alternatives, eg.
\href{http://wiki.c2.com/?DialecticMode}{DialecticMode})\footnote{Contemporary
  wikis have continued this conversation, see
  \href{https://communitywiki.org/wiki/DocumentsVsMessages}{DocumentsVsMessages}
  on communitywiki.org}.

Wikipedia cut the Gordian Knot by splitting each page into a separate
\emph{Article} and \emph{Talk} pages, with the talk page in its own
\textbf{Namespace} -- eg.
\href{https://en.wikipedia.org/wiki/Gordian_Knot}{Gordian\_Knot} vs
\href{https://en.wikipedia.org/wiki/Talk:Gordian_Knot}{Talk:Gordian\_Knot}.
Talk pages resemble a lot of the energy of early wikis: disorganized,
sometimes silly, sometimes angry, and usually charmingly pedantic.
Namespaces extend the traditional ``everything is a page'' notion
encoded in the WikiCase link system by giving different pages different
roles. In addition to having parallel conversations on articles and talk
pages, it is possible to have template pages that can be included on
wiki pages with
\texttt{\{\%\ raw\ \%\}\{\{double\ curly\ bracket\}\}\{\%\ endraw\ \%\}}
syntax -- eg.
\href{https://en.wikipedia.org/wiki/Template:Citation_needed}{Template:Citation\_Needed}
renders
\texttt{\{\%\ raw\ \%\}\{\{Citation\ needed\}\}\{\%\ endraw\ \%\}} as
{[}citation needed{]}. Talk pages have their own \textbf{functional
differentiation,} with features for threading and annotating discussions
that aren't present on the main article pages (see
\href{https://en.wikipedia.org/wiki/Wikipedia:Flow}{Wikipedia:Flow} \citep{WikipediaFlow2021} ).

!! from a starting point of all things being one thing, a la wikis, we
frame this as functional differentiation in a namespace. We can
reciprocally frame this as giving a common name to a variety of
different systems. Talk pages have slowly reinvented forums, but we can
also think about this problem as making forums and chatrooms and essays
that are different reflections of a wiki article page.

The complete segregation of discussion to Talk pages is driven by
Wikipedia's aspirations as an encyclopedia, with reminders that is the
``\href{https://en.wikipedia.org/wiki/Wikipedia:Don't_lose_the_thread\#Move_to_the_article_talk_page}{sole
purpose}'' peppered throughout the rules and guidelines. The presence of
messy subjective discussions would of course be discordant with the very
austere and ``neutral'' articles of an encyclopedia. There are no
visible indications that the talk pages even exist in the main text, and
so even deeply controversial topics have no references to the
conversations in talk pages that surround them --- despite this being a
requested feature by both administrators and editors \citep{schneiderUnderstandingImprovingWikipedia2011} .

Talk pages serve as one of the primary points of coordination and
conflict resolution on Wikipedia, and also provide a low-barrier
entrypoint for questions posed to a space they perceive to be ``an
approchable community of experts'' \citep{viegasTalkYouType2007} .
The separation of Talk pages and the
\href{https://en.wikipedia.org/wiki/Wikipedia:Talk_page_guidelines}{labyrinthine
rules} governing their use function to obscure the dialogical and
collective production of knowledge at the heart of wikis and Wikipedia.
The body of thought that structures Wikipedia, most of which is in its
\href{https://en.wikipedia.org/wiki/Wikipedia:Community_portal}{Wikipedia:*}
namespace, is immense and extremely valuable, but is largely hidden from
most people. Since Wikipedia is ``always already there'' often without
trace of its massively collective nature, relatively few people ever
contribute to it. Reciprocally, since acknowledging personal
contribution is or point of view is
\href{https://en.wikipedia.org/wiki/Wikipedia:No_original_research}{explicitly}
against some of its
\href{https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view}{core}
policies and
\href{https://en.wikipedia.org/wiki/Wikipedia:Avoid_thread_mode}{traditions},
there is little public credit outside the Wikipedia community itself for
the labor of maintaining it.

The forking of Wards Wikis into the first
\href{http://wiki.c2.com/?SisterSites}{SisterSites} teaches a parallel
strain of lessons. Ward's Wiki started as a means of organizing
knowledge for the Portland Pattern Repository\footnote{The initial
  motivations are actually stunningly close to the kinds of
  communication and knowledge organization problems we are still solving
  today (even in this piece) \textgreater{} Cunningham had developed a
  database to collect the contributions of the listserv members. He had
  noticed that the content of the listserv tended to get buried, and
  therefore the most recent post might be under-informed about posts
  which came before it. The way around this problem was to collect ideas
  in a database, and then edit those ideas rather than begin anew with
  each listserv posting. Cunningham's post states that ``The plan is to
  have interested parties write web pages about the People, Projects and
  Patterns that have changed the way they program. Short stories that
  hint at patterns are welcome too.'' As to the rhetorical expectations,
  Cunningham added ``The writing style is casual, like email or netnews,
  but doesn't have to be so repetitive since the things being discussed
  don't disappear. Think of it as a moderated list where anyone can be
  moderator and everything is archived. It's not quite a chat, still,
  conversation is possible.'' - \citep{cummingsWhatWasWikiWhy2009} }, a programming community (referred to as
\href{http://wiki.c2.com/?DesignPatterns}{DesignPatterns} below), and in
1998 they were overwhelmed with proponents of
\href{http://wiki.c2.com/?ExtremeProgramming}{ExtremeProgramming}, which
caused the first fissure in the wiki:

\begin{quote}
XP advocates seemed to be talking about XP at every possible opportunity
and seemingly on every page with content the least bit related to
software development. This annoyed a number people who were here to
discuss patterns, leading to the tag
\href{http://wiki.c2.com/?XpFreeZone}{XpFreeZone}, as a request not to
talk about ExtremeProgramming on that page.

It was difficult to pick out the
\href{http://wiki.c2.com/?DesignPatterns}{DesignPatterns} discussion on
\href{http://wiki.c2.com/?RecentChanges}{RecentChanges}\footnote{Recent
  Changes was the dominant, if not controversial means of keeping track
  with recent wiki traffic, see
  \href{http://wiki.c2.com/?RecentChangesJunkie}{RecentChangesJunkie}},
because most of the activity was related to ExtremeProgramming.
Eventually, most of the
\href{http://wiki.c2.com/?DesignPatterns}{DesignPatterns} people left,
to discuss patterns in a ``quieter'' environment, and people started
referring to this site as
\href{http://wiki.c2.com/?WardsWiki}{WardsWiki} instead of the
\href{http://wiki.c2.com/?PortlandPatternRepository}{PortlandPatternRepository}
\citep{C2wikiWikiHistory} 
\end{quote}

One of the first and most influential Sister Sites was
\href{http://meatballwiki.org/}{Meatball Wiki}, described on Wards Wiki:

\begin{quote}
SunirShah founded MeatballWiki to absorb and enlarge the discussion of
what wiki and wiki like sites might be. That discussion still simmers
here. But here it can take on a negative tone sounding more like
complaining. On meatball, under Sunir's careful leadership, the ideas,
wild or not, stay amazingly upbeat. -
\href{http://wiki.c2.com/?SisterSites}{SisterSites}
\end{quote}

MeatballWiki became the spiritual successor to Ward's Wiki, which at
that point had its own momentum of culture less interested in being the
repository of wiki thought\footnote{There seems to have been an
  overriding belief that theoretical ideas about wikis and wiki culture
  belong on Meatball Wiki, from
  \href{http://wiki.c2.com/?WikiWikiWebFaq}{WikiWikiWebFaq}:
  \textgreater{} Q: Do two separate wikis ever merge together to create
  one new wiki? Has this happened before? Keep in mind that I don't just
  mean two different pages within a wiki. (And for that matter, where is
  an appropriate page where I can post questions about the history of
  all wikis, not just this one?) \textgreater{} \textgreater{} A1: I
  don't know of any such wiki merge, nor of any discussion of the
  history of all wikis. Such a discussion should probably reside (if
  created) on MeatballWiki.}. Though there are a truly monstrous number
of ideas on MeatballWiki, the most relevant here here might be those
concerning its very existence as a SisterSite. These were a series of
discussions that melded thoughts from open source computing with social
systems; in part:
\href{http://meatballwiki.org/wiki/RightToFork}{RightToFork},
\href{http://meatballwiki.org/wiki/RightToLeave}{RightToLeave},
\href{http://meatballwiki.org/wiki/EnlargeSpace}{EnlargeSpace}, and
\href{http://meatballwiki.org/wiki/TransClusion}{TransClusion}.

What can be done when the internal divisions in a wiki community and the
weight of its history make healthy contribution impossible? The first
place to start is with the
\href{http://meatballwiki.org/wiki/RightToLeave}{RightToLeave}, where it
is always possible to just stop being part of a community. This approach
is clearly the most destructive, as it involves abandoning the emotional
bonds of a community, prior work (see the
\href{http://wiki.c2.com/?WikiMindWipe}{WikiMindWipe} where a user left
and took all their contributions with them), and doesn't necessarily
provide an alternative that alleviates the cause of the tension. The
next idea is to \emph{fork} the community, where the body of a community
--- in the case of wikis the pages and history --- can be duplicated so
that it can proceed along two parallel tracks. Exercising the right to
fork is, according to Meatball, ``people exercising their RightToLeave
whilst maintaining their emotional stake'' \citep{MeatballWikiRightToLeave} .

The discussion around the Right to Fork on Meatball is far from
uniformly positive, and is certainly colored by the strong presence of
its
\href{http://meatballwiki.org/wiki/BenevolentDictator}{BenevolentDictator}
Sunir Shah who viewed it as a last resort after all attempts at
\href{http://meatballwiki.org/wiki/ConflictResolution}{ConflictResolution}
have failed. They point to the potentially damaging effects of a fork,
like bitterness, disputes over content ownership (see
\href{http://meatballwiki.org/wiki/MeatballIsNotFree}{MeatballIsNotFree}),
and potentially an avoidance of conflict resolution that is a normal and
healthy part of any community. Others place it more in the realm of a
radical \emph{political} action rather than a strictly social action.
Writing about the fork of OpenOffice to LibreOffice, Terry Hancock
writes:

\begin{quote}
{[}In{]} proprietary software {[}a{]} political executive decision can
kill a project, regardless of developer or user interest. But with free
software, the power lies with the people who make it and use it, and the
freedom to fork is the guarantee of that power. {[}\ldots{]} The freedom
to fork a free software project is {[}a{]} ``tool of revolution''
intended to safeguard the real freedoms in free software. \citep{hancockOpenOfficeOrgDead2010} 
\end{quote}

Forking digital communities can be much less acrimonious than
physically-based communities because of the ability to
\href{http://meatballwiki.org/wiki/EnlargeSpace}{EnlargeSpace} given by
the medium:

\begin{quote}
In order to preserve
\href{http://meatballwiki.org/wiki/GlobalResource}{GlobalResources},
create more public space. This reduces limited resource tension. Unlike
the \href{http://meatballwiki.org/wiki/RealWorld}{RealWorld}, land is
cheap online. In effect, this nullifies the
\href{http://meatballwiki.org/wiki/GlobalResource}{TragedyOfTheCommons}
by removing the resource pressure that created the ``tragedy'' in the
first place. \textbf{You can't overgraze the infinity.} - \citep{MeatballWikiEnlargeSpace} 
\end{quote}

It is always possible to duplicate digital resources, create more spaces
to resolve tensions over shared resources, and so on. Enlarging space
has the natural potential to make the broader social scene bewildering
with a geyser of pages and communities, but can be further made less
damaging by having mechanisms to link histories, trace their divergence,
and potentially resolve a fork as is common in open source software
development. Forking is then a natural process of community
regeneration, allowing people to regroup to make healthier spaces when
needed, where the fork is itself part of the history of the community
rather than an unfathomable rift.

Forking communities is not the same as forking community resources:
``you can't fork a community {[}\ldots{]} what you can do is fork the
content and to \emph{split} the community'' \citep{MeatballWikiForkingOfOnlineCommunitiesa} . As described so far, a
fork divides people into unreconciled and separate communities. In some
cases this makes forking difficult, in others it makes it impossible:
the prime example, again, is Wikipedia. It is simply too large and too
culturally dominant to fork. Even though it is
\href{https://en.wikipedia.org/wiki/Wikipedia:FAQ/Forking\#Am_I_allowed_to_fork_Wikipedia?}{technically
possible} to fork Wikipedia, if you succeeded, then what? Who would come
with you to build it, and who would that be useful for? This is partly a
product of its totalizing effort to be an encyclopedia of everything
(what good would \emph{another} encyclopedia of everything be?) but also
the weight of history: you won't get enough long-encultured Wikipedians
to join you.

The last major effort to fork Wikipedia was in 2002 with an effort led
by Edgar Enyedy to move the Spanish Wikipedia to The Enciclopedia Libre
Universal en EspaÃ±ol \citep{tkaczSpanishForkWikipedia2011, tkaczWikipediaPoliticsOpenness2014} . Though it was brief and
unsuccessful, Enyedy claims that because Jimmy Wales was worried about
other non-English communities following their lead, he and the other
admins capitulated to the demands for no advertising and a transfer to a
.org domain, among others\footnote{Jimmy Wales, naturally, disputes this
  characterization of events.}. Even a politically symbolic fork is
dependent on the perceived threat to the original project, and that
window seems to have been closed after 2002.

The cultural tensions and difficulties that lead other wikis and
projects to fork have taken their toll on the editorship and culture of
Wikipedia. The community is drawn into
\href{https://meta.wikimedia.org/wiki/Conflicting_Wikipedia_philosophies}{dozens}
of conflicting philosophical camps: the
\href{https://meta.wikimedia.org/wiki/Special:MyLanguage/Deletionism}{Deletionists}\footnote{Also
  see
  \href{https://meta.wikimedia.org/wiki/Association_of_Wikipedians_Who_Dislike_Making_Broad_Judgments_About_the_Worthiness_of_a_General_Category_of_Article,_and_Who_Are_in_Favor_of_the_Deletion_of_Some_Particularly_Bad_Articles,_but_That_Doesn\%27t_Mean_They_Are_Deletionists}{Association
  of Wikipedians Who Dislike Making Broad Judgments About the Worthiness
  of a General Category of Article, and Who Are in Favor of the Deletion
  of Some Particularly Bad Articles, but That Doesn't Mean They Are
  Deletionists}} vs.~the
\href{https://meta.wikimedia.org/wiki/Special:MyLanguage/Inclusionism}{Inclusionists},
\href{https://meta.wikimedia.org/wiki/Eventualism}{Eventualists}
vs.~\href{https://meta.wikimedia.org/wiki/Immediatism}{Immediatists},
\href{https://meta.wikimedia.org/wiki/Special:MyLanguage/Mergism}{Mergists}
vs.~\href{https://meta.wikimedia.org/wiki/Special:MyLanguage/Separatism}{Separatists},
and yes even a stub page for
\href{https://meta.wikimedia.org/wiki/Wikisecessionism}{Wikisecessionism}.
Editorship has steadily declined from a peak in 2007. Its relatively
invisible community systems make it mostly a matter of chance or
ideology that new contributors are attracted in the first place. In its
calcification of norms, largely to protect against legitimate challenges
to the integrity of the encyclopedia, any newcomers that do find their
way into editing now have little chance to catch a foothold in the
culture before they are frustrated by (sometimes algorithmic) rejection
\citep{hillWikipediaEndOpen2019, halfakerRiseDeclineOpen2013} .

Arguably all internet communities have some kind of
\href{http://meatballwiki.org/wiki/WikiLifeCycle}{life cycle}, so the
question becomes how to design systems that support healthy forking
without replicating the current situation of fragmentation. Wikis,
including \href{http://www.meatballwiki.org/wiki/TransClusion}{Meatball}
and
\href{https://www.mediawiki.org/wiki/Extension:Interwiki}{MediaWiki}, as
well as other projects like
\href{https://en.wikipedia.org/wiki/Project_Xanadu}{Xanadu} often turn
to \textbf{transclusion} --- or being able to reference and include the
content of one wiki (or wiki page) in another. Rather than copying and
pasting, the remote content is kept updated with any changes made to it.

Transclusion naturally brings with it a set of additional challenges:
Who can transclude my work? Whose work can I transclude? Can my edits be
propagated back to their work? What can be transcluded, at what level of
granularity, and how? While before we had characterized splitting
communities as an intrinsic part of a fork, that need not be the case in
a system built for transclusion. Instead relationships post-fork are
then made an \emph{explicit social process} within the system, where
even if a community wants to work as separate subgroups, it is possible
for them to arrive at some agreement over what they want to share and
what they want to keep separate. This kind of decentralized work system
resembles radical organizing tactics like affinity groups where many
autonomous groups fluidly work together or separately on an array of
shared projects without aspiring to create ``one big movement'' \citep{kleinWereDCSeattle2001} . Murray Bookchin describes:

\begin{quote}
The groups proliferate on a molecular level and they have their own
''Brownian movement.'' Whether they link together or separate is
determined by living situations, not by bureaucratic fiat from a distant
center. {[}\ldots{]}

{[}N{]}othing prevents affinity groups from working together closely on
any scale required by a living situation. They can easily federate by
means of local, regional or national assemblies to formulate common
policies and they can create temporary action committees (like those of
the French students and workers in 1968) to coordinate specific tasks.
{[}\ldots{]} As a result of their autonomy and localism, the groups can
retain a sensitive appreciation of new possibilities. Intensely
experimental and variegated in lifestyles, they act as a stimulus on
each other as well as on the popular movement. \citep{bookchinNoteAffinityGroups1969} 
\end{quote}

To cherrypick a few lessons from more than 25 years of thought from tens
of thousands of people: The differing models of document vs.~thread
modes and separate article vs.~talk pages show us that using
\textbf{namespaces} is an effective way to bridge multimodal expression
on the same topic across
\href{https://communitywiki.org/wiki/TimeInWikis}{percerceived
timescales} or other conflicting communicative needs. This is especially
true when the namespaces have \textbf{functional
differentiation}\footnote{Tim Berners-Lee described this notion of
  functional differentiation in a much more general way in describing
  the nature of the URI: \textgreater{} The technology should define
  mechanisms wherever possible without defining policy. \textgreater{}
  \textgreater{} because we recognize here that many properties of URIs
  are social rather than technical in origin. \textgreater{}
  \textgreater{} Therefore, you will find pointers in hypertext which
  point to documents which never change but you will also find pointers
  to documents which change with time. You will find pointers to
  documents which are available in more than one format. You will find
  pointers to documents which look different depending on who is asking
  for them. There are ways to describe in a machine or human readable
  way exactly what sort of repeatability you would expect from a URI,
  but the architecture of the Web is that that is for something for the
  owner of the URI to determine.
  https://www.w3.org/DesignIssues/Axioms.html} like the tools for
threading conversations on Wikipedia Talk pages and the parsing and code
generation tools of Templates. These namespaces need to be
\textbf{visibly crosslinked} both to preserve the social character of
knowledge work, but also to provide a means of credit assignment and
tool development between namespaces. Any communication system needs to
be designed to \textbf{prioritize ease of leaving} and \textbf{ease of
forking} such that a person can take their work and represent it on some
new system or start a new group to encourage experimentation in
governance models and technologies. One way of accomplishing these goals
might be to build a system around \textbf{social transclusion} such that
work across many systems and domains can be linked into a larger body of
work without needing to create a system that becomes too large to fork.
The need for communication across namespaces and systems, coupled with
transclusion further implies the need for \textbf{bidirectional
transclusion} so that in addition to being able to transclude something
in a document, there is visible representation of work being transcluded
(eg. commented on, used in an analysis, etc.) by allowed peers and
federations.

These lessons, coupled with those from private bittorrent trackers,
linked data communities, and the p2p federated system we have sketched
so far give us some guidelines and motivating examples to build a varied
space of communication tools to communicate our work, govern the system,
and grow a shared, cumulative body of knowledge.






\subsection{Rebuilding Scientific
Communication}



 It's time to start thinking about interfaces. We have
sketched our system in turtle-like pseudocode, but directly interacting
with our linking syntax would be labor intensive and technically
challenging. Instead we can start thinking about tools for interacting
with it in an abstract way. Beneath every good interface we're familiar
with, a data model lies in wait. A .docx file is just a zipped archive
full of xml, so a blank word document that contains the single word
``melon'' is actually represented (after some preamble) like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{w:body}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{w:p} 
\OtherTok{    w14:paraId=}\StringTok{"0667868A"} 
\OtherTok{    w14:textId=}\StringTok{"50600F77"} 
\OtherTok{    w:rsidR=}\StringTok{"002B7ADC"} 
\OtherTok{    w:rsidRDefault=}\StringTok{"00A776E4"}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{w:r}\NormalTok{\textgreater{}}
\NormalTok{        \textless{}}\KeywordTok{w:t}\NormalTok{\textgreater{}melon\textless{}/}\KeywordTok{w:t}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}/}\KeywordTok{w:r}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{w:p}\NormalTok{\textgreater{}  }
\NormalTok{\textless{}/}\KeywordTok{w:body}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

Same thing with jupyter notebooks, where a block of code:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\textgreater{}\textgreater{}\textgreater{}}\NormalTok{ rating }\OperatorTok{=} \DecValTok{100}
\OperatorTok{\textgreater{}\textgreater{}\textgreater{}} \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}I rate this dream }\SpecialCharTok{\{}\NormalTok{rating}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\CommentTok{\textquotesingle{}I rate this dream 100\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

is represented as JSON (simplified for brevity):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\{}
  \DataTypeTok{"cell\_type"}\FunctionTok{:} \StringTok{"code"}\FunctionTok{,}
  \DataTypeTok{"id"}\FunctionTok{:} \StringTok{"thousand{-}vermont"}\FunctionTok{,}
  \DataTypeTok{"outputs"}\FunctionTok{:} \OtherTok{[}\FunctionTok{\{}
    \DataTypeTok{"name"}\FunctionTok{:} \StringTok{"stdout"}\FunctionTok{,}
    \DataTypeTok{"output\_type"}\FunctionTok{:} \StringTok{"stream"}\FunctionTok{,}
    \DataTypeTok{"text"}\FunctionTok{:} \OtherTok{[}
      \StringTok{"I rate this dream 100}\CharTok{\textbackslash{}n}\StringTok{"}
    \OtherTok{]}
  \FunctionTok{\}}\OtherTok{]}\FunctionTok{,}
  \DataTypeTok{"source"}\FunctionTok{:} \OtherTok{[}
    \StringTok{"rating = 100}\CharTok{\textbackslash{}n}\StringTok{"}\OtherTok{,}
    \StringTok{"print(f\textquotesingle{}I rate this dream \{rating\}\textquotesingle{})"}
  \OtherTok{]}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

So we are already used to working with interfaces to data models, we
just need to think about what kind of interfaces we need for a
scientific communication system.

Let's pick up where we left off with our linked data and tools. Recall
that we had a \texttt{project} named \texttt{\#my-project} that made
reference to our experiment, a few datasets that it produced, and an
analysis pipeline that we ran on it. We \emph{could} just ship the raw
numbers from the analysis, wash our hands of it, and walk straight into
the ocean without looking back, but usually scientists like to take a
few additional steps to visualize the data and write about what it
means.


\paragraph{Notebooks (JSON-LD)}

Say we have a means of downloading the results of some analysis we have
already run as a result of \texttt{\#my-project}. Recall that the data
system we described was a system that links names under our
\texttt{@jonny} namespace to a content-addressed p2p system, but someone
has built a package to handle that under the hood. We might do a quick
writeup in a notebook like this:

The .json inside our notebook file would look something like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\{}
   \DataTypeTok{"cell\_type"}\FunctionTok{:} \StringTok{"code"}\FunctionTok{,}
   \DataTypeTok{"execution\_count"}\FunctionTok{:} \DecValTok{2}\FunctionTok{,}
   \DataTypeTok{"id"}\FunctionTok{:} \StringTok{"rapid{-}information"}\FunctionTok{,}
   \DataTypeTok{"metadata"}\FunctionTok{:} \FunctionTok{\{}
    \DataTypeTok{"scrolled"}\FunctionTok{:} \KeywordTok{true}
   \FunctionTok{\},}
   \DataTypeTok{"outputs"}\FunctionTok{:} \OtherTok{[}
    \StringTok{"..."}
   \OtherTok{]}\FunctionTok{,}
   \DataTypeTok{"source"}\FunctionTok{:} \OtherTok{[}
    \StringTok{"x, y, sizes = get\_data(\textquotesingle{}@jonny:my{-}project:Analysis1\textquotesingle{})"}
   \OtherTok{]}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

We could make use of another linked data technology,
\href{https://json-ld.org/}{JSON-LD}, that is an extension and format
that is interoperable with the RDF links we have been using implicitly
throughout, to note that this cell\footnote{An individual unit of code
  or writing in a notebook is called a ``cell''} contains a reference to
our dataset. Say we use a \texttt{@comms} ontology to denote the various
parts of our communication system, and put that in the \texttt{metadata}
field:

\begin{Shaded}
\begin{Highlighting}[]
\ErrorTok{"metadata":} \FunctionTok{\{}
  \DataTypeTok{"scrolled"}\FunctionTok{:} \KeywordTok{true}\FunctionTok{,}
  \DataTypeTok{"@comms:usesData"}\FunctionTok{:} \StringTok{"@jonny:my{-}project:Analysis1"}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

Now say we have another little interface to declare links inline in our
notebook using
\href{https://ipython.readthedocs.io/en/stable/interactive/magics.html}{magic
commands}. We might declare the name of our notebook like

\texttt{\%\%docId\ @jonny:my-project:Writeup}

and then in the cell we indicate that we have plotted our data like
this:

\begin{verbatim}
%%cellId Plotty
%%cellLink @comms:plotsData @jonny:my-project:Analysis1
\end{verbatim}

So then, say, we indicate in \texttt{@jonny:my-project} that this
document is related to it, and the links embedded within the notebook
indicate that it has cells that use a specific result and plot it. If I
enable sharing from my namespace, it becomes a creditable and
discoverable part of my scientific work --- a straightforward means of
breaking up the scientific paper as the unit of knowledge work. Recall
that our sharing rules weren't just a binary switch, but can indicate
different people and groups, so we can communicate the intention of
publication and status of the document\footnote{While we're at it, why
  not make it explicit by declaring its
  \href{https://schema.org/creativeWorkStatus}{\texttt{creativeWorkStatus}}
  as \texttt{Draft}} on an analogue scale from a private demo to our
lab, a presentation to an institute or conference, or a public part of
the scientific discourse.

\paragraph{Forums \& Feeds}

Communication doesn't need to be (and shouldn't be) exclusively
unidirectional statements of fact. Our linked data system that allows us
to directly references the subcomponents of an experiment, including
analysis results and visualizations, naturally lends itself to use in a
\textbf{forum.} In between feed-only mediums like most social media
platforms and the indexical permanence of a wiki or publication, forums
are a currently missing piece in most scientific communication systems:
a way to have longform discussions about science in a public and
semipermanent environment.

We can start by imagining a forum where people in our discipline go to
present their work and solicit feedback. We think we really have
something, and it challenges some widely held previous results:

\begin{quote}
hi everyone it is me, take a look at my analysis:
\texttt{{[}{[}@forum:showImage\ @jonny:my-project:Writeup:Plotty{]}{]}}
!!render inline

I think it raises a number of interesting questions, in particular about
\texttt{@rival}'s long-standing argument
\texttt{@rival:hillsToDieOn:earthIsInsideTheSun} I also wonder what this
means about this conversation we've been having more broadly about
\texttt{@discipline:whereAreThePlanets}. Anyway, write back soon, xoxo
\end{quote}

Our rival is polite and professional, so they take the criticism in
stride and do their own analysis:

\begin{quote}
Interesting results! I think I will have to revisit that, as well as
something else I have been working on,
\texttt{@rival:projects:escapeTheSun}. I wonder what it would look like
if we used my analysis pipeline instead. I wrote a few conversion nodes
(\texttt{@rival:nodes:newNode}) that could make our work easier to
synchronize in the future.

\texttt{{[}{[}@forum:rerunAnalysis\ @jonny:my-project:Analysis1\ @rival:newAnalysis{]}{]}}

\texttt{{[}{[}@forum:completeGraph\ @rival:newAnalysis\ @jonny:my-project:Writeup:Plotty{]}{]}}

\texttt{{[}{[}@forum:showImage\ @rival:newAnalysis:Writeup:Plotty{]}{]}}
\end{quote}

They have their own compute server set up that listens for commands like
\texttt{@forum:rerunAnalysis} and so once they post, their server
downloads the container and re-runs the analysis. \texttt{rerunAnalysis}
is a link between our two analysis pipelines, so it is also possible to
cross-apply the other parts of my analysis chain to their reanalysis. In
this case say my \texttt{@rival} was careful to ensure their pipeline
returned exactly the same data format as mine did, so it's possible to
use something like \texttt{completeGraph} to retrace the steps in
between the results and the plots that were generated. These are, of
course, speculative features of a speculative forum, but they serve as
examples of how this kind of federated naming system allows for new
kinds of tools.

Sharing results, communicating them to the people that might be
interested, reconsidering and re-analyzing work is an extremely normal
part of science, but in this parallel universe we have the tools to also
contribute to a cumulative body of knowledge that is explicit and
public. If we allowed it, people that were interested in our data would
be able to find the other ways it was analyzed, visualized, and
discussed. We have recontextualized ours and our \texttt{@rival}'s
previously published work and enriched the discussion surrounding our
discipline's ongoing struggle to understand \texttt{whereAreThePlanets}.
And we managed to do it incrementally, with a smaller document than an
occasionally-titanic manuscript might be.

Traditional forums like \href{https://www.phpbb.com/}{phpBB} are housed
on a single domain and server, and have fixed moderation and structure.
A forum built on top of a p2p system of linked data designed for
\textbf{transclusion} and \textbf{ease of forking} could look a little
different. Rather than independent web service, we could build a forum
as another peer in our p2p swarm, and the forum could operate as an
\emph{interface} to the linked data system.

For concreteness, let's call our forum \texttt{@neurochat}. We join the
forum with our existing identity by sending them a
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-join}{\texttt{@as.Join}}
request from their login portal, which gives them permission to issue
certain links and activity on our behalf. \texttt{@neurochat} is a
minimal forum, a glassy reflection of a platonic ideal projected against
the cave wall of our laptop. It has a few broad categories like
``Neuromodulation'' and ``Sensory Neuroscience,'' within which are
collections of threads full of chronologically-sorted posts. This
organization is reflective of their internal concept model, so, for
example, threads within the Neuromodulation category are represented as
members of \texttt{@neurochat:categories:Neuromod} and so on. When we
post through their web interface, we create a few links with shared
custody: We create a
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-note}{\texttt{@as:Note}}
that is
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-attributedto}{\texttt{@as:attributedTo}}
us, has the
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-context}{\texttt{@as:context}}
of the thread we're posting in, and is linked as
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-inreplyto}{\texttt{@as:inReplyTo}}
the preceding post or any we've quoted. The forum is thus represented as
a \emph{discourse graph} whose structure is encoded as triplet links,
but also provides a set of UX tools for viewing and interacting with it.
Our humble \texttt{@jonny:myproject} now also carries with it references
to the places where it is discussed.

In the simplest case, the content of our posts could be mirrored between
the \texttt{@neurochat} server and our own namespace. Say our post
\texttt{@neurochat:posts:\textless{}post\_id\textgreater{}} is mirrored
as \texttt{@jonny:neurochat:...}. The embedding within our linked system
give us a much richer space of negotiation over permissions and the
status of our writing, though. Since this is a public forum, the server
might set posts to be able to be seen and re-represented by default. We
could then imagine a set of federated forums where a single post to one
of them is then crossposted to several different communities: eg. if our
work was an interdisciplinary project that was also releant to some
people from \texttt{@linguisticsChat}. If we have need for a bit more
privacy, our forum could take into account our own blocks of users and
federations, eg. if we never wanted our data/posts to be used by any
\texttt{@amazon}-affiliated federations or by known troll users.
\texttt{@neurochat} is a very barebones forum, so it would also be
possible for someone to create their own \emph{fork} of the
\emph{interface} to provide additional functionality, ux improvements,
etc. We could then trivially make a \emph{fork} of the \emph{community}
by picking up our corner of the discourse graph and associating it with
a new forum in the event of, eg. disagreements with the moderation team,
the strictures of the category system, etc. Since our posts are in our
own namespace, we could then transclude them wherever we wanted, eg. in
a wiki page about a topic as in
\href{https://anagora.org/node/agora-bot}{agora's twitter bot}.

We have been considering \texttt{@neurochat} as a distinct site with its
own code and features, presumably located at something like
\texttt{neurochat.com}, but we can further imagine it in conversation
with the parallel namespaces of wiki \texttt{Talk:} pages. If we think
of a paper or some other primary text as the ``Article'' page, we can
imagine being able to have a \texttt{Forum:} attached to it for further
discussion. This isn't far-fetched at all: this paper has
\href{https://gitter.im/scientific-infrastructure/community}{its own
gitter chatroom}, which is a primarily web-based
\href{https://matrix.org/}{Matrix} client \citep{hodgsonGitterNowSpeaks2020, hodgsonWelcomingGitterMatrix2020} .
Combined with transclusion between instances of forums, we could imagine
the forum for our particular project being indexed in a larger system of
scientific forums. So rather than a collection of empty rooms and new
logins to make, our forum is part of a broader scientific conversation,
but remains under our control.

Forums are just one point in a continuous feature space of communication
media: nested, chronological, feedlike collections of threads within
categories. If we were to take forum threads out of their categories,
pour them into our water supply, and drink whatever came our way like a
dog drinking out of an algorithmic fire hydrant, we would have Twitter.
Algorithmic, rather than purposefully organized feed systems have their
own sort of tachycardic charm. They are effective at what they aim to
do, presenting us whatever maximizes the amount of time we spend looking
at them in a sort of hallucinatory timeless now of infinite
disorganization --- at the expense of desirable features of a
communication system like a sense of stable, autonomously chosen
community, perspective on broader conversation, and cumulative
collective memory.

Still, the emergence of a recognizable ``Science Twitter'' demonstrates
the depth of need for rapid, informal communication systems in science.
We should embrace the plurality of registers in scientific
communication, that there needs to be space for near-amateurs to pose
naive questions alongside careful and considered formal scholarship.
That is just to say that we should reflect the division of formality
from scientific value in what we build, and build systems to support the
implicit communicative labor of science like whisper networks, mailing
lists, and groupchats that have always existed. The blending of digital
cultures, and broadly `non-academic scientists' with traditional
scientific communication streams is healthy: with appropriate caveats
for abuse, strawmen, et al.~I don't think it takes that much critical
analysis to argue that ``shitposts are good, actually, for science.''

A federated, multi-interface, autonomously-hosted system of social media
systems already exists, and we've been talking about it: the roughly
construed ``\href{https://fediverse.party/}{Fediverse}'' based (largely)
on ActivityPub.

Mastodon already implements most of the forum example described above:
it has its own protocol that
\href{https://docs.joinmastodon.org/spec/activitypub/}{extends
activitypub}, but it functions as an interface to a protocol-based
threaded communication. For example
\href{https://social.coop/@jonny/107328829457619549}{this post} is
represented in (abbreviated) JSON:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\{}
    \DataTypeTok{"id"}\FunctionTok{:} \StringTok{"107328829457619549"}\FunctionTok{,}
    \DataTypeTok{"created\_at"}\FunctionTok{:} \StringTok{"2021{-}11{-}23T22:52:49.044Z"}\FunctionTok{,}
    \DataTypeTok{"in\_reply\_to\_id"}\FunctionTok{:} \StringTok{"107328825611826508"}\FunctionTok{,}
    \DataTypeTok{"in\_reply\_to\_account\_id"}\FunctionTok{:} \StringTok{"274647"}\FunctionTok{,}
    \DataTypeTok{"sensitive"}\FunctionTok{:} \KeywordTok{false}\FunctionTok{,}
    \DataTypeTok{"spoiler\_text"}\FunctionTok{:} \StringTok{""}\FunctionTok{,}
    \DataTypeTok{"visibility"}\FunctionTok{:} \StringTok{"public"}\FunctionTok{,}
    \DataTypeTok{"url"}\FunctionTok{:} \StringTok{"https://social.coop/@jonny/107328829457619549"}\FunctionTok{,}
    \DataTypeTok{"content"}\FunctionTok{:} \StringTok{"\textless{}p\textgreater{}and making a reply to the post to show the in\_reply\_to and context fields\textless{}/p\textgreater{}"}\FunctionTok{,}
    \DataTypeTok{"account"}\FunctionTok{:}
    \FunctionTok{\{}
        \DataTypeTok{"id"}\FunctionTok{:} \StringTok{"274647"}\FunctionTok{,}
        \DataTypeTok{"username"}\FunctionTok{:} \StringTok{"jonny"}\FunctionTok{,}
        \DataTypeTok{"fields"}\FunctionTok{:}
        \OtherTok{[} \ErrorTok{...} \OtherTok{]}
    \FunctionTok{\},}
    \DataTypeTok{"media\_attachments"}\FunctionTok{:} \OtherTok{[]}\FunctionTok{,}
    \DataTypeTok{"mentions"}\FunctionTok{:} \OtherTok{[]}\FunctionTok{,}
    \DataTypeTok{"tags"}\FunctionTok{:} \OtherTok{[]}\FunctionTok{,}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

and then rendered by the particular version of Mastodon implemented on
the host, \href{https://social.coop}{social.coop}. As long as the host
sends and receives post (and other) data in a compatible format, it can
render it however it wants, add tools, etc. It becomes trivial to
imagine, then, a continuum of communication tools between and around
microblogging sites like Twitter and Mastodon and forums: just add
categorization, tagging, or systems for whatever need is revealed by the
normal dynamics of use.

The problem with an endless homogenous feed is filtering and
prioritizing what to show. The lack of control over feed content is not
an accident: it's the product --- ready access to a hundred million
hamsters on personalized content wheels with whatever combination of
micro and macrotargeting you could want. Nothing seems out of the
ordinary when you have no control over what you see. Reciprocally,
there's no way aside from herding a flock of alternate accounts to
direct what you say to different audiences. Mastodon can filter posts at
a federation level\footnote{Only other servers that the host server
  federates with are listed}, with hashtags, and lets users make lists
of peers, but is a proudly chronological feed. No algorithms allowed.
Using it has a learning curve, as when you start you see nothing, but
before you know it you can't find anything in the pile. Forums threads,
within categories are also typically chronologically sorted, but because
they are identified with a \emph{subject} rather than by the
\emph{person} who started the thread typically have longer lifespans and
more findable.

There is no single answer to systems of discovery, but somewhere between
explicit categorical organization, person and subject-centric threads,
semantic annotation, and making smaller p2p federations is a recipe for
a broad, continuous, and cumulative scientific discussion. Instead of
casting about for advice within our information bubbles, we might aspire
to having a \emph{place} to \emph{ask} the people who \emph{might know}.
Instead of starting another new slack with a few hundred posts that then
vanishes entirely, we might imagine being able to fluidly form and
dissolve communities and be able to build on their history.


\paragraph{Annotation \& Overlays}

We can't expect the entire practice of academic publishing to transition
to cell-based text editors anytime soon. In the same way that we
discussed frameworks for integrating heterogeneous analytical and
experimental tools, we need some means of \textbf{bridging}
communication tools and \textbf{overlays} for interacting with
communication formats. Bridging communication protocols is a relatively
well-defined project, eg. the \href{https://matrix.org/bridges/}{many
ways to use Matrix} with
\href{https://matrix.org/bridges/\#slack}{Slack},
\href{https://matrix.org/bridges/\#email}{email},
\href{https://matrix.org/bridges/\#signal}{Signal}, etc. The overlays
for websites, pdfs, and other more static media that we'll discuss are
means for annotation and bidirectional transclusion: including pieces of
the work elsewhere, and representing inclusions elsewhere on the work.
In representing the intrinsically interactive and social nature of
reading (eg. see \citep{jacksonMarginaliaReadersWriting2001} ),
overlays naturally lend themselves to imagining new systems to replace
traditional mechanisms for peer-review and criticism. We don't need to
look far to find a well-trod interface for annotation overlays: we
shouldn't underrate the humble highlighter.

\href{https://hypothes.is}{Hypothes.is}, enabled on this page, lets
readers highlight and annotate any webpage with a
\href{https://chrome.google.com/webstore/detail/hypothesis-web-pdf-annota/bjfhmglciegochdpefhhlphglcehbmek}{browser
extension} or javascript bookmarklet. At its heart is a system for
making anchors, references to specific places in a text, and the means
of matching them even when the text changes or the reference is
ambiguous \citep{csillagFuzzyAnchoring2013} . For example,
\href{https://hypothes.is/a/oLw4uk7_Eeyt5N-FVlE3fw}{this anchor} has
three features, a \texttt{RangeSelector} that anchors it given the
position within the paragraph, an absolute
\texttt{TextPositionSelector}, and a contextual
\texttt{TextQuoteSelector} that you can see with an
\href{https://api.hypothes.is/api/annotations/oLw4uk7_Eeyt5N-FVlE3fw}{API
call}

\begin{marginfigure}
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\{}\DataTypeTok{"selector"}\FunctionTok{:}
\OtherTok{[}\FunctionTok{\{}
    \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"RangeSelector"}\FunctionTok{,}
    \DataTypeTok{"endOffset"}\FunctionTok{:} \DecValTok{178}\FunctionTok{,}
    \DataTypeTok{"startOffset"}\FunctionTok{:} \DecValTok{171}\FunctionTok{,}
    \DataTypeTok{"endContainer"}\FunctionTok{:} \StringTok{"/main[1]/div[1]/p[368]"}\FunctionTok{,}
    \DataTypeTok{"startContainer"}\FunctionTok{:} \StringTok{"/main[1]/div[1]/p[368]"}
\FunctionTok{\}}\OtherTok{,}
\FunctionTok{\{}
    \DataTypeTok{"end"}\FunctionTok{:} \DecValTok{254006}\FunctionTok{,}
    \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"TextPositionSelector"}\FunctionTok{,}
    \DataTypeTok{"start"}\FunctionTok{:} \DecValTok{253999}
\FunctionTok{\}}\OtherTok{,}
\FunctionTok{\{}
    \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"TextQuoteSelector"}\FunctionTok{,}
    \DataTypeTok{"exact"}\FunctionTok{:} \StringTok{"anchors"}\FunctionTok{,}
    \DataTypeTok{"prefix"}\FunctionTok{:} \StringTok{"ts heart is a system for making "}\FunctionTok{,}
    \DataTypeTok{"suffix"}\FunctionTok{:} \StringTok{", references to specific places "}
\FunctionTok{\}}
\OtherTok{]}\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}
\end{marginfigure}

On its own, it serves to give a \texttt{Talk:} page to every website.
With an integration into a system of linked data and identity, it also
serves as a means of extending the notion of bidirectional transclusion
described above to work that is not explicitly formatted for it. Most
scientific work is represented as \texttt{.pdf}s rather than
\texttt{.html} pages, and hypothes.is
\href{https://web.hypothes.is/help/annotating-locally-saved-pdfs/}{already
supports} annotating PDFs. With an integration into pdf reading
software, for example
\href{https://www.zotero.org/support/pdf_reader_preview}{Zotero's
(currently beta) PDF reader}, there would be a relatively low barrier to
integrating collaborative annotation into existing workflows and
practices.

The dream of public peer review with public annotation is relatively
straightforward, but so are the nightmares of a scientific literature
swamped with trolls. Talking about our work on a forum with a
``forward'' reference, of the work linked to by the forum or on PubPeer
feels fine, but the ``reverse'' reference of an annotation appearing on
your page that links to a forum discussion is significantly more
challenging --- ``who gets to annotate my work?''

Framed as an annotation system, the answer given by the current model of
peer review is ``usually three, usually anonymous people.'' Except the
document and annotations are usually private until the author revises
the document to the point where no annotations remain, and the peer
reviewers become invisible along with the social nature of the review.
The notion that the body of scientific knowledge is best curated by
passing each paper through a gauntlet of three anonymous reviewers,
after which it becomes Fact and nearly as a rule never changed is
ridiculous on its face.

Digital publishing makes imagining the social regulation of science as a
much more broadly based and continuous process much easier, but the
problem of moderation remains. Some movement has been made towards
public peer review: eLife has integrated hypothes.is since 2016 \citep{ELifePartnersHypothes2016} , and bioRxiv had decided to
integrate it as well in 2017 \citep{dwhlyBioRxivSelectsHypothesis2017}  before getting cold feet about the genuinely hard problem of
moderation (among others \citep{heatherstainesPreprintServicesGather2018} ) and instead adopting the
more publisher-friendly TRiP system of refereed peer-reviews \citep{nateangellAnnouncingTRiPTransparent2019} .

Asking every author to become a forum moderator and constantly
patrolling the annotations of their papers sounds bad, as does the work
of maintaining block and allowlists for every individual account. While
a full description of the norms and tools needed to maintain healthy
public peer review is out of scope here, the system of autonomous users
being able to organize into overlapping federations described throughout
\emph{provides a space for having that conversation.} Authors could, for
example, allow the display of annotations from a professional society
like \texttt{@sfn} that has a code of conduct and moderation team, or
annotations associated with comments on \texttt{@pubpeer}, or from a
looser organization of colleagues and other \texttt{@neurofriends}.
Conversely, being able to make annotations and comments from different
federations gives us a rough proxy to different registers of
communication and preserves the plurality of our expression. While my
official \texttt{@university}-affiliated federation is restrained and
academic, my \texttt{@neurotrans} alt might be a little more
freewheeling. A protocol for federating peers that we first described in
the context of sharing data has the more general consequence of creating
a means of negotiating and experimenting with different systems of
social norms and governance.

Social tools like these are in the hypothes.is team's
\href{https://web.archive.org/web/20211015213849/https://github.com/hypothesis/product-backlog/projects/6}{development
roadmap}, but I intend it as a well-developed and mature example of a
general type of technology\footnote[][-6cm]{cf.~the
  \href{https://genius.com}{genius.com} overlay.}. Many scientists are
already familiar with another implementation: the comment and review
features of Google Docs and Microsoft Word. We already use these tools
to work together to improve our work, but there's no reason the process
should stop at the time of publication. Combined with a system for
valuing and publishing smaller units of work, the process of public peer
review starts to look a lot healthier as a continuous process of
communication and collective mentorship instead of the current system of
a gladitorial thumbs up/down indictment on years of your life.


\paragraph{Trackers \& Wikis}

The final set of social interfaces is effectively the ``body'' of social
technology. So far our infrastructural systems have an unsatisfyingly
hollow center: there's a lot of talk about tool frameworks and protocols
for linked data, but \emph{where is it? what does it look like?} We can
pick up the threads left hanging from our description of
\protect\hyperlink{archives-need-communities}{bittorrent trackers} and
knit them in with those from \protect\hyperlink{the-wiki-way}{the wiki
way} and describe how systems for surfacing procedural and technical
knowledge work can also serve as a basis of searching, indexing, and
governing the rest of the system.

Bittorrent trackers serve to index data and organize a curation
community --- we need that too, let's start there. Say we have a tracker
that indexes a particular format of data, as
\href{https://hub.dandiarchive.org}{\texttt{@dandihub}} does with
\texttt{@nwb}. We can search for data using all the fields of NWB, but
don't want to rely just on the peers that are active, so the role of the
tracker is to maintain a searchable index of metadata that refers to the
datasets shared by peers. We want to be interoperable with other
trackers that index compatible data, so let's say that's implemented as
a database that supports
\href{https://www.w3.org/TR/sparql11-federated-query/}{SPARQL federated
queries}\footnote[][-15cm]{Tim Berners-Lee \href{https://www.w3.org/DesignIssues/RDFnot.html}{describes} the distinction between
  traditional relationship databases and RDF databases: 


  "Relational database systems, manage RDF data, but in a specialized
  way. In a table, there are many records with the same set of
  properties. An individual cell (which corresponds to an RDF property)
  is not often thought of on its own. SQL queries can join tables and
  extract data from tables, and the result is generally a table. So, the
  practical use for which RDB software is used typically optimized for
  soing operations with a small number of tables some of which may have
  a large number of elements.

  RDB systems
  have datatypes at the atomic (unstructured) level, as RDF and XML
  will/do. Combination rules tend in RDBs to be loosely enforced, in
  that a query can join tables by any comlumns which match by datatype
  -- without any check on the semantics. You could for example create a
  list of houses that have the same number as rooms as an employee's
  shoe size, for every employee, even though the sense of that would be
  questionable.

  The Semantic Web is not
  designed just as a new data model - it is specifically appropriate to
  the linking of data of many different models. One of the great things
  it will allow is to add information relating different databases on
  the Web, to allow sophisticated operations to be performed across
  them. } where requests can
be spread across many databases. For concreteness, let's assume that the
results of our search are some content-addressed reference to a resource
on a p2p network like a
\href{https://en.wikipedia.org/wiki/Magnet_URI_scheme}{magnet link}.

We need some kind of \emph{client} that can download files and run in
the background to share them. We can start with the image of a
bittorrent client like \href{https://www.qbittorrent.org/}{qBittorrent}
that does just that, but we also need a means of making the link
declarations that we did before in pseudocode, and it makes sense for
the client to handle that as well. Let's say our client handles our
identity, either by a self-created cryptographic hash as in IPFS\citep{benetIPFSContentAddressed2014} , or attested by some trusted
third party as in ActivityPub. Instead of our identity being tied to the
services provided by the server, however, we can think of this as a
peer-to-peer ActivityPub where we can directly send and receive messages
containing our links and negotiating our connections. As an interface,
say we have a typical file browser that we can set permissions for
files, group them into projects, and share them with others. Since the
system consists of links, an editor that allows users to visualize and
edit a hierarchical graph of nodes and (typed) edges:

!! input network editing React figure from presentation here!

So say it's time for us to share a dataset. We click the `share' button
in our client which sends an ActivityPub-style message saying we have
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-create}{\texttt{@as:Create}}d
a new resource to the other peers indicated in our permission settings.
This message both uploads the metadata for our dataset to the, say,
\texttt{@dandihub} tracker, but since \texttt{@dandihub} is an
equivalent peer in our system, and modeling off ActivityPub we are able
to have ``friends,'' we can notify other researchers directly. The
tracker can host our metadata pointing to our data so it's available
from any other peer that's hosting it even if we go offline, but peers
can query us directly to enumerate all the links, datasets, etc. we have
allowed them to.

What about handling format extensions not included in the base
\texttt{@nwb} format? Since we own the representation of our data, we
can imagine a strict base \texttt{@nwb}-only tracker, but also think of
\texttt{@dandihub} that has built tools to handle extensions. So
alongside our dataset we can upload an extension like our
\texttt{@jonny:SolarEphys} example that derives from
\texttt{@nwb:ElectricalSeries}, and the tracker then can display our
extension as well as all the other extensions that branch off the
various points of the standard. At this point we can imagine a spray of
thousands of trivially different extensions to handle overlapping data
types, which is where most data stores typically stop, but let's explore
community systems built on forums and wikis for schema resolution as an
example of \emph{distributed governance.}

!! figure of lots of leaf nodes hanging off ElectricalSeries

Wikis are not magical systems of infinite pluralistic knowledge, but one
thing they do well is provide the means of developing durable but
plastic systems norms and policies for a wide variety of social systems.
Butler, Joyce and Pike, emphasis mine:

\begin{quote}
Providing tools and infrastructure mechanisms that support the
development and management of policies is an important part of creating
social computing systems that work. {[}\ldots{]}

When organizations invest in {[}collaborative{]} technologies,
{[}\ldots{]} their first step is often to put in place a collection of
policies and guidelines regarding their use. \textbf{However, less
attention is given to the policies and guidelines created by the groups
that use these systems which are often left to ``emerge''
spontaneously.} The examples and concepts described in this paper
highlight the complexity of rule formation and suggest that support
should be provided to help collaborating groups create and maintain
effective rulespaces.

{[}\ldots{]} \textbf{The true power of wikis lies in the fact that they
are a platform that provides affordances which allow for a wide variety
of rich, multifaceted organizational structures.} Rather than assuming
that rules, policies, and guidelines are operating in only one fashion,
wikis allow for, and in fact facilitate, the creation of policies and
procedures that serve a wide variety of functions \citep{butlerDonLookNow2008} 
\end{quote}

So between discussion on the forum or in \texttt{Talk:}-like pages, we
can imagine a set of norms and policies evolving from the community on
this particuar tracker, perhaps unlike other trackers. In this case we
can imagine someone wanting to clean up some near-equivalent extensions
by starting a thread in the forum to discuss the proposed changes. Say
we want to merge \texttt{@jonny:Extension1} and
\texttt{@rumbly:Extension2} -- the forum notifies us that someone is
talking about our extension so we have a chance to weigh in. If we reach
some sort of amicable consensus where we agree to supercede it with a
merged \texttt{@forum:Extension3} type, the forum could send us a
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-offer}{\texttt{@as:Offer}}
to
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-update}{\texttt{@as:Update}}
our extension, which should we
\href{https://www.w3.org/TR/activitystreams-vocabulary/\#dfn-accept}{\texttt{@as:Accept}}
from our client then notifies all the downstream consumers of our data
and extension that its format has changed.

What if consensus fails? Since every link in the system is underneath a
\texttt{@namespace}, links never have a pretense of ``correctness,'' but
have the ontological status of a linguistic gesture: links are
``something someone said'' that we're free to disagree with\footnote{``For
  example, one person may define a vehicle as having a number of wheels
  and a weight and a length, but not foresee a color. This will not stop
  another person making the assertion that a given car is red, using the
  color vocabular from elsewhere.'' -
  https://www.w3.org/DesignIssues/RDB-RDF.html}. In that case, the
\texttt{@forum:Extension3} exists as ``someone said these are
equivalent, but I don't necessarily agree'' and the forum is free to
represent its cleaned up representation while preserving the plurality
of expression in our data format. If I want to go to greener pastures to
a forum that has policies and culture closer to mine, it's relatively
straightforward to federate with a new tracker and move my data there
since I still own it all.

Let's pick up scientific communication in linked data
\protect\hyperlink{forums--feeds}{forums} in conversation with the
\protect\hyperlink{archives-need-communities}{social incentives for
curation} of trackers. This system as described is a forum where
everyone in the conversation has access to the data and results in
question reminiscent of What.cd and access to music. While
upload/download ratio might not be the best social incentive system for
scientific trackers, there are plenty of others.

For example, we briefly mentioned a Folding@Home-like system of donated
computing resources, and separately described embedding analyses in a
forum by calling our own compute resources. Together, a tracker could
implement a compute ratio where to use shared computing resources you
need to contribute a certain amount of your own. The bounty system where
peers would donate their excess upload in exchange for uploading a rare
album on what.cd could translate to one where someone who has donated a
lot of excess compute time could donate it for someone uploading or
collecting a particular dataset.

Another tracker more focused on sharing and reviewing results might make
a review ratio system, where for every review your work receives you
need to review n other works. This would effectively function as a
\textbf{reviewer co-op} that can make the implicit labor of reviewing
explicit, and develop systems for tying the reviews required for
frequent publication with explicit norms around reciprocal reviewing.

Forum and feedlike media are good for organizing continuous
conversation, but wikis serve as a more durable knowledge store for
cumulative reference information. We don't need to imagine wikis as
being text-only, with wiki formatting used just to change the appearance
of text, but as a means of declaring and manipulating semantic links.
For example,
\href{https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki}{Semantic
MediaWiki} is an extension to Wikipedia's wiki system that extends
\texttt{{[}{[}Wikilinks{]}{]}} to be able to declare semantic links like
\texttt{{[}{[}linkType::Target{]}{]}}. For example, if our project had a
wiki page like \texttt{{[}{[}My\ Project{]}{]}} we could say it
\texttt{{[}{[}hasType::@analysis:project{]}{]}} and
\texttt{{[}{[}usesDataset::@jonny:mydata1{]}{]}} etc. These wikis have
the capability to not only organize knowledge, but also serve as a
flexible means of declaring new programming interfaces and assigning
credit.

As a live example, let's consider the
\href{https://wiki.auto-pi-lot.com}{Autopilot Wiki} at
\url{https://wiki.auto-pi-lot.com}. This wiki has a set of categories,
properties, templates, and forms for describing the additional
contextual technical knowledge needed to use
\href{https://docs.auto-pi-lot.com/en/latest/}{Autopilot}, a framework
for behavioral experiments \citep{saundersAutopilotAutomatingBehavioral2019} . The semantic structure
of the links is useful for designing interfaces based on complex
queries, for example ``find me all the
\href{https://wiki.auto-pi-lot.com/index.php/Category:Passive_Component}{\texttt{passive\ electronic\ components}}
that have a
\href{https://wiki.auto-pi-lot.com/index.php/Category:Guide}{\texttt{guide}}
that describes
\href{https://wiki.auto-pi-lot.com/index.php/Property:Uses_Tool}{\texttt{using}}
a
\href{https://wiki.auto-pi-lot.com/index.php?title=Property\%3AUses+Tool\&limit=20\&offset=0\&filter=Soldering+Iron}{\texttt{soldering\ iron}}
to build
\href{https://wiki.auto-pi-lot.com/index.php?title=Property\%3AModality\&limit=20\&offset=0\&filter=Illumination}{\texttt{lighting}}
for a behavioral
\href{https://wiki.auto-pi-lot.com/index.php?title=Property\%3AModality\&limit=20\&offset=0\&filter=Enclosures}{\texttt{enclosure}}''.
Each page can have a
\href{https://wiki.auto-pi-lot.com/index.php/Autopilot_Behavior_Box\#tab-content-facts-list}{rich
semantic description} with multimodal links describing tools, CAD
diagrams, associated DOIs, software dependencies, etc. Links can be
declared \texttt{{[}{[}linkModality::inline{]}{]}} as a fluid part of
writing, but also can be submitted by using forms (eg for new
\href{https://wiki.auto-pi-lot.com/index.php/Form:Part}{Parts}) with
structured, autocompleting properties to lower syntax barriers for new
users.

The ``soft durability'' of wikis makes space to discuss ``off-label''
uses for hardware common across many disciplines that typically exists
as lab lore rather than documented. For example, an early-adopter of
Autopilot sent me a message saying they weren't able to get ultrasound
from an
\href{https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2}{amplifier}
that was advertised up to 192kHz. Upon further study, we found there was
a 20kHz low-pass output filter and were able to find and remove the
components and leave a trail of breadcrumbs for future users. Though
this is a simple example, it is emblematic of the kind of knowledge work
that currently has no good means of communication or professional
valuation.

The blend of programmatic and natural language descriptions makes it
easy to contribute to, but also makes knowledge organization improve the
software that uses it. The
\href{https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2}{Amp2} page
lists which of the GPIO pins of a raspberry pi it depends on, so
Autopilot will be extended to check for conflicting hardware
configurations\footnote{for example, pin 7 mutes the board, but is still
  exposed in the 40-pin header. We powered an LED with pin 7 and were
  absolutely baffled why the sound would mute every time the light went
  on for a week or so.}. Better: since it's possible for anyone to make
new templates, forms, categories, and pages, the wiki can be used to
build new programming interfaces entirely. Autopilot's
\href{https://docs.auto-pi-lot.com/en/latest/guide/plugins.html}{plugin
system} is built this way, where one submits a
\href{https://wiki.auto-pi-lot.com/index.php/Autopilot_Plugins}{plugin}
with a
\href{https://wiki.auto-pi-lot.com/index.php/Form:Autopilot_Plugin}{form}
which then makes it immediately available to any Autopilot user.

The addition of structured contextual knowledge to our system gives us
an almost comical degree of provenance: from conversations in a forum
that reference a paper, that links to its analysis, data, experimental
software, all the way back to the properties of the solenoids used in
the experiment. It's not just provenance for provenance's sake as extra
labor, every step is \emph{useful} to the experimenter. I give the
example of the Autopilot wiki for concreteness, but the broader point is
that forums and wikis can serve the role of negotiating systems of
expression for different parts of the system.

The same combination of trackers, forums, and wikis has a natural
application to analysis pipelines. Ideally, to move beyond fragile code
reduplicated in every lab, we need some means of reaching consensus on a
few canonical implementations of fundamental analysis operations. Given
a system where analysis chains are linked to the formats and
subdisciplines they are used with, we can map a semantically dense map
of the analysis paths used in a research domain. In neurophysiology:
``What are the different ways spikes are extracted and analyzed from
extracellular electrophysiology recordings?'' Having the ability to
discuss and contextualize different analytical methods elevates all the
exasperated methods critiques and exhortations to ``not use this
statistically unsound technique'' into something \emph{structurally
expressed in the practice of science.} See all the \texttt{@neurotheory}
threads about this specific analysis chain, or the \texttt{@methodswiki}
page that summarizes this general category of techniques.

We're now in a place where we can address the problem of a cumulative
knowledge system for science directly. In many (most?) scientific
epistemologies, scientific results do not directly reflect some truth
about reality, but instead instead are embedded in a system of meaning
through a process of active interpretation (eg. \citep{meehlTheoreticalRisksTabular1978} ). The interpretation of every
scientific result is left as the responsibility of the authors to
recreate and a few reviewers to evaluate, which would be a monumental
amount of labor given the velocity of papers, so researchers do the best
they can engaging with a small amount of research. Since the space of
argumentation is built from scratch each time from incomplete
information, there's no guarantee of making cumulative progress on a
shared set of theories, and most fall far from the supposed ideal of
hard refutation and can have long lives as ``zombie theories.'' van
Rooij and Baggio describe the ``collecting seashells'' approach of
gathering many results and leaving the theory for later with an analogy:

\begin{quote}
``In a sense, trying to build theories on collections of effects is much
like trying to write novels by collecting sentences from randomly
generated letter strings. Indeed, each novel ultimately consists of
strings of letters, and theories should ultimately be compatible with
effects. Still, the majority of the (infinitely possible) effects are
irrelevant for the aims of theory building, just as the majority of
(infinitely possible) sentences are irrelevant for writing a novel.''
\citep{vanrooijTheoryTestHow2021} 
\end{quote}

They and others (eg. \citep{guestHowComputationalModeling2021} )
have argued for an iterative process of experiments informed by theory
and modeling that confirm or constrain future models. Their articulation
of the need for multiple registers of formality and rigidity is
particularly resonant here. van Rooij and Baggio again:

\begin{quote}
``The first sketch of an f need not be the final one; what matters is
how the initial f is constrained and refined and how the rectification
process can actually drive the theory forward. Theory building is a
creative process involving a dialectic of divergent and convergent
thinking, informal and formal thinking.'' \citep{vanrooijTheoryTestHow2021} 
\end{quote}

Let's turn our provenance chain into a circle: a means of linking
theories to analytical results and interpretation as well as
experimental design and tooling. Say the theorists have a wiki. They
start making some loose schematic descriptions of their theories and
linking them to different experimental results that constrain, affirm,
refute, or otherwise interact with them. These could be forward or
backlinks: declared by the original author or by someone else describing
their results.

In the most optimistic case, where we have a full provenance chain from
analytical results back through experimental practice, we have a means
of formally evaluating the empirical contingencies that serve as the
evidence for scientific theories. For a given body of experimental data
bearing on a theoretical question, what kinds of evidence exist? As the
state of the art in analytical tooling changes, how are the
interpretations of prior results changed by different analyses? How do
different experimental methodologies influence the form of our theories?
The points of conflicting evidence and unevaluated predictions of theory
are then a means of distributed coordination of future experiments:
guided by a distributed body of evidence and interpretation, rather than
the amount of the literature base individual researchers are able to
hold in mind, what are the most informative experiments to do?

The pessimistic case where we only have scientific papers in their
current form to evaluate is not that much worse --- it requires the
normal reading and evaluation of experimental results of a review paper,
but the process of annotating the paper to describe its experimental and
analytical methods as a shared body of links makes that work cumulative.
Even more pessimistic, where for some reason we aren't able to formulate
theories even as rough schematics but just link experimental results to
rough topic domains is still vastly better than the current state of
disorganization and proprietary indices.

For both researchers and the public at large a meta-organization of
experimental results changes the way we interact with scientific
literature. It currently takes many years of implicit knowledge to
understand any scientific subfield: finding canonical papers, knowing
which researchers to follow, which keywords to search in table of
contents alerts. Being able to find a collection of papers about an
object of research, as well as the conversations at all levels of
formality that contextualize them --- to say nothing of building a world
without paywalls --- would profoundly lower barriers to access to
primary scientific knowledge for \emph{everyone.}

It is worth pausing to compare a world where we boisterously and fluidly
organize knowledge explicitly as a collective project of understanding
with one where knowledge organization is weaponized into a product that
lets us get ahead of our competitors without necessarily improving our
understanding of the body of scientific literature. One sounds like
science, the other sounds like industry capture.

All the technological-social tools described here are not a definitive
set of tools needed for scientific communications infrastructure, but
\emph{examples of interfaces to a linked data system.} Using JSON-LD
notebooks to enable us to embed links in our writing to be mentioned or
transcluded elsewhere. Using a forum as a means of creating linked
discussions about experimental results and analyses. Using linked
microblogging tools for a rapid, informal means of organizing and
discussing knowledge. Using all of the above to represent the many
expressions of a work across multiple linked namespaces. Using
annotation tools to create anchors and links for referencing links in
other communication media. Using tracker-like and wiki-like systems to
interact with, negotiate about, and govern a wily body of autonomously
declared links.

Each is intended to be mutable, easy to iterate on, uncontrolling,
mutually coordinated. Each interacts with and augments the previously
described systems for shared data, analytical, and experimental tools.
The purpose of this section is not to advocate a specific set of
technologies, but to describe a base layer of familiar technologies for
an indefinite future of possible interfaces for representing and
interacting with a body of shared knowledge.

What we've described is a nonutopian, fully realizable path to making a
scientific system that is fully negotiable through the entire
theoretical-empirical loop with minor development of existing tools and
minimal adjustment of scientific practices. No clouds, no journals, a
little rough around the edges but collectively owned by all scientists.


\subsection{Credit Assignment}



\begin{quote} The reason we are (once again) having a
fight about whether the producers of publicly available/published data
should be authors on any work using said data is that we have a
completely dysfunctional system for crediting the generation of useful
data. \citep{eisenReasonWeAre2021} 

The same is true for people who generate useful reagents, resources and
software. \citep{eisenSameTruePeople2021} 

And like everything, the real answer lies on how we
assess candidates for jobs, grants, etc\ldots{} So long as people treat
authorship as the most/only valuable currency, this debate will fester.
But it's in our power to change it. - Michael Eisen, EIC eLife \citep{eisenEverythingRealAnswer2021} 
\end{quote}

The critical anchor for changes to the scientific infrastructural
systems is the system of professional incentives that structure it. As
long as the only thing that has professional value is authorship in
journal papers, the system stays: Blog posts, analysis pathways, wikis,
and forums are nice and all, but they don't count as \emph{science.}

Imagining different systems of credit assignment is easy: just make a
new DOI-like identifier for my datasets that I can put on my CV.
Integrating systems of credit assignment into commonly-held beliefs
about what is valuable is harder. One way to frame solutions to the
credit assignment problem is as a collective action problem:
everyone/funding agencies/hiring committees just need to \emph{decide}
that publishing data, reviewing, criticism et al.~is valuable without
any serious changes to broader scientific infrastructure. Another is to
\emph{displace} the system of credit assignment by aligning the
interests of the broad array of researchers, technicians, and students
that it directly impacts to build an alternative.

The sheer quantity of work that is currently uncredited in science is a
structural advantage to any more expansive system of credit assignment.
The strategic question is how to design a system that aligns the
self-interest of everyone with uncredited work to build it.

That's what I've tried to do here. Everything that exists in this system
is attributable to one or many equal peers. Rather than attempting to be
an abstract body of knowledge, clean and tidy, that conceals its social
underpinnings, we embrace its messy and pluralistic personality. We have
\emph{not} been focused on some techno-utopian dream of automatically
computing over a system of universally linked data, but on representing
and negotiating over a globally discontinuous body of work and ideas
linked to people and groups. We have \emph{not} been imagining new
platforms and services to suit a limited set of needs, but on a set of
tools and frameworks to let people work together to cumulatively build
what they need.

Credit is woven throughout this system: the means of using someone
else's work are tied to crediting it. While credit is currently meted
out by proprietary journal aggregators like google scholar, citeseer, or
web of science; downloading a dataset, using an analysis tool, and so on
should be directly attributed to a digital identity that you control.

The first-order effects for the usual suspects in need of credit are
straightforward: counting the number of analyses and papers our datasets
are cited in, seeing the type of experiments our software was used to
perform. Control over the means of credit assignment also opens the
possibility of surfacing the work that happens invisibly but is
nonetheless essential for the normal operation of research. Why
shouldn't the animal care technician receive credit for caring for the
animals that were involved with a study, its results, and its impact on
science more broadly?

Contextual technical knowledge is an example that warrants special
consideration. Why would anyone spend the time to describe the fine
technical details of how to use a type of motor, or which solenoids last
the longest, or how to solder this particular type of circuit board?

First (and hopefully familiarly), by making it practically useful for
the researchers involved: say in this example we're using a lab wiki to
coordinate work locally, using tools that can use the wiki information
to automatically configure
\href{https://wiki.auto-pi-lot.com/index.php/Lee_LHDA0531115H}{solenoid}
or
\href{https://wiki.auto-pi-lot.com/index.php/TT_Electronics_OPB903L55}{sensor}
polarity, or the
\href{https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2}{dependencies
for a sound card}.

Second, by making sure the researchers are credited for their work. A
name prominently displayed on a wiki page and a permalink for a CV is
ok, but clearly not enough. Foundational technical and documentation
work like this is useful in itself, but its impact is mostly felt
\emph{downstream} in the work it enables. Beyond first-order credit, a
linked credit assignment system lets us evaluate \emph{higher-order}
effects of work that \emph{more closely resemble} the actual impact of
the work. Say we find someone else's
\href{https://wiki.auto-pi-lot.com/index.php/3D_CAD}{3D Model}, modify
it for our use, and then use it to collect a dataset and publish a
paper. Someone else sees it and links a colleague to it, and they too
use it in their work. Over time someone else updates the design and puts
it in some derivative component. Most of the linking is automatic, built
into the interfaces of the relevant tools, and soon the network of links
is dense and deep.

The incentives here are all aligned towards creating links and assigning
credit: For us, instead of just getting professional credit for our
paper, we also get credit for extending someone else's work, for
documenting it, and for the potentially large number of nth-order
derivative uses. Our credit extends multimodally, including papers that
cite papers that use our tool, and the ``amount'' of credit can be
contextualized because the type of link between them is explicit -- as
opposed to the non-semantic links of citation. Our colleague that
recommended our part gets credit as well, as they should since helpful
communication is presumably something we want to reward. I \emph{want}
to use the extended graph of credit rather than just listing my paper
because it's a lot more impressive! Since I want to be credited, I'm
also invested in expanding the space of linked tools. Rather than the
scarcity mindset of authorship, a link-based system can push us towards
abundance: ``good'' work is work that engages with and extends a broad
array of techniques, technologies, and expertise.

It's easy to imagine extended credit scenarios for a broad array of
workers: since my work happens at \texttt{@institution}, and the
\texttt{@institution:mice} are cared for by the members of the
\texttt{@institution:animal\_care} team, we can measure the impact of
their work on the downstream work it supports. A grad student rotating
in a lab might not get enough data to make a paper, but they might make
some tangible improvement to lab infrastructure, which they can document
and receive credit for. Open source software developers might get some
credit from a code paper, but will be systematically undervalued from
failure to cite it and undercounted in derivative packages. The many
groups of workers whose work is formally excluded from scientific
valuation are those with the most to gain by reimagining credit systems,
and an infrastructural plan that actively involves them and elevates
their work has a much broader base of labor, expertise, and potential
for buy-in.

Some of my more communitarian colleagues might share my distaste for
metricizing knowledge work --- but hiring committees and granting
agencies are going to use \emph{some} metric, the question is whether
it's a good reflection of our work and who controls it. Our problems
with the h-index (eg. \citep{teixeiradasilvaMultipleVersionsHindex2018, costasReflectionsCautionaryUse2018} ) are problems with paper
citations being a bad basis for evaluating scientific ``value'', and
their primacy is in turn a consequence of the monopoly over scientific
communication and organization by publishers and aggregators like Scopus
and Google Scholar. Their successors, black box algorithmic tools like
SciVal with valuation criteria that are bad for science (but good for
administrators) like `trendiness' are here whether we like it or not. A
transparent graph of scientific credit at least gives the
\emph{possibility} for reimagining the more fundamental questions of
scientific valuation: assigning credit for communication, maintenance,
mentorship, and so on. So some misguided reductions of the complexity of
scientific labor to a single number are inevitable, but at least we'll
be able to \emph{see what they're based on} and \emph{propose
alternatives.}

It's true that some of these extended metrics are already possible to
compute. One could crawl package dependencies for code, or download the
\href{https://academictorrents.com/details/e4287cb7619999709f6e9db5c359dda17e93d515}{100GB
Crossref database} \citep{crossrefJanuary2021Public2021}  and
manually crunch our statistics, but being \emph{able} to compute some
means of credit is very different than making it a \emph{normal part} of
doing and evaluating research. The multimodality of credit assignment
that's possible with a linked data system is part of its power: our work
\emph{actually does} have impacts across modalities, and we should be
able to represent that as part of our contribution to science.

Reaching a critical mass of linked tools and peers is not altogether
necessary for them to be useful, but critical mass may trigger a
positive feedback loop for the development of the system itself. Even in
isolation, a semantic wiki is a better means of assigning credit than a
handful of google docs, experimental tools that automatically annotate
data are better than a pile of \texttt{.csv} files, etc. Bridging two
tools to share credit is better than one tool in isolation, and more
people using them are better than fewer for any given user of the
system. Lessons learned from STS, Computer-Supported Cooperative Work
(CSCW), pirates, wikis, forums, et al.~make it clear that \emph{the
labor of maintaining and building the system can't be invisible.}




\chapter{Conclusion}










\section{Tactics \& Strategy}



(Gestural) Roadmap

Some of the tactical vision for this piece is embedded in its structure
and has been discussed throughout, but to again reaffirm the strictly
\emph{non-utopian} nature of this argument it's worth revisiting the
practical means by which we might build it. I have tried to take care to
hew as close to existing technologies and practices as possible, and so
the amount of new development that is needed is relatively light. As is
true in the rest of the piece, the recommendations here are just for the
purpose of illustration, and here more than anywhere else every step of
this is subject to negotiation and the contingency of future work.

For the purposes of brevity, I'm going to refer to the family of
RDF-based tools like JSON-LD, turtle, OWL, and so on as ``RDF-like.''

These, I think, are the most minimal development steps that would get a
system like this off the ground and offer some basic use.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}

\item
  Build framework for \textbf{bridging RDF-like schema to p2p client} -
  The implementation of a given schema needs to be made abstract so that
  data can be subset from a given dataset using the notation of the
  schema namespace. This can happen gradually -- at first mapping from
  metadata to the entire dataset, but need to be able to read/write
  individual entities. \href{https://docs.dat.foundation/docs/faq}{Dat}
  can share subsets of data using Hypercore's
  \href{https://hypercore-protocol.org/guides/walkthroughs/creating-and-sharing-hypercores/}{sparse
  mode}. Work could also happen in the other direction, extending
  DANDI/Datalad with a peer-to-peer backend.
\item
  Build \textbf{p2p/ActivityPub client} - currently activitypub accounts
  are associated with a homeserver like \url{https://mastodon.social/},
  but the authors of the ActivityPub protocol and JSON-LD have described
  how a system of decentralized identity (eg. DID \citep{spornyDecentralizedIdentifiersDIDs2021} ) could make it fully
  peer-to-peer \citep{webberActivityPubDecentralizedDistributed2017} . Joint ActivityPub/p2p systems that use AP to index data and p2p
  to share it already exist (eg.
  \href{https://joinpeertube.org/\#what-is-peertube}{PeerTube}). Much of
  the work left undefined here would be interface and UX design to make
  hosting an instance as easy as possible.
\item
  Experiment with \textbf{minimal schema for federation.} We want to get
  code running first in smaller communities to experiment with the basic
  ontologies for communication, data sharing, and framework linking.
  There are a \href{https://lov.linkeddata.es/dataset/lov/vocabs}{large
  number} of existing vocabularies and ontologies to draw from, so we
  don't need to start from scratch. More important than \emph{which}
  ontology is chosen is making it \emph{simple} to browse and manipulate
  them. Eg.:

  \begin{itemize}
  
  \item
    Social interaction:
    \href{https://www.w3.org/TR/activitystreams-vocabulary/}{ActivityStreams}
    \citep{snellActivityStreams2017} , Semantically Interlinked
    Online Communities (\href{http://sioc-project.org/}{SIOC}) \citep{harthLinkingSemanticallyEnabled2004} 
  \item
    Permissions: Open Digital Rights Language
    (\href{https://www.w3.org/ns/odrl/2/}{ODRL})
  \item
    Scientific communication -
    \href{http://linkedscience.org/lsc/ns/}{Linked Science Core} \citep{kauppinenLinkedOpenScienceCommunicating2011} , Modern
    Science Ontology
    (\href{https://saidfathalla.github.io/Science-knowledge-graph-ontologies/doc/ModSci_doc/index-en.html\#dataproperties-headline}{ModSci})
  \item
    Workflows/Analysis Pipelines - Open Provenance Model for Workflows
    (\href{https://www.opmw.org/model/OPMW/}{OPMW})
  \item
    and many, many more.
  \end{itemize}
\item
  A basic \textbf{tracker-like web framework} for caching and serving
  metadata, as well as hosting some initial forums or other semidurable
  communication system for organizing federations. It's an open question
  to me how much of
  \href{https://github.com/WhatCD/Gazelle}{Gazelle}/\href{https://github.com/WhatCD/Ocelot}{Ocelot}
  is worth resurrecting, or whether it would be better to build from an
  ActivityPub client or a torrent tracker (or some other existing code
  I'm not familiar with).
\end{enumerate}

From that basic means of communication, the rest of the development path
needs less specification, the examples I have given are just one way of
realizing the \protect\hyperlink{design-principles}{design principles}.
Beyond that, doing accounting for the other functionality described
above:

\begin{itemize}

\item
  Start building \textbf{bridges to existing repositories} like
  \href{https://gui.dandiarchive.org/\#/}{Dandihub}, Wikidata, and so
  on.
\item
  Refine vocabularies! Refine schema negotiation, etc.
\item
  Build tools for data translation - \emph{theoretically} building I/O
  tools from RDF-like schema to individual data formats should allow for
  interconversion, but it will of course be more difficult than that.
\item
  Extend analysis and experimental tools to incorporate and produce
  linked data within our p2p/AP system.
\item
  Integrate dependency management for linked data that specifies code to
  run, eg. see \href{https://spack.readthedocs.io/en/latest/\#}{spack}
  (see spack)
\item
  The wide world of communication tools awaits\ldots{}
\end{itemize}

The numerical order of these lists is just a byproduct of the linearity
of text, and many of these development projects can happen
simultaneously with minimal mutual coordination. The chronology given is
mostly strategic: finding an incremental development path where every
step is useful and allows for the next to be taken.

A p2p data system is, to me, is the most likely and proximal anchor from
which the rest of the technologies might flow --- this is part of the
idea animating the Solid project as well \citep{berners-leeSociallyAwareCloud2009, finleyTimBernersLeeInventor2017, barabasDefendingInternetFreedom2017, sambraSolidPlatformDecentralized2016} .

There is no reason, in principle, that data must first be converted into
some standardized format --- ideally, it would be possible to fluidly
link data in-place, incorporating whatever means by which metadata is
stored already. Passing through a point of standardization serves a few
purposes: first, lowering the work associated with linking by only
needing to declare links between a few hundred formats instead of the
infinity of arbitrarily structured data. Second, to minimize frustration
and maximize delight of early adopters: people are more likely to stick
around if they can run a client, plug their data in, and see it hosted
with the links prepopulated from the format schema. Third, to integrate
with existing tools and databases to avoid the perception of potential
sunk cost spending time formatting data in some new idiosyncratic way.

Shared data is a concrete, widely understood goal shared by many
scientists already, but there are relatively slim incentives for
spending the time to do it. The first major hurdle is to make those
incentives. Propping up a p2p system will eventually need new
development, but existing p2p systems can still make a strong case for
themselves with small, local examples: using them to share data with
local collaborators, or to share data during a workshop or conference,
or even to start rehosting already-public centrally hosted data. Small
communities of practice can start their own ``retreat from the cloud''
by documenting their process and setting up their own local hosting and
servers. They also make natural allies with the p2p tool developers.
Being a test case for their software and cultivating social ties across
domains is one way to start aligning our goals and movement building.
Tools like Dat and Solid are good fits, though they currently need some
UX and docs work to be accessible to a broad audience.

Cultivating new relationships with knowledge and technical communities
outside our usual academic circles is a critical part of any
infrastructural development project. Though these days we hear about
disaffected people abandoning academia for industry, there is plenty of
disaffectation to go around on the other side --- particularly in
software world. There are a growing number of extremely talented
programmers that grow disillusioned with working for companies they view
as unethical, which unfortunately happen to be some of the largest
employers in the field. Ethical, important, and dare I say fun software
often has no business model, but data, computation, and communications
infrastructural infrastructure in science, especially with a coherent
frame could give many of them a job they don't have to feel conflicted
about.

\subsection{To Whom It May
Concern\ldots{}}

Who is supposed to do what, and why would different interested groups
want to pursue decentralized infrastructure? A few love letters
addressed to different groups:


\paragraph{Rank and File Researchers}

\begin{itemize}

\item
  PIs vs grad students/post-docs: PIs need to realize that the true cost
  is doing nothing, the ROI on infrastructure is massive given the
  extremely high costs of labor for doing all this work. Grad students
  and Post docs should start seeing the total isolation of their local
  tooling as problematic and engage with their neighboring labs to share
  technologies and start building locally integrated tools!
\item
  We need to start making alliances we're not necessarily used to, but
  this is the fun part! We can ally with the many disaffected tech workers
  that don't want to work with google and facebook -- we all talk about
  academics fleeing for industry, but what academia can offer in return is
  jobs building tools that aren't soul sucking click maximizers.
\item
  We should also start working closer with our Librarians, they are also
  facing the squeeze and have had their profession degraded to being
  subscription custodians.
\item
  We need to recognize our place outside of the highest echelons as
  fundamentally in danger by advancing infrastructural polarization. A
  ``not my job'' mentality might work for now, but for how long?
\item
  Less concretely, we need to start expanding what we think is possible!
  We need to be realistic and demand the impossible! Let's let the work
  of escaping ownership by platform capitalism be joyful, a rennaisance
  of working cooperatively and rejuvinating the sense of purpose as
  scientists invested in the health of society.
\end{itemize}


\paragraph{Open Source Developers}

\begin{itemize}

\item
  UX and community systems first! Start a project by reaching out to
  other devs and seeing who's doing overlapping work.
\item
  Integrate with existing tools rather than bulding new ones is holy:
  you can still get credit in existing systems for writing the paper,
  and your tool is more likely to be used, and it's likely to benefit
  from some of the structuring elements of the framework.
\item
  Stop building cloud-exclusive tools! Or if you have to make something for the
  cloud bc compute infrastructure isn't there, make sure it's also
  deployable on local computers. We don't need any more single-use
  platforms!
\end{itemize}


\paragraph{Funding Agencies}

\begin{itemize}

\item
  If you pay us we will build it!
\item
  Fund integrating existing tools in addition to maintaining them.
  Target funding for new tools that fill specific gaps --- it's almost
  impossible to get a really well maintained library off the ground b/c
  catch-22 of development!
\item
  You're being swindled! There is an unspoken a conflict of interest because to some
  degree centralization is politically useful: eg. see the RELX data sharing
  agreement with ICE, but this probably isn't shared by the people
  actually at the granting agencies and I don't want to speculate on
  some conspiracy theory.
\end{itemize}


\paragraph{University Administrators}

\begin{itemize}

\item
  You're also being swindled!
\item
  Local infrastructure is good for you too --- many universities are
  plagued by SaaS that is expedient but ultimately makes the entire
  operation of the university very fragile.
\item
  Having good local data infrastructure is a really good thing to be
  able to tell applicants, and makes use of intranet for collaboration
  instead of external bandwidth. You get to say ``we have a sick new
  storage and compute server'' instead of ``we're a huge subscriber to
  AWS''
\item
  Y'all are the ones who have to pay the journal costs and deal with
  your university being uncompetetive with other institutions that can
  afford more, and so you should be leading the charge to nonprofit
  journals and a move beyond them, rather than mandating Open Access
  which is a regressive move. 


\end{itemize}


\section{Limitations}


We can start with a few of the big and obvious
limitations: people could ignore this piece entirely and it is dead on
arrival. This project would be a direct threat to some of the most
powerful entities in science, and they will likely actively work against
it. I could be completely misinformed and missing something fundamental
about the underlying technologies. The social tensions between the
relevant development communities could be too great to overcome.

Two outstanding problems on Mastodon hint at a few open challenges to
development: feed organization and the fluidity of federation formation,
dissolution, and interaction.

By default, and affirmed by maybe an understandable reaction against
algorithmic feed organization, Mastodon is a mostly chronological list
of posts from people that you follow and that are in your host server's
federated networks. While this transparency is reassuring that we aren't
being microtargeted for advertising, it does make the system
overwhelming to navigate, and splitting accounts multiple times to
accomodate is common. A system of semantic organization is a distinct
third way between algorithmic and chronological organization. Building a
system that goes beyond moderator-specified category systems familiar in
forums towards a sensible interface for navigating tangled concept
hierarchies is an open challenge, as far as I'm aware.

An intermediate goal might be to give finer control over groups, but
groups are currently a complicated question between fediverse
implementations \citep{StandardizingActivityPubGroups2021} .


\section{Contrasting Visions of
Science}



 Through this text I have tried to sketch in parallel a
potentially liberatory infrastructural future with the many offramps and
alternatives that could lead us astray, but haven't given a picture of
what \emph{actually doing research} might be like were this project to
come anywhere close to succeeding. Through the hints at what could be
our current and future information capitalism dystopia the alternative
is too a little foggy: what happens if we do nothing? Let me make the
point with a bit of speculative fiction.


\subsection{What if we do nothing?}

You're a researcher with dead-center median funding at an institute with
dead-center median prestige, and you have a new idea.

The publishing industry has built its surveillance systems into much of
the practice of science: their SeamlessAccess login system and browser
fingerprinting harvest your reading patterns across the web\citep{sariGuestPostTechnology2018, brembsSNSINewPRISM2020, nisoNISORP272019Recommended2019, snsiCybersecurityLandscapeProtecting2020, SeamlessAccessActionSeamlessAccess} , Mendeley watches what you
highlight and how you organize papers, and with a data sharing agreement
with Google crossreference and deanonymize your drafts in progress \citep{pooleySurveillancePublishing2021} . Managing constant
surveillance is a normal part of doing science now, so when reading
papers you are careful to always use a VPN, stay off the WiFi whenever
possible, randomly scroll around the page to appear productive while the
PDF is printing to read offline. The publishers have finally managed to
kill sci-hub with a combination of litigation and lobbying universities
to implement mandatory multifactor authentication, cutting off their
ability to scrape new papers. The few papers you're able to find, and
fewer that you're able to access, after several weeks of carefully
covering your tracks while hopping citation trees make you think your
hunch might be right --- you're on to something.

This is a perfect project for a collaboration with an old colleague from
back in grad school. Their SciVal Ranking is a little low, so you're
taking a risk by working with them, but friendship has to be worth
something right? ``Don't tell me I never did nothing for you.'' You
haven't spoken in many years though, so you have to be careful on your
approach. The repackaged products of all their surveillance are sold
back to the few top-tier labs able to afford the hype-prediction
products that steer all of their research programs \citep{lifesciencesprofessionalservicesEmergingTrendsPancreatitis2021, elsevierTopicProminenceScience} . The publishers sell tips on what's
hot, and since they sell the same products to granting agencies and
control the publishing process, every prediction can be self-fulfilling
--- the product is plainly prestige, and the product is good. If you
approach your colleague carelessly, they could turn around and plug the
idea into the algorithm to check its score, tipping off the larger labs
that can turn their armies of postdocs on a dime to pounce. There is no
keeping up with the elites anymore.

Even if you do manage to keep it a secret, it'll be a hard road to pull
off the experiment at all. There are a few scattered open source tools
left, but the rest have been salami sliced into a few dozen mutually
incompatible platforms (compatibility only available with the HyperGold
Editions). The larger labs are able to afford all the engineers they
need to build tools, but have little reason to share any of the
technical knowledge with the rest of us --- why should they spoil the
chance to spin it off into a startup? There aren't any jobs left in
academia anyway.

Industry capture has crept into ever more of the little grant funding
you have, all the subscriptions and fees add up, so you can only afford
to mentor one grad student at a time while keeping plausibly up to date
with new instrument technology. You can't choose who they are anymore
really. The candidate ranking algorithms have thoroughly baked the
exclusionary biases of the history of science into the pool of
applicants\citep{pooleySurveillancePublishing2021, brembsAlgorithmicEmploymentDecisions2021} , so the only ones left are
those who have been playing to the algorithm since they were in middle
school. Advocates for any sort of diversity in academia are long gone.
We've never been able to confirm it, but everyone knows that the
publishers tip the scales of the algorithm to downrank anyone who starts
organizing against them.

Your colleague and you manage to coordinate. they're the same as they've
always been, trustworthy. You really need someone from a different field
at least in consultation, but there isn't really a good way to find who
would be a good fit. Somehow Twitter is still the best way to
communicate at large, but you've never really gotten how it works and
the discourse has gotten \emph{dark} so you don't have enough followers
to reach outside your small bubble of friends. You decide to go it your
own, and find the best papers you can from what you think is the right
literature base, but there's no good way of knowing you're following the
right track. Maybe that part of the paper is for the supplement.

Data is expensive, if you can find it. Who can pay the egress costs for
several TB anymore? You forego some modeling that would help with
designing the experiment because you don't have the right subscription
to access the data you need. You'll have to wait until there is a
promotional event to to get some from a Science Influencer.

You experiment in public silence until you've collected your data. Phew,
probably safe from getting scooped. You start the long slog of batch
analysis with the scraps of Cloud Compute time you can afford.

Papers are largely unchanged, still the same old PDFs. They're a source
of grim nostalgia, at least we'll always have PDF. What has changed is
citation: since it's the major component of the ranking algorithm,
nobody cites to reference ideas anymore, just to try and keep their
colleagues afloat. The researchers who still care about the state of
science publish a parallel list of citations for those who still care to
read them, but most just ignore them --- the past is irrelevant anyway,
the only way to stay afloat is hunting hype. You know this is distorting
the literature base, feeding the algorithm junk data that will steer the
research recommendations off course, but you don't want to see your
colleague down the hall fired \citep{brembsAlgorithmicEmploymentDecisions2021} . Their rankings have been
sinking lately.

Uploading preprints is expensive now too, and they charge by the
version, so you make sure you've checked every letter before sending it
off. It's a really compelling bit of science, some of that old style
science, fundamental mechanisms, basic research kind of stuff. You check
your social media metrics to perfectly time your posts about it, click
send, and wait. Your friends reply with their congratulations, glad you
managed to pull it off, but there's not really a lot that can be made a
meme of, and it's not inflammatory enough to bait a sea of hot takes.
You watch your Altmetric idle and sigh. You won't get a rankings boost,
but at least it looks like you're safe from sinking for awhile.

You're going to take a few weeks off before starting the multi-year
process of publication. Few researchers are willing to review for free
anymore, everyone is sick of publisher profiteering, but we didn't
manage to build an alternative in time, and now it's too dangerous to
try. Triage at the top of the journal prestige hierarchy is ruthless.
Most submissions not pre-coordinated with the editor are pre-desk
rejected after failing any one of the dozen or so benchmarks for
``quality'' and trendiness crunched by their black box algorithms.
Instead we ping-pong papers down the hierarchy, paying submission fees
all along the way. Don't worry, there's always some journal that will
take any work --- they want the publication fees in any case. If you're
cynically playing the metrics game, you can rely on the class of
blatantly sacrificial junk journals that can be hastily folded up when
some unpaid PubPeer blogger manages to summon enough outrage on social
media. We haven't managed to fix the problems with peer review that
favor in-crowd, clickbait-friendly, though not necessarily reproducible,
research. It turned out to have been a feature, not a bug for their
profit model all along.

You're not sure if you've made a contribution to the field, there isn't
any sense of cumulative consensus on basic problems. People study things
that are similar to you, lots of them, and you talk. You forget what
they've been doing sometimes, though, and you catch what you can. You
like your work, and even find value in it. You can forget about the rest
when you do it. And you like your lab. The system isn't perfect but
everyone knows that. Some good science still gets done, you see it all
the time from the people you respect. It's a lot of work to keep track
of, at least without the subscription. But you managed to make it
through another round. That feels ok for now. And it's not your job,
your job is to do science.

The attention span of your discipline has gotten shorter and shorter,
twisting in concentric hype cycles, the new \emph{rota fortuna.} It's
good business, keeping research programs moving helps the other end of
the recommendation system. It started with advertising that looked like
research \citep{elsevier360AdvertisingSolutions} , but the ability
to sell influence over the course of basic science turned out to be
particularly lucrative. Just little nudges here and there, you know,
just supply responding to demand. They turn a blind eye to the botnets
hired to manipulate trending research topics by simulating waves of
clicks and scrolls. More clicks, more ads, the market speaks, everybody
wins.

The publishers are just one piece of the interlocking swarm of the
information economy. The publishers sell their data to all the others,
and buy whatever they need to complete their profiles. They move in
lockstep: profit together, lobby together. The US Supreme Court is
expected to legalize copyrighting facts soon, opening up new markets for
renting licenses to research by topic area. No one really notices
intellectual property expansions anymore. There are more papers than
ever, but the science is all ``fake news.'' Nobody reads it anyway.


\subsection{What we could build}

You're a researcher with dead-center median funding at an institute with
dead-center median prestige, and you have a new idea.

You are federated with a few organizations in your subdiscipline that
have agreed to share their full namespaces, as well as a broader, public
multidisciplinary indexing federation that organizes metadata more
coarsely. You navigate to a few nodes in the public index that track
work from some related research questions. You're able to find a number
of forum conversations, blog posts, and notebooks in the intersection
between the question nodes, but none that are exactly what you're
thinking about. There's no such thing as paywalls anymore, but some of
the researchers have requested to be credited on view, so you accept the
prompts that make a \texttt{read} link between you and their work. You
can tell relatively quickly that there is affirmatively a gap in
understanding here, rather than needing to spend weeks reading to rule
it out by process of elimination --- you're on to something.

You request access to some of the private sections of federations that
claim to have data related to the question nodes. They have some
writing, data, and code public, but the data you're after is very raw
and was never written up --- just left with a reference to a topic in
case someone else wanted to use it later. Most accept you since they can
see your affiliation in good standing with people and federations they
know and trust. Others are a little more cagey, asking that you request
again when you have a more developed project rather than just looking
around so they can direct your permissions more finely, or else not
responding at all. The price of privacy, autonomy, and consent: we might
grumble about it sometimes, but all things considered are glad to pay
it.

Your home federations have a few different names for things than those
you've joined, so you spend a few hours making some new mappings between
your communities, and send them along with some terms they don't have
but you think might be useful for them and post them to their link
proposals inbox. They each have their own governance process to approve
the links and associate them with their namespace, but in the meantime
they exist on yours so you use them to start gathering and linking data
from a few different disciplines to answer some preliminary questions
you have. In the course of feeling out a project, you've made some new
connections between communities, concepts, and formats, and made
incremental improvements on knowledge organization in multiple fields.
You're rehosting some of their data as a gesture of good faith, because
you're using it and it's become part of your project, (and because a few
of the federations have ratio requirements).

You do some preliminary analysis to refine your hypotheses and direct
the experimental design. You are able to find some analysis code from
your new colleagues in a notebook linked to the data of theirs that
you're using. It doesn't do \emph{exactly} what you want, but you're
able to extend it to do a variation on the analysis and link it from
their code in case anyone else wants to do something similar.

You post a notebook of some preliminary results from your secondary
analysis and a brief description of your idea and experimental plan in a
thread that is transcluded between the forums of the now several
federations involved in your project. There's little reason to fear
being scooped: since you're in public conversation with a lot of the
people in the relevant research areas, and have been linking your work
to the concepts and groups that any competitor also would have to, it
doesn't really make sense to try and rush out a result faster than you
to take credit for your ideas. All the provenance of your conversations
and analyses is already public, and so if someone did try and take
credit for your idea, you would be able to link to their work with some
``uncredited derivation'' link.

In the thread, several people from another discipline point out that
they have already done some of what you planned to do, so you link to
their post to give them credit for pointing you in the right direction
and transclude the relevant work in your project. Others spitball some
ideas for refinements to the experiment, and try out alternate analysis
strategies on your preliminary results. It's interesting and useful, you
hadn't thought about it that way. They give you access to some of their
nonpublic datasets that they never had a chance to write up. It'll be
useful in combination with your experimental results, and in the process
you'll be helping them analyze and interpret their unused data.

You're ready to start your experiment. They say an hour in the library
is worth a thousand at the bench, and your preliminary work has let you
skip about a third of what you had initially planned to do. The project
gives credit and attribution to the many people whose work you are
building on and who have helped you so far, and has been made richer
from the discussion and half dozen alternative analyses proposed and
linked from your thread.

Some of the techniques and instruments are new to you, but you're able
to piece together how they work by surfing between the quasi-continuous
wikis shared between federations. Hardware still costs money, but since
most people able to make do with less specialized scientific instruments
because of the wealth of DIY instrument documentation, and scientists
are able to maintain grant funded nonprofit instrument fabrication
organizations because their work is appropriately credited by the work
that uses them, it's a lot less expensive. You try out some parameter
sets and experiment scripts in your experimental software linked by some
technical developers in the other fields. You get to skip a lot of the
fine tuning by making use of the contextual knowledge: less dead ends on
the wrong equipment, not having to rediscover the subtleties of how the
parameters interact, knowing that the animals do the experiment better
if the second phase is delayed by a second or two more than you'd
usually think. Your experimental software lets you automatically return
the favor, linking your new parameters and experimental scripts as
extensions of the prior work.

While you were planning and discussing your experiment you had been
contributing your lab's computing hardware to a computational co-op so
other people could deploy analyses on it while it was idle. Now you have
some credit stored up and distribute the chunks of your analysis across
the network. It takes a little bit of tweaking to get some of the more
resource-intensive analysis steps to work on the available machines. You
don't have time to organize a full pull request to the main analysis
code, but if someone wants to do something similar they'll be able to
find your version since it's linked to the main library as well as the
rest of your project.

You combine the various intermediary results you have posted and been
discussing in the forums into a more formal piece of writing. You need
to engage with the legacy scientific literature for context, so you
highlight the segments you need and make direct reference to and
transclude the arguments that they are making in your piece. While
you're writing you annotate inline how your work
\texttt{{[}{[}extends::@oldWork{]}{]}} because it
\texttt{{[}{[}hasPerspective::@newDiscipline{]}{]}}. Some of your
results \texttt{{[}{[}contradict::@oldWork:a-claim{]}{]}} and so the
people who have published work affirming it are notified and invited to
comment.

There isn't any need for explicit peer review to confirm your work as
``real science'' or not. The social production of science is very
visible already, and the smaller pieces you have been discussing
publicly are densely contextualized by affirmative and skeptical voices
from the several disciplines you were engaging with. You have
\texttt{@public} annotations enabled on my writing, so anyone reading my
work is able to see the inbound links from others highlighting and
commenting on it. Submitting in smaller pieces with continual feedback
has let you steer your work in more useful directions than your initial
experimental plan, so you've already been in contact with many of the
people who would otherwise have been your biggest skeptics and partially
addressed their concerns. People are used to assessing the social
context of a work: the interfaces make it visually obvious that work
that has few annotations, a narrow link tree, or has a really restricted
circle of people able to annotate it has relatively less support. When a
previously well-supported set of ideas is called into question by new
methods or measurements, it's straightforward to explore how its
contextual understanding has changed over time.

It's rare for people to submit massive singular works with little public
engagement beforehand. There isn't a lot of reward for minimal
authorship because the notion of ``authorship'' has been dissolved in
favor of fluid and continuous credit assignment --- engaging with little
prior work and making few contributions to the data and tooling where it
would have been obvious to do so is generally seen as antisocial. They
are in the unenviable position of having sunk several years of work into
a flawed experimental design that many people in the community could
have warned about and helped with, but now since the criticisms are
annotated on their work they likely will have to do yet more work if
they can't be adequately addressed or dismissed. We don't miss the old
system of peer review.

It's clear that you have made a contribution to not only your field, but
several that you collaborated with. Your project is a lot more than a
single PDF: you can see (and be credited for) the links between data
formats, communities, forum posts, notebooks, analytical tools,
theories, etc. that you created. It's clear how your work relates to and
extends prior work because you were engaging with the structure of
scientific research throughout. Your work implies further open questions
in the open spaces in the concept graphs of several different research
communities, and can organize future experiments without the need for
explicit coordination.

There are a dozen or so metrics that are used to evaluate research and
researchers. None of them are exactly neutral, and there is ongoing
debate about the meaning and use of each since there are so many
modalities of credit in a given person's graph. There isn't such a thing
as a \emph{proprietary} metric though, because no company has a monopoly
on proprietary information that they could say makes it unique, and why
would you trust a random number given by a company when there are plenty
of ways to measure the public credit graphs? It's relatively hard to
game the system, there aren't any proprietary algorithms to fool, and
trust is a social process based on mutual affiliation instead of a
filter bubble.

The public inspectability of scientific results, the lowered barriers to
scientific communication, and ability to find research and researchers
without specialized training has dramatically changed between science
and the public at large. It's straightforward to find a community of
scientists for a given topic and ask questions in the public community
forums. Scientific communication resembles the modes of communication
most people are familiar with, and have shed some of the stilted
formality that made it impenetrable. There isn't such a firm boundary
between `scientist' and `nonscientist' because anyone can make use of
public data and community clusters to make arguments on the same forums
and feeds that the scientists do with the same mechanism of credit
assignment.

Scientists, building new systems of communication and tooling and then
seeding them with their communities has provided alternatives to some of
the platforms that dominated the earlier web. The scientists were able
to use some of their labor and funding to overcome the development
problems of prior alternatives, so they are just as easy to use as (and
much more fun than) platforms like Twitter and Facebook. Their
well-documented and easily deployed experimental hardware and software
has empowered a new generation of DIY enthusiasts, making it possible
for many people to build low-cost networked electronics to avoid the
surveillance of the ad-based ``Internet of Things,'' air quality
sensors, medical devices, wireless meshnets, and so on. The scientists
helped make controlling and using personal data much more accessible and
fluid. We now control our own medical data and selectively share it
as-needed with healthcare providers. Mass genetics databases collected
by companies like 23andme and abused by law enforcement slowly fall out
of date because we can do anything the geneticists can do.

By taking seriously the obligation conferred by their stewardship of the
human knowledge project, the scientists rebuilt their infrastructure to
serve the public good instead of the companies that parasitize it. In
the process they did their part ending some of the worst harms of the
era of global information oligopoly.

Most things aren't completely automatic or infinite, but you don't want
them to be. It's nice to negotiate with your federations and
communities, it makes you feel like a person instead of a product. Being
in a silent room where algorithms shimmer data as a dark wind
friction-free through the clouds sounds lonely. Now we are the winds and
clouds and the birds that gossip between them, and all the chatter
reminds us that we forgot what we were taught to want. You take the
hiccups and errors and dead links as the work of the world we built
together.

Everything is a little rough, a little gestural, and all very human.



\backmatter
\begin{fullwidth}
\bibliography{infrastructure_tex}
\bibliographystyle{unsrtnat}
 \end{fullwidth}

\end{document}